{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a5fb280",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T06:45:42.087898Z",
     "iopub.status.busy": "2022-01-24T06:45:42.084270Z",
     "iopub.status.idle": "2022-01-24T06:46:14.969671Z",
     "shell.execute_reply": "2022-01-24T06:46:14.968434Z",
     "shell.execute_reply.started": "2022-01-24T01:06:02.891624Z"
    },
    "papermill": {
     "duration": 32.924191,
     "end_time": "2022-01-24T06:46:14.969978",
     "exception": false,
     "start_time": "2022-01-24T06:45:42.045787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.12.5)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.96)\r\n",
      "Collecting indobenchmark-toolkit==0.0.4\r\n",
      "  Downloading indobenchmark_toolkit-0.0.4-py3-none-any.whl (8.0 kB)\r\n",
      "Collecting sacrebleu\r\n",
      "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\r\n",
      "     |████████████████████████████████| 90 kB 666 kB/s            \r\n",
      "\u001b[?25hCollecting sentencepiece\r\n",
      "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\r\n",
      "     |████████████████████████████████| 1.2 MB 2.8 MB/s            \r\n",
      "\u001b[?25hCollecting datasets==1.4.1\r\n",
      "  Downloading datasets-1.4.1-py3-none-any.whl (186 kB)\r\n",
      "     |████████████████████████████████| 186 kB 46.0 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.7.1 in /opt/conda/lib/python3.7/site-packages (from indobenchmark-toolkit==0.0.4) (1.9.1)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (4.10.0)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.0.2)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (0.70.12.2)\r\n",
      "Collecting huggingface-hub==0.0.2\r\n",
      "  Downloading huggingface_hub-0.0.2-py3-none-any.whl (24 kB)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (0.3.4)\r\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (6.0.1)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.3.4)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.26.0)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2021.11.1)\r\n",
      "Collecting tqdm<4.50.0,>=4.27\r\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\r\n",
      "     |████████████████████████████████| 69 kB 5.3 MB/s             \r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.19.5)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub==0.0.2->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (3.3.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\r\n",
      "Collecting transformers\r\n",
      "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\r\n",
      "     |████████████████████████████████| 3.4 MB 41.6 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.14.1-py3-none-any.whl (3.4 MB)\r\n",
      "     |████████████████████████████████| 3.4 MB 36.6 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\r\n",
      "     |████████████████████████████████| 3.3 MB 39.9 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.4-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 35.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 32.1 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.2-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 35.4 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.46)\r\n",
      "  Downloading transformers-4.12.1-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 41.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.0-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 40.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 33.9 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.2-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 28.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.1-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 31.1 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.0-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 37.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 30.2 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 35.6 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.1-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 34.2 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 29.9 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\r\n",
      "     |████████████████████████████████| 2.6 MB 28.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\r\n",
      "     |████████████████████████████████| 2.6 MB 34.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.9.0-py3-none-any.whl (2.6 MB)\r\n",
      "     |████████████████████████████████| 2.6 MB 37.4 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 32.4 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 34.3 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.8.0-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 30.3 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.7.0-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 37.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\r\n",
      "     |████████████████████████████████| 2.2 MB 24.3 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.6.0-py3-none-any.whl (2.3 MB)\r\n",
      "     |████████████████████████████████| 2.3 MB 32.1 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\r\n",
      "     |████████████████████████████████| 2.1 MB 33.4 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.8.9)\r\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (2.3.2)\r\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (3.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.26.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2021.10.8)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.0.8)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.7.1->indobenchmark-toolkit==0.0.4) (3.10.0.2)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (3.6.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.8.0)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2021.3)\r\n",
      "Installing collected packages: tqdm, huggingface-hub, transformers, sentencepiece, datasets, sacrebleu, indobenchmark-toolkit\r\n",
      "  Attempting uninstall: tqdm\r\n",
      "    Found existing installation: tqdm 4.62.3\r\n",
      "    Uninstalling tqdm-4.62.3:\r\n",
      "      Successfully uninstalled tqdm-4.62.3\r\n",
      "  Attempting uninstall: huggingface-hub\r\n",
      "    Found existing installation: huggingface-hub 0.1.2\r\n",
      "    Uninstalling huggingface-hub-0.1.2:\r\n",
      "      Successfully uninstalled huggingface-hub-0.1.2\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.12.5\r\n",
      "    Uninstalling transformers-4.12.5:\r\n",
      "      Successfully uninstalled transformers-4.12.5\r\n",
      "  Attempting uninstall: sentencepiece\r\n",
      "    Found existing installation: sentencepiece 0.1.96\r\n",
      "    Uninstalling sentencepiece-0.1.96:\r\n",
      "      Successfully uninstalled sentencepiece-0.1.96\r\n",
      "  Attempting uninstall: datasets\r\n",
      "    Found existing installation: datasets 1.17.0\r\n",
      "    Uninstalling datasets-1.17.0:\r\n",
      "      Successfully uninstalled datasets-1.17.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "beatrix-jupyterlab 3.1.4 requires google-cloud-bigquery-storage, which is not installed.\r\n",
      "tsfresh 0.19.0 requires statsmodels>=0.13, but you have statsmodels 0.12.2 which is incompatible.\r\n",
      "cached-path 0.3.2 requires huggingface-hub<0.2.0,>=0.0.12, but you have huggingface-hub 0.0.2 which is incompatible.\r\n",
      "cached-path 0.3.2 requires tqdm<4.63,>=4.62, but you have tqdm 4.49.0 which is incompatible.\r\n",
      "allennlp 2.8.0 requires huggingface-hub>=0.0.16, but you have huggingface-hub 0.0.2 which is incompatible.\u001b[0m\r\n",
      "Successfully installed datasets-1.4.1 huggingface-hub-0.0.2 indobenchmark-toolkit-0.0.4 sacrebleu-2.0.0 sentencepiece-0.1.95 tqdm-4.49.0 transformers-4.5.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece indobenchmark-toolkit==0.0.4 sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc9b5aeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T06:46:15.177642Z",
     "iopub.status.busy": "2022-01-24T06:46:15.176551Z",
     "iopub.status.idle": "2022-01-24T06:46:17.549796Z",
     "shell.execute_reply": "2022-01-24T06:46:17.548666Z",
     "shell.execute_reply.started": "2022-01-24T01:06:28.662781Z"
    },
    "papermill": {
     "duration": 2.468648,
     "end_time": "2022-01-24T06:46:17.550057",
     "exception": false,
     "start_time": "2022-01-24T06:46:15.081409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'amr-to-text-indonesia'...\r\n",
      "remote: Enumerating objects: 537, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (537/537), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (360/360), done.\u001b[K\r\n",
      "remote: Total 537 (delta 211), reused 444 (delta 121), pack-reused 0\u001b[K\r\n",
      "Receiving objects: 100% (537/537), 959.70 KiB | 2.52 MiB/s, done.\r\n",
      "Resolving deltas: 100% (211/211), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://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@github.com/taufiqhusada/amr-to-text-indonesia.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00732050",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T06:46:17.695835Z",
     "iopub.status.busy": "2022-01-24T06:46:17.694868Z",
     "iopub.status.idle": "2022-01-24T06:46:30.405503Z",
     "shell.execute_reply": "2022-01-24T06:46:30.404778Z",
     "shell.execute_reply.started": "2022-01-24T01:06:33.220694Z"
    },
    "papermill": {
     "duration": 12.786434,
     "end_time": "2022-01-24T06:46:30.405676",
     "exception": false,
     "start_time": "2022-01-24T06:46:17.619242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'amr-indo-dataset'...\r\n",
      "remote: Enumerating objects: 57, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (57/57), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (43/43), done.\u001b[K\r\n",
      "remote: Total 57 (delta 12), reused 53 (delta 11), pack-reused 0\u001b[K\r\n",
      "Unpacking objects: 100% (57/57), 55.48 MiB | 7.02 MiB/s, done.\r\n",
      "Updating files: 100% (50/50), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://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@github.com/taufiqhusada/amr-indo-dataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecb84bec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T06:46:30.580793Z",
     "iopub.status.busy": "2022-01-24T06:46:30.579712Z",
     "iopub.status.idle": "2022-01-24T06:46:30.586329Z",
     "shell.execute_reply": "2022-01-24T06:46:30.585423Z",
     "shell.execute_reply.started": "2022-01-24T01:13:49.72211Z"
    },
    "papermill": {
     "duration": 0.095583,
     "end_time": "2022-01-24T06:46:30.586540",
     "exception": false,
     "start_time": "2022-01-24T06:46:30.490957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/train\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/amr-to-text-indonesia/train\n",
    "# %cd /kaggle/working/amr-indo-dataset\n",
    "# !git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d35aab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T06:46:30.757864Z",
     "iopub.status.busy": "2022-01-24T06:46:30.756903Z",
     "iopub.status.idle": "2022-01-24T09:05:50.818003Z",
     "shell.execute_reply": "2022-01-24T09:05:50.816971Z"
    },
    "papermill": {
     "duration": 8360.149658,
     "end_time": "2022-01-24T09:05:50.818278",
     "exception": false,
     "start_time": "2022-01-24T06:46:30.668620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing method linearized_penman\r\n",
      "silver data folder ../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/preprocessed_silver_data/train.amr.txt', result_sent_path='data/preprocessed_silver_data/train.sent.txt', source_file_path=None, source_folder_path='../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news')\r\n",
      "100%|███████████████████████████████| 265325/265325 [00:02<00:00, 128668.69it/s]\r\n",
      "100%|███████████████████████████████| 264330/264330 [00:02<00:00, 128946.60it/s]\r\n",
      "100%|███████████████████████████████| 269258/269258 [00:02<00:00, 112764.11it/s]\r\n",
      "100%|███████████████████████████████| 270251/270251 [00:02<00:00, 124631.39it/s]\r\n",
      "100%|████████████████████████████████| 268262/268262 [00:02<00:00, 94275.23it/s]\r\n",
      "100%|███████████████████████████████| 266966/266966 [00:02<00:00, 125062.23it/s]\r\n",
      "100%|███████████████████████████████| 263263/263263 [00:02<00:00, 115141.61it/s]\r\n",
      "100%|███████████████████████████████| 263508/263508 [00:02<00:00, 120454.43it/s]\r\n",
      "100%|███████████████████████████████| 264253/264253 [00:01<00:00, 132250.75it/s]\r\n",
      "100%|███████████████████████████████| 268572/268572 [00:02<00:00, 127509.07it/s]\r\n",
      "total: 154892 pair sent-amr\r\n",
      "Running on the GPU\r\n",
      "Downloading: 100%|███████████████████████████| 777k/777k [00:00<00:00, 1.22MB/s]\r\n",
      "Downloading: 100%|██████████████████████████████| 696/696 [00:00<00:00, 594kB/s]\r\n",
      "Downloading: 100%|███████████████████████████| 990M/990M [00:46<00:00, 21.2MB/s]\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  154892\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  38723\r\n",
      "(Epoch 1) TRAIN LOSS:1.2151 LR:0.00003000: 100%|█| 38723/38723 [2:17:06<00:00,  \r\n",
      "(Epoch 1) DEV LOSS:2.6260 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 19.15it/s]\r\n",
      "bleu score on dev:  32.10573235211473\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:27<00:00,  2.79it/s]\r\n",
      "sample:  ilham meniup balon. ---- balon itu ditiup ilham\r\n",
      "sample:  obe menulis puisi. ---- obe menulis puisi\r\n",
      "sample:  saya mengetik makalah itu. ---- saya mengetik makalah\r\n",
      "sample:  angga, anak ajaib. ---- angga anak ajaib\r\n",
      "sample:  ibu saya orang dosen. ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  37.498881544316596\r\n",
      "saya mengetik makalah itu.\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x train_indoT5_on_silver_data.sh\n",
    "!./train_indoT5_on_silver_data.sh linearized_penman ../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "573af59f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T09:06:41.375968Z",
     "iopub.status.busy": "2022-01-24T09:06:41.374995Z",
     "iopub.status.idle": "2022-01-24T09:06:42.121809Z",
     "shell.execute_reply": "2022-01-24T09:06:42.121168Z"
    },
    "papermill": {
     "duration": 26.697152,
     "end_time": "2022-01-24T09:06:42.121985",
     "exception": false,
     "start_time": "2022-01-24T09:06:15.424833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir result/result_linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8f6bb0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T09:07:31.595456Z",
     "iopub.status.busy": "2022-01-24T09:07:31.594466Z",
     "iopub.status.idle": "2022-01-24T09:12:18.684976Z",
     "shell.execute_reply": "2022-01-24T09:12:18.615924Z",
     "shell.execute_reply.started": "2022-01-24T01:11:35.750582Z"
    },
    "papermill": {
     "duration": 311.87014,
     "end_time": "2022-01-24T09:12:18.685130",
     "exception": false,
     "start_time": "2022-01-24T09:07:06.814990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "resume from checkpoint\r\n",
      "added 0 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:1.0102 LR:0.00003000: 100%|█| 662/662 [01:49<00:00,  6.02it\r\n",
      "(Epoch 1) DEV LOSS:0.9685 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 22.14it/s]\r\n",
      "bleu score on dev:  37.84992363082028\r\n",
      "(Epoch 2) TRAIN LOSS:0.6590 LR:0.00003000: 100%|█| 662/662 [01:49<00:00,  6.03it\r\n",
      "(Epoch 2) DEV LOSS:0.8073 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 21.61it/s]\r\n",
      "bleu score on dev:  42.8581490799429\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:37<00:00,  2.05it/s]\r\n",
      "sample:  balon ditiup ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis obe ---- obe menulis puisi\r\n",
      "sample:  saya mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  angga anak ajaib ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  46.60994896210265\r\n",
      "saya mengetik makalah\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoT5.py --model_type indo-t5 --n_epochs 2 --data_folder ../data/preprocessed_data/linearized_penman  --result_folder result/result_linearized_penman --resume_from_checkpoint True --saved_model_folder_path result/result_supervised_task_adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71c01761",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T09:13:09.962119Z",
     "iopub.status.busy": "2022-01-24T09:13:09.961090Z",
     "iopub.status.idle": "2022-01-24T09:13:09.968641Z",
     "shell.execute_reply": "2022-01-24T09:13:09.968096Z"
    },
    "papermill": {
     "duration": 25.601863,
     "end_time": "2022-01-24T09:13:09.968771",
     "exception": false,
     "start_time": "2022-01-24T09:12:44.366908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/evaluate\n"
     ]
    }
   ],
   "source": [
    "%cd ../evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f317caf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T09:14:00.249260Z",
     "iopub.status.busy": "2022-01-24T09:14:00.247962Z",
     "iopub.status.idle": "2022-01-24T09:14:01.173701Z",
     "shell.execute_reply": "2022-01-24T09:14:01.171053Z"
    },
    "papermill": {
     "duration": 25.682564,
     "end_time": "2022-01-24T09:14:01.174061",
     "exception": false,
     "start_time": "2022-01-24T09:13:35.491497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!chmod +x evaluate_indoT5_to_all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9e099f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T09:14:52.169480Z",
     "iopub.status.busy": "2022-01-24T09:14:52.168408Z",
     "iopub.status.idle": "2022-01-24T09:17:26.970685Z",
     "shell.execute_reply": "2022-01-24T09:17:26.970097Z"
    },
    "papermill": {
     "duration": 180.190196,
     "end_time": "2022-01-24T09:17:26.970853",
     "exception": false,
     "start_time": "2022-01-24T09:14:26.780657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model folder ../train/result/result_linearized_penman\r\n",
      "preprocessing method linearized_penman\r\n",
      "preprocess amr_simple_test\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/amr_simple_test/test.amr.txt', result_sent_path='data/test/preprocessed_data/amr_simple_test/test.sent.txt', source_file_path='data/test/amr_simple_test.txt', source_folder_path=None)\r\n",
      "100%|███████████████████████████████████| 2618/2618 [00:00<00:00, 155743.39it/s]\r\n",
      "total: 306 pair sent-amr\r\n",
      "preprocess b-salah-darat\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/b-salah-darat/test.amr.txt', result_sent_path='data/test/preprocessed_data/b-salah-darat/test.sent.txt', source_file_path='data/test/b-salah-darat.txt', source_folder_path=None)\r\n",
      "100%|██████████████████████████████████████| 676/676 [00:00<00:00, 88120.01it/s]\r\n",
      "total: 32 pair sent-amr\r\n",
      "preprocess c-gedung-roboh\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/c-gedung-roboh/test.amr.txt', result_sent_path='data/test/preprocessed_data/c-gedung-roboh/test.sent.txt', source_file_path='data/test/c-gedung-roboh.txt', source_folder_path=None)\r\n",
      "100%|██████████████████████████████████████| 606/606 [00:00<00:00, 88646.05it/s]\r\n",
      "total: 29 pair sent-amr\r\n",
      "preprocess d-indo-fuji\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/d-indo-fuji/test.amr.txt', result_sent_path='data/test/preprocessed_data/d-indo-fuji/test.sent.txt', source_file_path='data/test/d-indo-fuji.txt', source_folder_path=None)\r\n",
      "100%|██████████████████████████████████████| 745/745 [00:00<00:00, 90698.84it/s]\r\n",
      "total: 27 pair sent-amr\r\n",
      "preprocess f-bunuh-diri\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/f-bunuh-diri/test.amr.txt', result_sent_path='data/test/preprocessed_data/f-bunuh-diri/test.sent.txt', source_file_path='data/test/f-bunuh-diri.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 468/468 [00:00<00:00, 105127.16it/s]\r\n",
      "total: 23 pair sent-amr\r\n",
      "preprocess g-gempa-dieng\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/g-gempa-dieng/test.amr.txt', result_sent_path='data/test/preprocessed_data/g-gempa-dieng/test.sent.txt', source_file_path='data/test/g-gempa-dieng.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 412/412 [00:00<00:00, 103961.81it/s]\r\n",
      "total: 19 pair sent-amr\r\n",
      "evaluate on data amr_simple_test\r\n",
      "Running on the GPU\r\n",
      "PreTrainedTokenizerFast(name_or_path='../train/result/result_linearized_penman/tokenizer', vocab_size=32100, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "T5Config {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"T5ForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"d_ff\": 2048,\r\n",
      "  \"d_kv\": 64,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_start_token_id\": 0,\r\n",
      "  \"dropout_rate\": 0.1,\r\n",
      "  \"eos_token_id\": 1,\r\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"initializer_factor\": 1.0,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"layer_norm_epsilon\": 1e-06,\r\n",
      "  \"model_type\": \"t5\",\r\n",
      "  \"num_decoder_layers\": 12,\r\n",
      "  \"num_heads\": 12,\r\n",
      "  \"num_layers\": 12,\r\n",
      "  \"output_past\": true,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"relative_attention_num_buckets\": 32,\r\n",
      "  \"tie_word_embeddings\": false,\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 32109\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 306\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:24<00:00,  3.13it/s]\r\n",
      "sample:  balon ditiup ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis obe ---- obe menulis puisi\r\n",
      "sample:  saya mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  angga anak ajaib ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "sample:  dia sedang berenang ---- dia sedang berenang\r\n",
      "sample:  ilham mencari buku yang hilang ---- ilham sedang mencari bukunya yang hilang\r\n",
      "sample:  ibu menyapu halaman rumah ---- ibu sedang menyapu halaman rumah\r\n",
      "sample:  padi dibajak ayah ke sawah ---- ayah sedang membajak padi di sawah\r\n",
      "sample:  di tepi pantai, andi pergi kemah ---- andi pergi berkemah di tepi pantai\r\n",
      "bleu score on test dataset:  46.60994896210265\r\n",
      "evaluate on data b-salah-darat\r\n",
      "Running on the GPU\r\n",
      "PreTrainedTokenizerFast(name_or_path='../train/result/result_linearized_penman/tokenizer', vocab_size=32100, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "T5Config {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"T5ForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"d_ff\": 2048,\r\n",
      "  \"d_kv\": 64,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_start_token_id\": 0,\r\n",
      "  \"dropout_rate\": 0.1,\r\n",
      "  \"eos_token_id\": 1,\r\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"initializer_factor\": 1.0,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"layer_norm_epsilon\": 1e-06,\r\n",
      "  \"model_type\": \"t5\",\r\n",
      "  \"num_decoder_layers\": 12,\r\n",
      "  \"num_heads\": 12,\r\n",
      "  \"num_layers\": 12,\r\n",
      "  \"output_past\": true,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"relative_attention_num_buckets\": 32,\r\n",
      "  \"tie_word_embeddings\": false,\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 32109\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 32\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:08<00:00,  1.07s/it]\r\n",
      "sample:  kantor sama pt angkasa pura ii dan otoritas bandara soekarno-hatta membahas insiden penumpang yang terbang dari jakarta ke singapura pada 10 mei, 2016 di lion air jt 161 ---- kantor imigrasi bandara soekarno-hatta bersama kantor otoritas bandara soekarno-hatta dan pt angkasa pura ii membahas insiden penumpang penerbangan pesawat lion air jt 161 yang tiba di jakarta dari singapura 10 mei 2016.\r\n",
      "sample:  kepala kantor imigrasi soekarno-hatta alif suadi menuturkan, kami meeting soal kepada tempo pada sabtu malam ini, 14 mei, 2016 ---- kami meeting soal ini,\" ujar kepala kantor imigrasi soekarno-hatta alif suadi kepada tempo, sabtu malam ini, 14 mei 2016.\r\n",
      "sample:  anak temannya natalie dan menggunakan pesawat lion air jt 161 jt berangkat ke singapura pukul 18 55 ---- anak temannya natalie berangkat dari singapura dan menggunakan pesawat lion air jt 161, pada pukul 18.50.\r\n",
      "sample:  sekretaris agus haryadi dari perusahaan pt angkasa pura ii mengatakan, pihaknya akan melakukan koordinasi dengan kantor otoritas bandara wilayah 1 terkait dengan perisitiwa ---- sekretaris perusahaan pt angkasa pura ii agus haryadi mengatakan selaku pengelola bandara internasional soekarno-hatta, pihaknya akan berkoordinasi dengan kantor otoritas bandara wilayah i terkait dengan peristiwa ini.\r\n",
      "sample:  pesawat parkir di remote area, kami menyiapkan bus antar penumpang ke terminal ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "sample:  edward menuturkan, terdapat sopir yang menjemput penumpang di padang dengan bus ---- terdapat sopir bus yang akan menjemput penumpang dari padang\" ujar edward.\r\n",
      "sample:  pihak otoritas bandara soetta memanggil pihak terkait dan melakukan investigasi ---- pihak otoritas bandara soetta akan memanggil pihak terkait dan dilakukan investigasi.\r\n",
      "sample:  kemenhub memberikan sanksi kepada pihak-pihak yang lalai ---- kemenhub memberikan sanksi kepada pihak-pihak yang lalai,\" ujarnya.\r\n",
      "sample:  zara mengutip cerita temannya mengatakan pesawat tidak mendarat di terminal ii ---- pesawat itu mendarat tidak di terminal ii, kata zara mengutip cerita temannya.\r\n",
      "sample:  pesawat parkir di remote area, kami menyiapkan bus antar penumpang ke terminal ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "bleu score on test dataset:  29.358570349938002\r\n",
      "evaluate on data c-gedung-roboh\r\n",
      "Running on the GPU\r\n",
      "PreTrainedTokenizerFast(name_or_path='../train/result/result_linearized_penman/tokenizer', vocab_size=32100, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "T5Config {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"T5ForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"d_ff\": 2048,\r\n",
      "  \"d_kv\": 64,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_start_token_id\": 0,\r\n",
      "  \"dropout_rate\": 0.1,\r\n",
      "  \"eos_token_id\": 1,\r\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"initializer_factor\": 1.0,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"layer_norm_epsilon\": 1e-06,\r\n",
      "  \"model_type\": \"t5\",\r\n",
      "  \"num_decoder_layers\": 12,\r\n",
      "  \"num_heads\": 12,\r\n",
      "  \"num_layers\": 12,\r\n",
      "  \"output_past\": true,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"relative_attention_num_buckets\": 32,\r\n",
      "  \"tie_word_embeddings\": false,\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 32109\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 29\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:08<00:00,  1.02s/it]\r\n",
      "sample:  kendaraan yang melaju dari arah mal bintaro xchange dan tol bintaro menuju jalan boulevard bintaro dilakukan alih dengan menggunakan flyover di atas lampu merah indomobil ---- pengalihan dilakukan bagi pengendara yang melaju dari arah mal bintaro xchange dan tol bintaro menuju jalan boulevard bintaro dengan menggunakan flyover di atas lampu merah indomobil.\r\n",
      "sample:  struktur tersebut mengakibatkan kegagalan dan kesalahan tes tanah terjadi di menara saidah di jalan jakarta timur mt haryono ---- kegagalan struktur dan kesalahan tes tanah yang mengakibatkan gedung miring juga terjadi pada menara saidah di jalan mt haryono, jakarta timur.\r\n",
      "sample:  kapolres ayi supardan akbp di tangerang selatan membenarkan peristiwa di bintaro yang roboh gedung menarik hati masyarakat banyak ---- kapolres tangerang selatan, akbp ayi supardan membenarkan peristiwa robohnya gedung di bintaro yang menarik perhatian masyarakat banyak.\r\n",
      "sample:  panin memutuskan melanjutkan pembangunan gedung karena dinyatakan tidak lulus uji kelayakan ---- panin memutuskan tidak melanjutkan pembangunan gedung itu karena dinyatakan tidak lulus uji kelayakan.\r\n",
      "sample:  gedung yang disebutkan dinyatakan tidak lulus uji sehingga pembangunan gedung diputuskan tidak dilanjutkan ---- gedung tersebut dinyatakan tidak lulus uji sehingga pembangunan gedung diputuskan tidak dilanjutkan.\r\n",
      "sample:  kasat akp samian dari polres tangerang selatan mengatakan, reskrim dilakukan untuk mengetahui penyebab runtuhnya gedung saat ini ---- kasat reskrim polres tangerangg selatan, akp samian mengatakan saat ini pihaknya melakukan penyelidikan untuk mengetahui penyebab runtuhnya gedung.\r\n",
      "sample:  hingga ini, kami masih dalami peristiwa yang disebut ---- hingga kini kami masih mendalami peristiwa tersebut.\r\n",
      "sample:  disebutkan, kejadian tersebut tidak menimbulkan korban ---- kejadian tersebut tidak timbul korban.\r\n",
      "sample:  kerugian material terjadi ---- terjadi kerugian material.\r\n",
      "sample:  di sektor bintaro, tangerang selatan, 7 gedung dibongkar karena prosedur yang salah ---- pembongkaran gedung di sektor 7, bintaro, tangerang selatan, diduga menyalahi prosedur.\r\n",
      "bleu score on test dataset:  33.199164921475315\r\n",
      "evaluate on data d-indo-fuji\r\n",
      "Running on the GPU\r\n",
      "PreTrainedTokenizerFast(name_or_path='../train/result/result_linearized_penman/tokenizer', vocab_size=32100, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "T5Config {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"T5ForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"d_ff\": 2048,\r\n",
      "  \"d_kv\": 64,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_start_token_id\": 0,\r\n",
      "  \"dropout_rate\": 0.1,\r\n",
      "  \"eos_token_id\": 1,\r\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"initializer_factor\": 1.0,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"layer_norm_epsilon\": 1e-06,\r\n",
      "  \"model_type\": \"t5\",\r\n",
      "  \"num_decoder_layers\": 12,\r\n",
      "  \"num_heads\": 12,\r\n",
      "  \"num_layers\": 12,\r\n",
      "  \"output_past\": true,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"relative_attention_num_buckets\": 32,\r\n",
      "  \"tie_word_embeddings\": false,\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 32109\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 27\r\n",
      "  0%|                                                     | 0/7 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:08<00:00,  1.28s/it]\r\n",
      "sample:  indosat bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan mitra pembangunan yang telah dibangun pelanggan bisnis di dalam hadirnya solusi smart dan internet of things untuk mobility ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  indosat bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan mitra pembangunan yang telah dibangun pelanggan bisnis di dalam hadirnya solusi smart dan internet of things untuk mobility ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  smart mobility dan iot mengalami perkembangan yang cukup signifikan hingga mengubah cara perusahaan dalam mengelola aset ---- smart mobility dan iot mengalami perkembangan cukup signifikan sehingga mengubah cara perusahaan dalam mengelola aset.\r\n",
      "sample:  indosat bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan mitra pembangunan yang telah dibangun pelanggan bisnis di dalam hadirnya solusi smart dan internet of things untuk mobility ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  kerja sama difokuskan pada sektor transportasi dan otomotif yang luas untuk memenuhi kebutuhan pelanggan korporasi di indonesia dan sektor industri seperti halnya ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  president achmad sofwan, director fujitsu indonesia, menuturkan, dengan menyediakan layanan proses data realtime dan aplikasi, mobilitas merupakan cara mengoptimalkan strategi perusahaan ---- mobilitas akan menyediakan cara untuk mengoptimalkan strategi perusahaan dengan memanfaatkan aplikasi dan layanan yang dapat memproses data real-time, tutur achmad sofwan, president director fujitsu indonesia.\r\n",
      "sample:  kerja sama difokuskan pada sektor transportasi dan otomotif yang luas untuk memenuhi kebutuhan pelanggan korporasi di indonesia dan sektor industri seperti halnya ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  indosat ooredoo bekerja sama dengan fujitsu indonesia untuk menghadirkan solusi smart mobility dan internet of things ---- indosat ooredoo bekerja sama dengan fujitsu indonesia menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  president achmad sofwan, director fujitsu, mengungkapkan bahwa fujitsu menyediakan layanan yang memproses data realtime dan mobilitas sebagai cara mengoptimalkan strategi perusahaan ---- president director fujitsu, achmad sofwan mengungkap, mobilitas akan menyediakan cara mengoptimalisasikan strategi perusahaan dengan memanfaatkan aplikasi dan layanan dalam aktivitas memproses data real-time.\r\n",
      "sample:  kerja sama difokuskan pada sektor transportasi dan otomotif yang luas untuk memenuhi kebutuhan pelanggan korporasi di indonesia dan sektor industri seperti halnya ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "bleu score on test dataset:  24.535564940029346\r\n",
      "evaluate on data f-bunuh-diri\r\n",
      "Running on the GPU\r\n",
      "PreTrainedTokenizerFast(name_or_path='../train/result/result_linearized_penman/tokenizer', vocab_size=32100, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "T5Config {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"T5ForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"d_ff\": 2048,\r\n",
      "  \"d_kv\": 64,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_start_token_id\": 0,\r\n",
      "  \"dropout_rate\": 0.1,\r\n",
      "  \"eos_token_id\": 1,\r\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"initializer_factor\": 1.0,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"layer_norm_epsilon\": 1e-06,\r\n",
      "  \"model_type\": \"t5\",\r\n",
      "  \"num_decoder_layers\": 12,\r\n",
      "  \"num_heads\": 12,\r\n",
      "  \"num_layers\": 12,\r\n",
      "  \"output_past\": true,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"relative_attention_num_buckets\": 32,\r\n",
      "  \"tie_word_embeddings\": false,\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 32109\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 23\r\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:06<00:00,  1.09s/it]\r\n",
      "sample:  ciptadi melompat dari megatron jembatan yang tingginya diperkirakan mencapai 15 meter ketika menyeberang orang ---- ciptadi dengan nekat melompat dari megatron jembatan penyeberangan orang yang tingginya diperkirakan mencapai 15 meter.\r\n",
      "sample:  hal yang disebutkan diungkapkan kasubag reny marthaliana humas kompol kabag dede rojudin kompol ops di polrestabes bandung pada jumat di mapolrestabes bandung ---- hal tersebut diungkapkan kabag ops polrestabes bandung kompol dede rojudin didampingi kasubag humas kompol reny marthaliana, saat ditemui di mapolrestabes bandung, jumat.\r\n",
      "sample:  rezha noviana, warga kota bandung, mengatakan noviana telah memperhatikan jemaat yang sedang salat di masjid raya bandung jumat, ia dan jemaat lainnya sedang megatron di depan kantor pos besar di seberang jembatan orang ---- rezha noviana, warga kota bandung mengatakan ia dan jemaat lainnya langsung memperhatikan megatron jembatan penyeberangan orang depan kantor pos besar bandung setelah bubar salat jumat di masjid raya bandung.\r\n",
      "sample:  petugas linmas dan berapa satpam berdiri di atas jpo ---- ketika itu, petugas linmas dan beberapa satpam berdiri di atas jpo.\r\n",
      "sample:  aksi bunuh diri terjadi di alun-alun bandung belum lama ini ---- aksi bunuh diri di alun - alun bandung belum lama ini terjadi.\r\n",
      "sample:  orang tua itu menjatuhkan diri ke jpo di alun kota bandung di alun alun ---- seorang pria tua nekat menjatuhkan diri dari jpo di alun - alun kota bandung.\r\n",
      "sample:  sudirman yang menjadi saksi mata pembunuhan di alun-alun kota bandung oleh satpol pp mengatakan, pria tersebut diperkirakan berusia 30 tahun ---- sudirman, anggota linmas satpol pp kota bandung yang menjadi saksi mata aksi bunuh diri di alun - alun bandung itu mengatakan, pria tersebut diperkirakan berusia 50 tahun.\r\n",
      "sample:  lelaki itu menjatuhkan diri di megatron di depan bandung, pos giro besar bandung, di jembatan seberang, jumat ---- seorang lelaki menjatuhkan diri dari megatron jembatan penyeberangan di depan pos giro besar bandung, bandung, jumat.\r\n",
      "sample:  lelaki tengah baya mengakhiri hidupnya di tengah kepadatan lalu lintas di jalan asia afrika ---- seorang lelaki tengah baya nekat mengakhiri hidupnya di tengah kepadatan lalu lintas jalan asia afrika.\r\n",
      "sample:  pria misterius yang bunuh diri di jembatan jalan di bandung, jawa barat, asia afrika di seberang orang membuat pesan belum terjun nekat ---- pria misterius yang bunuh diri dari jembatan penyeberangan orang di jalan asia afrika, bandung, jawa barat, membuat pesan sebelum nekat terjun.\r\n",
      "bleu score on test dataset:  31.311124463391355\r\n",
      "evaluate on data g-gempa-dieng\r\n",
      "Running on the GPU\r\n",
      "PreTrainedTokenizerFast(name_or_path='../train/result/result_linearized_penman/tokenizer', vocab_size=32100, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "T5Config {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"T5ForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"d_ff\": 2048,\r\n",
      "  \"d_kv\": 64,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_start_token_id\": 0,\r\n",
      "  \"dropout_rate\": 0.1,\r\n",
      "  \"eos_token_id\": 1,\r\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"initializer_factor\": 1.0,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"layer_norm_epsilon\": 1e-06,\r\n",
      "  \"model_type\": \"t5\",\r\n",
      "  \"num_decoder_layers\": 12,\r\n",
      "  \"num_heads\": 12,\r\n",
      "  \"num_layers\": 12,\r\n",
      "  \"output_past\": true,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"relative_attention_num_buckets\": 32,\r\n",
      "  \"tie_word_embeddings\": false,\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 32109\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 19\r\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:05<00:00,  1.12s/it]\r\n",
      "sample:  di banjarnegara, jawa tengah, badan meteorologi klimatologi geofisika menyatakan gempa berkekuatan 4 8 skala richter mengguncang dataran tinggi dieng pada pukul 19 00 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara, jawa tengah, menyatakan bahwa gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 pada skala richter.\r\n",
      "sample:  pusat gempa diperkirakan berada di desa kecamatan kabupaten batang, bawang, tanji gugur ---- pusat gempa diperkirakan berada di desa tanji gugur, kecamatan bawang, kabupaten batang.\r\n",
      "sample:  pvmbg merekomendasikan untuk sementara jalan yang dekat kawah di desa sumberejo ditutup seluruhnya ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  kepala pusat vulkanologi mitigasi bencana geologi mengatakan sebanyak 86 kali rekaman gempa terjadi pada pukul 19 00 wib di dataran tinggi dieng ---- kepala pusat vulkanologi dan mitigasi bencana geologi surono mengatakan bahwa gempa yang terjadi di dataran tinggi dieng pukul 19.00 wib terekam sebanyak 86 kali.\r\n",
      "sample:  tutup dilakukan hingga tim yang mengukur konsentrasi gas di lapangan menyatakan tidak ada gas berbahaya ---- penutupan ini dilakukan hingga tim di lapangan yang mengukur konsentrasi gas menyatakan tidak ada gas yang berbahaya.\r\n",
      "sample:  pvmbg merekomendasikan untuk sementara jalan yang dekat kawah di desa sumberejo ditutup seluruhnya ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  petugas pusat vulkanologi mitigasi bencana geologi diterjunkan untuk memantau aktivitas kawah pertimbangan dan memberikan informasi ---- petugas pusat vulkanologi dan mitigasi bencana geologi diterjunkan untuk memantau aktivitas kawah timbang dan memberikan informasi.\r\n",
      "sample:  data yang dikeluarkan oleh pusat vulkanologi mitigasi bencana geologi menyebutkan rasa dan telah merekam gempa sebanyak 86 kali mulai gempa pada pukul 19 00 wib ---- data yang dikeluarkan dari pusat vulkanologi dan mitigasi bencana geologi menyebutkan gempa mulai terasa pukul 19.00 wib dan telah terekam gempa sebanyak 86 kali.\r\n",
      "sample:  pengungsi yang mengungsi di kabupaten banjarnegara meninggal sabtu pagi ---- pengungsi di kabupaten banjarnegara, pada sabtu pagi mulai meninggalkan pengungsian.\r\n",
      "sample:  di banjarnegara, badan meteorologi klimatologi geofisika menyatakan gempa berkekuatan 4 8 skala richter mengguncang dataran tinggi dieng pada pukul 19 00 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara menyatakan gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 skala richter *(sr)*.\r\n",
      "bleu score on test dataset:  29.30236928073938\r\n",
      "mv: cannot move 'result/linearized_penman' to a subdirectory of itself, 'result/linearized_penman/linearized_penman'\r\n"
     ]
    }
   ],
   "source": [
    "!./evaluate_indoT5_to_all.sh ../train/result/result_linearized_penman linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae39fd4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T09:18:18.419110Z",
     "iopub.status.busy": "2022-01-24T09:18:18.418071Z",
     "iopub.status.idle": "2022-01-24T09:18:19.178380Z",
     "shell.execute_reply": "2022-01-24T09:18:19.177754Z"
    },
    "papermill": {
     "duration": 26.731772,
     "end_time": "2022-01-24T09:18:19.178546",
     "exception": false,
     "start_time": "2022-01-24T09:17:52.446774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/test_label.txt (deflated 86%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/test_generations.txt (deflated 85%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/test_label.txt (deflated 68%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/test_generations.txt (deflated 65%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/test_label.txt (deflated 66%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/test_generations.txt (deflated 65%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/test_label.txt (deflated 67%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/test_generations.txt (deflated 66%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/test_label.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/test_label.txt (deflated 60%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/test_generations.txt (deflated 62%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r result.zip result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9193.714969,
   "end_time": "2022-01-24T09:18:45.362548",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-24T06:45:31.647579",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
