{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f2f783c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T08:01:14.464610Z",
     "iopub.status.busy": "2022-05-18T08:01:14.463823Z",
     "iopub.status.idle": "2022-05-18T08:01:47.908103Z",
     "shell.execute_reply": "2022-05-18T08:01:47.907401Z",
     "shell.execute_reply.started": "2022-03-14T06:35:28.661684Z"
    },
    "papermill": {
     "duration": 33.503824,
     "end_time": "2022-05-18T08:01:47.908264",
     "exception": false,
     "start_time": "2022-05-18T08:01:14.404440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.15.0)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.96)\r\n",
      "Collecting indobenchmark-toolkit==0.0.4\r\n",
      "  Downloading indobenchmark_toolkit-0.0.4-py3-none-any.whl (8.0 kB)\r\n",
      "Collecting sacrebleu\r\n",
      "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\r\n",
      "     |████████████████████████████████| 90 kB 632 kB/s            \r\n",
      "\u001b[?25hCollecting datasets==1.4.1\r\n",
      "  Downloading datasets-1.4.1-py3-none-any.whl (186 kB)\r\n",
      "     |████████████████████████████████| 186 kB 2.7 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.7.1 in /opt/conda/lib/python3.7/site-packages (from indobenchmark-toolkit==0.0.4) (1.9.1)\r\n",
      "Collecting sentencepiece\r\n",
      "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\r\n",
      "     |████████████████████████████████| 1.2 MB 6.6 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.26.0)\r\n",
      "Collecting xxhash\r\n",
      "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\r\n",
      "     |████████████████████████████████| 212 kB 51.8 MB/s            \r\n",
      "\u001b[?25hCollecting tqdm<4.50.0,>=4.27\r\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\r\n",
      "     |████████████████████████████████| 69 kB 6.5 MB/s             \r\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (4.11.2)\r\n",
      "Collecting huggingface-hub==0.0.2\r\n",
      "  Downloading huggingface_hub-0.0.2-py3-none-any.whl (24 kB)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (0.3.4)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (0.70.12.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2022.2.0)\r\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (6.0.1)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.3.5)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.20.3)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub==0.0.2->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (3.4.2)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.47)\r\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\r\n",
      "Collecting transformers\r\n",
      "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\r\n",
      "     |████████████████████████████████| 4.2 MB 52.8 MB/s            \r\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\r\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\r\n",
      "     |████████████████████████████████| 6.6 MB 47.5 MB/s            \r\n",
      "\u001b[?25hCollecting transformers\r\n",
      "  Downloading transformers-4.19.1-py3-none-any.whl (4.2 MB)\r\n",
      "     |████████████████████████████████| 4.2 MB 48.3 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.19.0-py3-none-any.whl (4.2 MB)\r\n",
      "     |████████████████████████████████| 4.2 MB 56.2 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\r\n",
      "     |████████████████████████████████| 4.0 MB 48.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\r\n",
      "     |████████████████████████████████| 3.8 MB 55.1 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\r\n",
      "     |████████████████████████████████| 3.5 MB 57.0 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.16.1-py3-none-any.whl (3.5 MB)\r\n",
      "     |████████████████████████████████| 3.5 MB 56.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.16.0-py3-none-any.whl (3.5 MB)\r\n",
      "     |████████████████████████████████| 3.5 MB 51.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.14.1-py3-none-any.whl (3.4 MB)\r\n",
      "     |████████████████████████████████| 3.4 MB 48.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\r\n",
      "     |████████████████████████████████| 3.3 MB 57.9 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 49.1 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.4-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 49.2 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 45.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.2-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 43.6 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\r\n",
      "  Downloading transformers-4.12.1-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 54.2 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.0-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 57.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 51.3 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.2-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 45.4 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.1-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 45.3 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.0-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 48.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 57.0 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 54.0 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.1-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 50.4 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 43.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\r\n",
      "     |████████████████████████████████| 2.6 MB 48.3 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\r\n",
      "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\r\n",
      "     |████████████████████████████████| 2.6 MB 46.3 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.9.0-py3-none-any.whl (2.6 MB)\r\n",
      "     |████████████████████████████████| 2.6 MB 45.0 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 47.2 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 50.1 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.8.0-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 46.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.7.0-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 50.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\r\n",
      "     |████████████████████████████████| 2.2 MB 51.0 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.6.0-py3-none-any.whl (2.3 MB)\r\n",
      "     |████████████████████████████████| 2.3 MB 55.0 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\r\n",
      "     |████████████████████████████████| 2.1 MB 52.5 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (2.4.0)\r\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.4.4)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.8.9)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.26.7)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.0.9)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (3.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2021.10.8)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.7.1->indobenchmark-toolkit==0.0.4) (4.1.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (3.6.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2021.3)\r\n",
      "Installing collected packages: tqdm, xxhash, huggingface-hub, transformers, sentencepiece, datasets, sacrebleu, indobenchmark-toolkit\r\n",
      "  Attempting uninstall: tqdm\r\n",
      "    Found existing installation: tqdm 4.62.3\r\n",
      "    Uninstalling tqdm-4.62.3:\r\n",
      "      Successfully uninstalled tqdm-4.62.3\r\n",
      "  Attempting uninstall: huggingface-hub\r\n",
      "    Found existing installation: huggingface-hub 0.2.1\r\n",
      "    Uninstalling huggingface-hub-0.2.1:\r\n",
      "      Successfully uninstalled huggingface-hub-0.2.1\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.15.0\r\n",
      "    Uninstalling transformers-4.15.0:\r\n",
      "      Successfully uninstalled transformers-4.15.0\r\n",
      "  Attempting uninstall: sentencepiece\r\n",
      "    Found existing installation: sentencepiece 0.1.96\r\n",
      "    Uninstalling sentencepiece-0.1.96:\r\n",
      "      Successfully uninstalled sentencepiece-0.1.96\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "beatrix-jupyterlab 3.1.6 requires google-cloud-bigquery-storage, which is not installed.\r\n",
      "spacy 3.2.2 requires typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.1.1 which is incompatible.\r\n",
      "featuretools 1.6.0 requires numpy>=1.21.0, but you have numpy 1.20.3 which is incompatible.\r\n",
      "cached-path 1.0.2 requires huggingface-hub<0.3.0,>=0.0.12, but you have huggingface-hub 0.0.2 which is incompatible.\r\n",
      "cached-path 1.0.2 requires tqdm<4.63,>=4.62, but you have tqdm 4.49.0 which is incompatible.\r\n",
      "allennlp 2.9.0 requires huggingface-hub>=0.0.16, but you have huggingface-hub 0.0.2 which is incompatible.\r\n",
      "allennlp 2.9.0 requires tqdm>=4.62, but you have tqdm 4.49.0 which is incompatible.\u001b[0m\r\n",
      "Successfully installed datasets-1.4.1 huggingface-hub-0.0.2 indobenchmark-toolkit-0.0.4 sacrebleu-2.0.0 sentencepiece-0.1.95 tqdm-4.49.0 transformers-4.5.1 xxhash-3.0.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece indobenchmark-toolkit==0.0.4 sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa10190d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T08:01:48.110508Z",
     "iopub.status.busy": "2022-05-18T08:01:48.109674Z",
     "iopub.status.idle": "2022-05-18T08:01:51.370527Z",
     "shell.execute_reply": "2022-05-18T08:01:51.369356Z",
     "shell.execute_reply.started": "2022-03-14T06:36:04.472712Z"
    },
    "papermill": {
     "duration": 3.364599,
     "end_time": "2022-05-18T08:01:51.370668",
     "exception": false,
     "start_time": "2022-05-18T08:01:48.006069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'amr-to-text-indonesia'...\r\n",
      "remote: Enumerating objects: 1903, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (48/48), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (47/47), done.\u001b[K\r\n",
      "remote: Total 1903 (delta 2), reused 1 (delta 1), pack-reused 1855\u001b[K\r\n",
      "Receiving objects: 100% (1903/1903), 13.03 MiB | 12.84 MiB/s, done.\r\n",
      "Resolving deltas: 100% (768/768), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://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@github.com/taufiqhusada/amr-to-text-indonesia.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb2ce74c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T08:01:51.576200Z",
     "iopub.status.busy": "2022-05-18T08:01:51.575461Z",
     "iopub.status.idle": "2022-05-18T08:02:04.157692Z",
     "shell.execute_reply": "2022-05-18T08:02:04.157190Z",
     "shell.execute_reply.started": "2022-03-14T06:36:07.592912Z"
    },
    "papermill": {
     "duration": 12.68677,
     "end_time": "2022-05-18T08:02:04.157825",
     "exception": false,
     "start_time": "2022-05-18T08:01:51.471055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'amr-indo-dataset'...\r\n",
      "remote: Enumerating objects: 66, done.\u001b[K\r\n",
      "remote: Total 66 (delta 0), reused 0 (delta 0), pack-reused 66\u001b[K\r\n",
      "Unpacking objects: 100% (66/66), 60.94 MiB | 6.16 MiB/s, done.\r\n",
      "Updating files: 100% (52/52), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://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@github.com/taufiqhusada/amr-indo-dataset.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d412b7b3",
   "metadata": {
    "papermill": {
     "duration": 0.10822,
     "end_time": "2022-05-18T08:02:04.376480",
     "exception": false,
     "start_time": "2022-05-18T08:02:04.268260",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Finetune indobart with linearized penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35428bc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T08:02:04.599965Z",
     "iopub.status.busy": "2022-05-18T08:02:04.599368Z",
     "iopub.status.idle": "2022-05-18T08:02:04.602186Z",
     "shell.execute_reply": "2022-05-18T08:02:04.602624Z",
     "shell.execute_reply.started": "2022-03-14T06:38:45.630235Z"
    },
    "papermill": {
     "duration": 0.11785,
     "end_time": "2022-05-18T08:02:04.602773",
     "exception": false,
     "start_time": "2022-05-18T08:02:04.484923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/train\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/amr-to-text-indonesia/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbc8ce46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T08:02:04.827996Z",
     "iopub.status.busy": "2022-05-18T08:02:04.827196Z",
     "iopub.status.idle": "2022-05-18T08:02:05.486664Z",
     "shell.execute_reply": "2022-05-18T08:02:05.487068Z",
     "shell.execute_reply.started": "2022-03-14T06:38:45.864254Z"
    },
    "papermill": {
     "duration": 0.775603,
     "end_time": "2022-05-18T08:02:05.487239",
     "exception": false,
     "start_time": "2022-05-18T08:02:04.711636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetuning_indoBART.sh\ttrain_indoBART_on_silver_data.sh\r\n",
      "finetuning_indoT5.sh\ttrain_indoT5.py\r\n",
      "result\t\t\ttrain_indoT5_on_silver_data.sh\r\n",
      "train_indoBART.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "404bb16b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T08:02:05.713096Z",
     "iopub.status.busy": "2022-05-18T08:02:05.712244Z",
     "iopub.status.idle": "2022-05-18T08:05:48.361805Z",
     "shell.execute_reply": "2022-05-18T08:05:48.361275Z",
     "shell.execute_reply.started": "2022-03-14T06:38:46.955663Z"
    },
    "papermill": {
     "duration": 222.765359,
     "end_time": "2022-05-18T08:05:48.361946",
     "exception": false,
     "start_time": "2022-05-18T08:02:05.596587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "Downloading: 100%|███████████████████████████| 932k/932k [00:00<00:00, 2.08MB/s]\r\n",
      "Downloading: 100%|██████████████████████████████| 315/315 [00:00<00:00, 249kB/s]\r\n",
      "Downloading: 100%|██████████████████████████████| 339/339 [00:00<00:00, 239kB/s]\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "Downloading: 100%|█████████████████████████| 1.71k/1.71k [00:00<00:00, 1.47MB/s]\r\n",
      "Downloading: 100%|███████████████████████████| 526M/526M [00:17<00:00, 30.4MB/s]\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:1.3601 LR:0.00003000: 100%|█| 662/662 [00:41<00:00, 16.04it\r\n",
      "(Epoch 1) DEV LOSS:0.5929 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 52.86it/s]\r\n",
      "bleu score on dev:  57.38434026338936\r\n",
      "(Epoch 2) TRAIN LOSS:0.5418 LR:0.00003000: 100%|█| 662/662 [00:40<00:00, 16.36it\r\n",
      "(Epoch 2) DEV LOSS:0.6770 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 58.26it/s]\r\n",
      "bleu score on dev:  56.57797910100631\r\n",
      "(Epoch 3) TRAIN LOSS:0.4376 LR:0.00003000: 100%|█| 662/662 [00:40<00:00, 16.30it\r\n",
      "(Epoch 3) DEV LOSS:0.4651 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 57.29it/s]\r\n",
      "bleu score on dev:  59.79251743606556\r\n",
      "(Epoch 4) TRAIN LOSS:0.3903 LR:0.00003000: 100%|█| 662/662 [00:40<00:00, 16.15it\r\n",
      "(Epoch 4) DEV LOSS:0.5798 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 59.19it/s]\r\n",
      "bleu score on dev:  56.06612825420897\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:11<00:00,  6.50it/s]\r\n",
      "sample:  balon tersebut ditiup oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  obe sedang menulis puisi ---- obe menulis puisi\r\n",
      "sample:  saya sedang mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  anak yang ajaib itu bernama angga ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  32.137943149928766\r\n",
      "tensor([[    0,   382,  9338, 40007,   382,   475,   384, 40008,   382,  6353,\r\n",
      "           384,   384,   384,     2, 40002]])\r\n",
      "saya sedang mengetik makalah\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoBART.py --model_type indo-bart --n_epochs 4 --lr 3e-5 --num_beams 10 --data_folder ../data/preprocessed_data/linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "922afe78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T08:05:50.599968Z",
     "iopub.status.busy": "2022-05-18T08:05:50.599136Z",
     "iopub.status.idle": "2022-05-18T08:05:51.292696Z",
     "shell.execute_reply": "2022-05-18T08:05:51.291210Z",
     "shell.execute_reply.started": "2022-03-14T06:44:57.875847Z"
    },
    "papermill": {
     "duration": 1.847796,
     "end_time": "2022-05-18T08:05:51.292884",
     "exception": false,
     "start_time": "2022-05-18T08:05:49.445088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu_score_test.txt  loss_data.tsv  model  test_generations.txt  test_label.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1983798",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T08:05:53.615166Z",
     "iopub.status.busy": "2022-05-18T08:05:53.614274Z",
     "iopub.status.idle": "2022-05-18T08:06:25.114769Z",
     "shell.execute_reply": "2022-05-18T08:06:25.115254Z",
     "shell.execute_reply.started": "2022-03-14T06:44:58.623221Z"
    },
    "papermill": {
     "duration": 32.583932,
     "end_time": "2022-05-18T08:06:25.115422",
     "exception": false,
     "start_time": "2022-05-18T08:05:52.531490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result/ (stored 0%)\r\n",
      "  adding: result/model/ (stored 0%)\r\n",
      "  adding: result/model/config.json (deflated 63%)\r\n",
      "  adding: result/model/pytorch_model.bin (deflated 8%)\r\n",
      "  adding: result/test_generations.txt (deflated 64%)\r\n",
      "  adding: result/loss_data.tsv (deflated 33%)\r\n",
      "  adding: result/.gitignore (stored 0%)\r\n",
      "  adding: result/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/test_label.txt (deflated 60%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r result_without_sta.zip result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "484c49db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T08:06:27.292262Z",
     "iopub.status.busy": "2022-05-18T08:06:27.291576Z",
     "iopub.status.idle": "2022-05-18T08:06:27.294504Z",
     "shell.execute_reply": "2022-05-18T08:06:27.294912Z",
     "shell.execute_reply.started": "2022-03-14T06:45:31.282452Z"
    },
    "papermill": {
     "duration": 1.086909,
     "end_time": "2022-05-18T08:06:27.295044",
     "exception": false,
     "start_time": "2022-05-18T08:06:26.208135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/evaluate\n"
     ]
    }
   ],
   "source": [
    "%cd ../evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9bfbc68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T08:06:29.469341Z",
     "iopub.status.busy": "2022-05-18T08:06:29.468562Z",
     "iopub.status.idle": "2022-05-18T08:08:27.498268Z",
     "shell.execute_reply": "2022-05-18T08:08:27.497761Z",
     "shell.execute_reply.started": "2022-03-14T06:45:31.330294Z"
    },
    "papermill": {
     "duration": 119.109754,
     "end_time": "2022-05-18T08:08:27.498460",
     "exception": false,
     "start_time": "2022-05-18T08:06:28.388706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘result’: File exists\r\n",
      "saved model folder ../train/result\r\n",
      "preprocessing method linearized_penman\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data’: File exists\r\n",
      "preprocess amr_simple_test\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/amr_simple_test’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/amr_simple_test/test.amr.txt', result_sent_path='data/test/preprocessed_data/amr_simple_test/test.sent.txt', source_file_path='data/test/amr_simple_test.txt', source_folder_path=None)\r\n",
      "100%|███████████████████████████████████| 2618/2618 [00:00<00:00, 176017.69it/s]\r\n",
      "total: 306 pair sent-amr\r\n",
      "preprocess b-salah-darat\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/b-salah-darat’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/b-salah-darat/test.amr.txt', result_sent_path='data/test/preprocessed_data/b-salah-darat/test.sent.txt', source_file_path='data/test/b-salah-darat.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 676/676 [00:00<00:00, 124483.01it/s]\r\n",
      "total: 32 pair sent-amr\r\n",
      "preprocess c-gedung-roboh\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/c-gedung-roboh’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/c-gedung-roboh/test.amr.txt', result_sent_path='data/test/preprocessed_data/c-gedung-roboh/test.sent.txt', source_file_path='data/test/c-gedung-roboh.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 606/606 [00:00<00:00, 127482.61it/s]\r\n",
      "total: 29 pair sent-amr\r\n",
      "preprocess d-indo-fuji\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/d-indo-fuji’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/d-indo-fuji/test.amr.txt', result_sent_path='data/test/preprocessed_data/d-indo-fuji/test.sent.txt', source_file_path='data/test/d-indo-fuji.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 745/745 [00:00<00:00, 125689.09it/s]\r\n",
      "total: 27 pair sent-amr\r\n",
      "preprocess f-bunuh-diri\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/f-bunuh-diri’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/f-bunuh-diri/test.amr.txt', result_sent_path='data/test/preprocessed_data/f-bunuh-diri/test.sent.txt', source_file_path='data/test/f-bunuh-diri.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 468/468 [00:00<00:00, 129935.41it/s]\r\n",
      "total: 23 pair sent-amr\r\n",
      "preprocess g-gempa-dieng\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/g-gempa-dieng’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/g-gempa-dieng/test.amr.txt', result_sent_path='data/test/preprocessed_data/g-gempa-dieng/test.sent.txt', source_file_path='data/test/g-gempa-dieng.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 412/412 [00:00<00:00, 129704.51it/s]\r\n",
      "total: 19 pair sent-amr\r\n",
      "mkdir: cannot create directory ‘result’: File exists\r\n",
      "evaluate on data amr_simple_test\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 306\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:11<00:00,  6.54it/s]\r\n",
      "sample:  balon tersebut ditiup oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  obe sedang menulis puisi ---- obe menulis puisi\r\n",
      "sample:  saya sedang mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  anak yang ajaib itu bernama angga ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "sample:  dia berenang ---- dia sedang berenang\r\n",
      "sample:  buku yang hilang dicari oleh ilham ---- ilham sedang mencari bukunya yang hilang\r\n",
      "sample:  ibu sedang menyapu halaman rumah ---- ibu sedang menyapu halaman rumah\r\n",
      "sample:  padi yang ayah bajak di sawah ---- ayah sedang membajak padi di sawah\r\n",
      "sample:  di tepi pantai, andi pergi ---- andi pergi berkemah di tepi pantai\r\n",
      "bleu score on test dataset:  32.137943149928766\r\n",
      "evaluate on data b-salah-darat\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 32\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:02<00:00,  2.95it/s]\r\n",
      "sample:  kantor kantor sama itu sedang berdiskusi di bandara soekarnohatta mei 10 2016 ---- kantor imigrasi bandara soekarno-hatta bersama kantor otoritas bandara soekarno-hatta dan pt angkasa pura ii membahas insiden penumpang penerbangan pesawat lion air jt 161 yang tiba di jakarta dari singapura 10 mei 2016.\r\n",
      "sample:  kepala kantor imigrasi soekarnohatta alif siang ini sedang meeting soal kepada sabtu 14 mei ---- kami meeting soal ini,\" ujar kepala kantor imigrasi soekarno-hatta alif suadi kepada tempo, sabtu malam ini, 14 mei 2016.\r\n",
      "sample:  pada pukul 18, anak natalie berangkat ke singapura ---- anak temannya natalie berangkat dari singapura dan menggunakan pesawat lion air jt 161, pada pukul 18.50.\r\n",
      "sample:  kata sekretaris agus haryadi di perusahaan pt angkasa ii, akan melakukan laku kelola bandara internasional soekarnohatta ---- sekretaris perusahaan pt angkasa pura ii agus haryadi mengatakan selaku pengelola bandara internasional soekarno-hatta, pihaknya akan berkoordinasi dengan kantor otoritas bandara wilayah i terkait dengan peristiwa ini.\r\n",
      "sample:  di remote area, pesawat parkir bersiap-siap ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "sample:  di padang, sopir jemput oleh edward ---- terdapat sopir bus yang akan menjemput penumpang dari padang\" ujar edward.\r\n",
      "sample:  pihak otoritas bandara soetta memanggil pihak yang melakukan investigasi ---- pihak otoritas bandara soetta akan memanggil pihak terkait dan dilakukan investigasi.\r\n",
      "sample:  kemenhub memberikan sanksi kepada pihak yang lalai ---- kemenhub memberikan sanksi kepada pihak-pihak yang lalai,\" ujarnya.\r\n",
      "sample:  cerita teman tersebut ditulis oleh zara di terminal ii ---- pesawat itu mendarat tidak di terminal ii, kata zara mengutip cerita temannya.\r\n",
      "sample:  di remote area, pesawat parkir bersiap-siap ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "bleu score on test dataset:  7.779637653277772\r\n",
      "evaluate on data c-gedung-roboh\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 29\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:02<00:00,  3.06it/s]\r\n",
      "sample:  kendara laju arah mal bintaro xchange akan melakukan flyover di atas lampu merah ---- pengalihan dilakukan bagi pengendara yang melaju dari arah mal bintaro xchange dan tol bintaro menuju jalan boulevard bintaro dengan menggunakan flyover di atas lampu merah indomobil.\r\n",
      "sample:  terjadi kecelakan tanah dan gedung miring itu terjadi di menara jakarta timur ---- kegagalan struktur dan kesalahan tes tanah yang mengakibatkan gedung miring juga terjadi pada menara saidah di jalan mt haryono, jakarta timur.\r\n",
      "sample:  di tangerang selatan, terjadi kecelakan gedung itu ---- kapolres tangerang selatan, akbp ayi supardan membenarkan peristiwa robohnya gedung di bintaro yang menarik perhatian masyarakat banyak.\r\n",
      "sample:  pembangunan gedung itu putus oleh panin karena lulus yang layak ---- panin memutuskan tidak melanjutkan pembangunan gedung itu karena dinyatakan tidak lulus uji kelayakan.\r\n",
      "sample:  gedung itu dibangun oleh lulus tidak sehingga pembangunan gedung itu putus ---- gedung tersebut dinyatakan tidak lulus uji sehingga pembangunan gedung diputuskan tidak dilanjutkan.\r\n",
      "sample:  kata kasat akp samian di polres tangerang selatan, pembangunan gedung itu dilakukan oleh pihak yang tahu saat ini ---- kasat reskrim polres tangerangg selatan, akp samian mengatakan saat ini pihaknya melakukan penyelidikan untuk mengetahui penyebab runtuhnya gedung.\r\n",
      "sample:  kami sedang meliput peristiwa yang sebut tadi ---- hingga kini kami masih mendalami peristiwa tersebut.\r\n",
      "sample:  terjadi kecelekaan oleh korban ---- kejadian tersebut tidak timbul korban.\r\n",
      "sample:  rugi material yang terjadi ---- terjadi kerugian material.\r\n",
      "sample:  di tangerang selatan, terjadi kecelakan gedung ---- pembongkaran gedung di sektor 7, bintaro, tangerang selatan, diduga menyalahi prosedur.\r\n",
      "bleu score on test dataset:  7.49138735703375\r\n",
      "evaluate on data d-indo-fuji\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 27\r\n",
      "  0%|                                                     | 0/7 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:02<00:00,  2.72it/s]\r\n",
      "sample:  kejadian ini ditandatangani oleh indosat ooredoo indonesia dan langgan bisnis itu hadir dalam bentuk smart internet ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  kejadian ini ditandatangani oleh indosat ooredoo indonesia dan langgan bisnis itu hadir dalam bentuk smart internet ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  smart mobility iot dan peningkatan aset akan dilakukan oleh perusahaan itu ---- smart mobility dan iot mengalami perkembangan cukup signifikan sehingga mengubah cara perusahaan dalam mengelola aset.\r\n",
      "sample:  kejadian ini ditandatangani oleh indosat ooredoo indonesia dan langgan bisnis itu hadir dalam bentuk smart internet ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  di indonesia, kerja sama akan dilakukan oleh perusahaan itu ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  president achmad sofwan memimpin perusahaan realtime dan mobilitas akan dilakukan dengan cara yang optimal ---- mobilitas akan menyediakan cara untuk mengoptimalkan strategi perusahaan dengan memanfaatkan aplikasi dan layanan yang dapat memproses data real-time, tutur achmad sofwan, president director fujitsu indonesia.\r\n",
      "sample:  di indonesia, kerja sama akan dilakukan oleh perusahaan itu ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  kejadian ini dikerjakan oleh indosat ooredoo dan fujitsu indonesia ---- indosat ooredoo bekerja sama dengan fujitsu indonesia menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  president achmad sofwan memimpin perusahaan realtime dan mobilitas dilakukan dengan cara optimal ---- president director fujitsu, achmad sofwan mengungkap, mobilitas akan menyediakan cara mengoptimalisasikan strategi perusahaan dengan memanfaatkan aplikasi dan layanan dalam aktivitas memproses data real-time.\r\n",
      "sample:  di indonesia, kerja sama akan dilakukan oleh perusahaan itu ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "bleu score on test dataset:  0.6374410792131114\r\n",
      "evaluate on data f-bunuh-diri\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 23\r\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:02<00:00,  2.25it/s]\r\n",
      "sample:  ciptadi melompat di jembatan yang tinggi 15 meter ---- ciptadi dengan nekat melompat dari megatron jembatan penyeberangan orang yang tingginya diperkirakan mencapai 15 meter.\r\n",
      "sample:  hal yang disampaikan oleh kasubag reny kompol di mapolrestabes bandung ---- hal tersebut diungkapkan kabag ops polrestabes bandung kompol dede rojudin didampingi kasubag humas kompol reny marthaliana, saat ditemui di mapolrestabes bandung, jumat.\r\n",
      "sample:  di kota bandung, rezha telah bubar salat di masjid raya bandung jumat dan ia meledakkan megatron di depan kantor ---- rezha noviana, warga kota bandung mengatakan ia dan jemaat lainnya langsung memperhatikan megatron jembatan penyeberangan orang depan kantor pos besar bandung setelah bubar salat jumat di masjid raya bandung.\r\n",
      "sample:  di atas jpo, tugas linmas ---- ketika itu, petugas linmas dan beberapa satpam berdiri di atas jpo.\r\n",
      "sample:  aksi bunuh diri dilakukan di alun alun ---- aksi bunuh diri di alun - alun bandung belum lama ini terjadi.\r\n",
      "sample:  orang tua terjatuh dalam diri di alun bandung ---- seorang pria tua nekat menjatuhkan diri dari jpo di alun - alun kota bandung.\r\n",
      "sample:  di alun alun, sudirman menjadi saksi mata anggota linmas di kota bandung ---- sudirman, anggota linmas satpol pp kota bandung yang menjadi saksi mata aksi bunuh diri di alun - alun bandung itu mengatakan, pria tersebut diperkirakan berusia 50 tahun.\r\n",
      "sample:  di depan bandung, lelaki itu terjatuh di jembatan seberang jumat ---- seorang lelaki menjatuhkan diri dari megatron jembatan penyeberangan di depan pos giro besar bandung, bandung, jumat.\r\n",
      "sample:  hidup itu diakhiri oleh lelaki tengah baya di tengah jalan asia afrika ---- seorang lelaki tengah baya nekat mengakhiri hidupnya di tengah kepadatan lalu lintas jalan asia afrika.\r\n",
      "sample:  di bandung, seorang pria misterius membuat bom di seberang jalan ---- pria misterius yang bunuh diri dari jembatan penyeberangan orang di jalan asia afrika, bandung, jawa barat, membuat pesan sebelum nekat terjun.\r\n",
      "bleu score on test dataset:  6.409873137202585\r\n",
      "evaluate on data g-gempa-dieng\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 19\r\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:01<00:00,  2.67it/s]\r\n",
      "sample:  stasiun banjarnegara berada di jawa tengah badan meteorologi geofisika pukul 19 00 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara, jawa tengah, menyatakan bahwa gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 pada skala richter.\r\n",
      "sample:  di desa kecamatan batang, pusat gempa berada tanji gugur ---- pusat gempa diperkirakan berada di desa tanji gugur, kecamatan bawang, kabupaten batang.\r\n",
      "sample:  di desa sumberejo, terdapat tutup sementara ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  kepala surono memimpin pusat vulkanologi mitigasi bencana geologi pukul 19 00 wib di dataran tinggi dieng ---- kepala pusat vulkanologi dan mitigasi bencana geologi surono mengatakan bahwa gempa yang terjadi di dataran tinggi dieng pukul 19.00 wib terekam sebanyak 86 kali.\r\n",
      "sample:  tutup dilakukan oleh tim ukur di lapangan ---- penutupan ini dilakukan hingga tim di lapangan yang mengukur konsentrasi gas menyatakan tidak ada gas yang berbahaya.\r\n",
      "sample:  di desa sumberejo, terdapat tutup sementara ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  tugas pusat vulkanologi mitigasi bencana geologi yang diberikan oleh pantau kawah timbang ---- petugas pusat vulkanologi dan mitigasi bencana geologi diterjunkan untuk memantau aktivitas kawah timbang dan memberikan informasi.\r\n",
      "sample:  di pusat vulkanologi mitigasi bencana geologi, terjadi gempa pukul 19 00 wib ---- data yang dikeluarkan dari pusat vulkanologi dan mitigasi bencana geologi menyebutkan gempa mulai terasa pukul 19.00 wib dan telah terekam gempa sebanyak 86 kali.\r\n",
      "sample:  pengungsi tinggal di kabupaten banjarnegara sabtu pagi ---- pengungsi di kabupaten banjarnegara, pada sabtu pagi mulai meninggalkan pengungsian.\r\n",
      "sample:  stasiun banjarnegara diresmikan oleh badan meteorologi klimatologi geofisika pukul 19 ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara menyatakan gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 skala richter *(sr)*.\r\n",
      "bleu score on test dataset:  7.697201928691179\r\n",
      "mkdir: cannot create directory ‘result/linearized_penman’: File exists\r\n",
      "mv: cannot move 'result/amr_simple_test' to 'result/linearized_penman/amr_simple_test': Directory not empty\r\n",
      "mv: cannot move 'result/b-salah-darat' to 'result/linearized_penman/b-salah-darat': Directory not empty\r\n",
      "mv: cannot move 'result/c-gedung-roboh' to 'result/linearized_penman/c-gedung-roboh': Directory not empty\r\n",
      "mv: cannot move 'result/d-indo-fuji' to 'result/linearized_penman/d-indo-fuji': Directory not empty\r\n",
      "mv: cannot move 'result/f-bunuh-diri' to 'result/linearized_penman/f-bunuh-diri': Directory not empty\r\n",
      "mv: cannot move 'result/g-gempa-dieng' to 'result/linearized_penman/g-gempa-dieng': Directory not empty\r\n",
      "mv: cannot move 'result/linearized_penman' to a subdirectory of itself, 'result/linearized_penman/linearized_penman'\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x ./evaluate_indoBART_to_all.sh\n",
    "!mkdir result\n",
    "!./evaluate_indoBART_to_all.sh ../train/result linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe7c4198",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T08:08:29.811904Z",
     "iopub.status.busy": "2022-05-18T08:08:29.811085Z",
     "iopub.status.idle": "2022-05-18T08:08:30.476896Z",
     "shell.execute_reply": "2022-05-18T08:08:30.476412Z",
     "shell.execute_reply.started": "2022-03-14T06:46:21.715669Z"
    },
    "papermill": {
     "duration": 1.798861,
     "end_time": "2022-05-18T08:08:30.477030",
     "exception": false,
     "start_time": "2022-05-18T08:08:28.678169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result/ (stored 0%)\r\n",
      "  adding: result/amr_simple_test/ (stored 0%)\r\n",
      "  adding: result/amr_simple_test/test_generations.txt (deflated 64%)\r\n",
      "  adding: result/amr_simple_test/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/amr_simple_test/test_label.txt (deflated 60%)\r\n",
      "  adding: result/b-salah-darat/ (stored 0%)\r\n",
      "  adding: result/b-salah-darat/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/b-salah-darat/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/b-salah-darat/test_label.txt (deflated 66%)\r\n",
      "  adding: result/g-gempa-dieng/ (stored 0%)\r\n",
      "  adding: result/g-gempa-dieng/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/g-gempa-dieng/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/g-gempa-dieng/test_label.txt (deflated 67%)\r\n",
      "  adding: result/f-bunuh-diri/ (stored 0%)\r\n",
      "  adding: result/f-bunuh-diri/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/f-bunuh-diri/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/f-bunuh-diri/test_label.txt (deflated 68%)\r\n",
      "  adding: result/linearized_penman/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/test_label.txt (deflated 60%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/test_generations.txt (deflated 66%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/test_label.txt (deflated 66%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/test_generations.txt (deflated 66%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/test_label.txt (deflated 67%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/test_generations.txt (deflated 65%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/test_label.txt (deflated 68%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/bleu_score_test.txt (deflated 12%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/test_label.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/test_generations.txt (deflated 86%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/test_label.txt (deflated 86%)\r\n",
      "  adding: result/c-gedung-roboh/ (stored 0%)\r\n",
      "  adding: result/c-gedung-roboh/test_generations.txt (deflated 61%)\r\n",
      "  adding: result/c-gedung-roboh/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/c-gedung-roboh/test_label.txt (deflated 62%)\r\n",
      "  adding: result/d-indo-fuji/ (stored 0%)\r\n",
      "  adding: result/d-indo-fuji/test_generations.txt (deflated 84%)\r\n",
      "  adding: result/d-indo-fuji/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/d-indo-fuji/test_label.txt (deflated 86%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r result_without_sta.zip result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2d535a",
   "metadata": {
    "papermill": {
     "duration": 1.124304,
     "end_time": "2022-05-18T08:08:32.749784",
     "exception": false,
     "start_time": "2022-05-18T08:08:31.625480",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Finetune indobart with linearized penman + tree level embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "271bb244",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T08:08:35.017208Z",
     "iopub.status.busy": "2022-05-18T08:08:35.016192Z",
     "iopub.status.idle": "2022-05-18T08:08:35.019570Z",
     "shell.execute_reply": "2022-05-18T08:08:35.020077Z"
    },
    "papermill": {
     "duration": 1.149251,
     "end_time": "2022-05-18T08:08:35.020216",
     "exception": false,
     "start_time": "2022-05-18T08:08:33.870965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/tree_level_embeddings\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/amr-to-text-indonesia/tree_level_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "175cd224",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T08:08:37.601858Z",
     "iopub.status.busy": "2022-05-18T08:08:37.601028Z",
     "iopub.status.idle": "2022-05-18T08:08:38.256682Z",
     "shell.execute_reply": "2022-05-18T08:08:38.256167Z"
    },
    "papermill": {
     "duration": 2.063357,
     "end_time": "2022-05-18T08:08:38.256818",
     "exception": false,
     "start_time": "2022-05-18T08:08:36.193461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16334be6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T08:08:40.518705Z",
     "iopub.status.busy": "2022-05-18T08:08:40.517923Z",
     "iopub.status.idle": "2022-05-18T08:12:24.994583Z",
     "shell.execute_reply": "2022-05-18T08:12:24.994047Z"
    },
    "papermill": {
     "duration": 225.610677,
     "end_time": "2022-05-18T08:12:24.994747",
     "exception": false,
     "start_time": "2022-05-18T08:08:39.384070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "Some weights of MBartForConditionalGeneration were not initialized from the model checkpoint at indobenchmark/indobart and are newly initialized: ['model.encoder.tree_embed.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:1.3870 LR:0.00003000: 100%|█| 662/662 [00:48<00:00, 13.58it\r\n",
      "(Epoch 1) DEV LOSS:0.4111 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 42.37it/s]\r\n",
      "bleu score on dev:  58.45206044614471\r\n",
      "(Epoch 2) TRAIN LOSS:0.5565 LR:0.00003000: 100%|█| 662/662 [00:47<00:00, 13.86it\r\n",
      "(Epoch 2) DEV LOSS:0.5936 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 42.68it/s]\r\n",
      "bleu score on dev:  52.416784417341034\r\n",
      "(Epoch 3) TRAIN LOSS:0.4405 LR:0.00003000: 100%|█| 662/662 [00:47<00:00, 13.86it\r\n",
      "(Epoch 3) DEV LOSS:0.5785 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 19.03it/s]\r\n",
      "bleu score on dev:  40.23496367556997\r\n",
      "(Epoch 4) TRAIN LOSS:0.3927 LR:0.00003000: 100%|█| 662/662 [00:49<00:00, 13.48it\r\n",
      "(Epoch 4) DEV LOSS:0.5856 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 42.62it/s]\r\n",
      "bleu score on dev:  57.13385738234364\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:12<00:00,  6.23it/s]\r\n",
      "sample:  balon ditiup oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  saya sedang mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  anak ajaib itu bernama ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  32.03378873198283\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoBART.py --model_type indo-bart --n_epochs 4 --lr 3e-5 --num_beams 10 --data_folder ../data/preprocessed_data/linearized_penman_with_tree_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "550be7ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T08:12:29.831466Z",
     "iopub.status.busy": "2022-05-18T08:12:29.830299Z",
     "iopub.status.idle": "2022-05-18T08:12:30.494551Z",
     "shell.execute_reply": "2022-05-18T08:12:30.493431Z"
    },
    "papermill": {
     "duration": 3.333219,
     "end_time": "2022-05-18T08:12:30.494698",
     "exception": false,
     "start_time": "2022-05-18T08:12:27.161479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!chmod +x ./evaluate_indoBART_to_all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27dc4b0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T08:12:34.812005Z",
     "iopub.status.busy": "2022-05-18T08:12:34.811109Z",
     "iopub.status.idle": "2022-05-18T08:14:35.963833Z",
     "shell.execute_reply": "2022-05-18T08:14:35.962978Z"
    },
    "papermill": {
     "duration": 123.34338,
     "end_time": "2022-05-18T08:14:35.963987",
     "exception": false,
     "start_time": "2022-05-18T08:12:32.620607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model folder result\r\n",
      "preprocessing method linearized_penman_with_tree_level\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data’: File exists\r\n",
      "preprocess amr_simple_test\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/amr_simple_test’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/amr_simple_test/test.amr.txt', result_sent_path='data/test/preprocessed_data/amr_simple_test/test.sent.txt', source_file_path='data/test/amr_simple_test.txt', source_folder_path=None)\r\n",
      "100%|███████████████████████████████████| 2618/2618 [00:00<00:00, 178475.22it/s]\r\n",
      "total: 306  tuple_sent_amr_level\r\n",
      "preprocess b-salah-darat\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/b-salah-darat’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/b-salah-darat/test.amr.txt', result_sent_path='data/test/preprocessed_data/b-salah-darat/test.sent.txt', source_file_path='data/test/b-salah-darat.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 676/676 [00:00<00:00, 129598.20it/s]\r\n",
      "total: 32  tuple_sent_amr_level\r\n",
      "preprocess c-gedung-roboh\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/c-gedung-roboh’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/c-gedung-roboh/test.amr.txt', result_sent_path='data/test/preprocessed_data/c-gedung-roboh/test.sent.txt', source_file_path='data/test/c-gedung-roboh.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 606/606 [00:00<00:00, 134740.68it/s]\r\n",
      "total: 29  tuple_sent_amr_level\r\n",
      "preprocess d-indo-fuji\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/d-indo-fuji’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/d-indo-fuji/test.amr.txt', result_sent_path='data/test/preprocessed_data/d-indo-fuji/test.sent.txt', source_file_path='data/test/d-indo-fuji.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 745/745 [00:00<00:00, 122920.28it/s]\r\n",
      "total: 27  tuple_sent_amr_level\r\n",
      "preprocess f-bunuh-diri\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/f-bunuh-diri’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/f-bunuh-diri/test.amr.txt', result_sent_path='data/test/preprocessed_data/f-bunuh-diri/test.sent.txt', source_file_path='data/test/f-bunuh-diri.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 468/468 [00:00<00:00, 120952.26it/s]\r\n",
      "total: 23  tuple_sent_amr_level\r\n",
      "preprocess g-gempa-dieng\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/g-gempa-dieng’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/g-gempa-dieng/test.amr.txt', result_sent_path='data/test/preprocessed_data/g-gempa-dieng/test.sent.txt', source_file_path='data/test/g-gempa-dieng.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 412/412 [00:00<00:00, 121556.93it/s]\r\n",
      "total: 19  tuple_sent_amr_level\r\n",
      "evaluate on data amr_simple_test\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 306\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:12<00:00,  6.18it/s]\r\n",
      "sample:  balon ditiup oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  saya sedang mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  anak ajaib itu bernama ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "sample:  dia berenang ---- dia sedang berenang\r\n",
      "sample:  buku hilang dicari ilham ---- ilham sedang mencari bukunya yang hilang\r\n",
      "sample:  ibu sedang menyapu halaman rumah ---- ibu sedang menyapu halaman rumah\r\n",
      "sample:  padi ditanam ayah di sawah ---- ayah sedang membajak padi di sawah\r\n",
      "sample:  di tepi pantai, andi pergi ---- andi pergi berkemah di tepi pantai\r\n",
      "bleu score on test dataset:  32.03378873198283\r\n",
      "evaluate on data b-salah-darat\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 32\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:03<00:00,  2.41it/s]\r\n",
      "sample:  di kantor pt pt angkasa pura ii, di bandara soekarnohatta, terjadi insiden terbang yang tiba di jakarta dari mei 2016 ---- kantor imigrasi bandara soekarno-hatta bersama kantor otoritas bandara soekarno-hatta dan pt angkasa pura ii membahas insiden penumpang penerbangan pesawat lion air jt 161 yang tiba di jakarta dari singapura 10 mei 2016.\r\n",
      "sample:  di kantor imigrasi soekarnohatta, kami bertemu pada sabtu ini ---- kami meeting soal ini,\" ujar kepala kantor imigrasi soekarno-hatta alif suadi kepada tempo, sabtu malam ini, 14 mei 2016.\r\n",
      "sample:  anak natalie berangkat ke pesawat lion air jt 161 pada pukul 18 55 ---- anak temannya natalie berangkat dari singapura dan menggunakan pesawat lion air jt 161, pada pukul 18.50.\r\n",
      "sample:  direktur haryadi seorang perusahaan pt angkasa pura ii yang melakukan laku kelola bandara internasional soekarnohatta ---- sekretaris perusahaan pt angkasa pura ii agus haryadi mengatakan selaku pengelola bandara internasional soekarno-hatta, pihaknya akan berkoordinasi dengan kantor otoritas bandara wilayah i terkait dengan peristiwa ini.\r\n",
      "sample:  pesawat parkir itu disiapkan oleh kami di remote area ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "sample:  dari padang, edward mendapatkan pekerjaan sebagai sopir jemput bus ---- terdapat sopir bus yang akan menjemput penumpang dari padang\" ujar edward.\r\n",
      "sample:  pihak bandara soetta yang memanggil pihak kait-nasional ---- pihak otoritas bandara soetta akan memanggil pihak terkait dan dilakukan investigasi.\r\n",
      "sample:  kemenhub memberikan sanksi kepada pihakpihak yang lalai ---- kemenhub memberikan sanksi kepada pihak-pihak yang lalai,\" ujarnya.\r\n",
      "sample:  zara menulis cerita temannya di terminal ii ---- pesawat itu mendarat tidak di terminal ii, kata zara mengutip cerita temannya.\r\n",
      "sample:  pesawat parkir itu disiapkan oleh kami di remote area ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "bleu score on test dataset:  9.879791521839332\r\n",
      "evaluate on data c-gedung-roboh\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 29\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:02<00:00,  2.92it/s]\r\n",
      "sample:  kendara itu bergerak semakin arah mal bintaro xchange di atas jalan boulevard bintaro ---- pengalihan dilakukan bagi pengendara yang melaju dari arah mal bintaro xchange dan tol bintaro menuju jalan boulevard bintaro dengan menggunakan flyover di atas lampu merah indomobil.\r\n",
      "sample:  kejadian ini terjadi ketika pembangunan gedung miring dan di jalan jakarta timur, gedung itu roboh ---- kegagalan struktur dan kesalahan tes tanah yang mengakibatkan gedung miring juga terjadi pada menara saidah di jalan mt haryono, jakarta timur.\r\n",
      "sample:  di tangerang selatan, terjadi peristiwa roboh gedung itu ---- kapolres tangerang selatan, akbp ayi supardan membenarkan peristiwa robohnya gedung di bintaro yang menarik perhatian masyarakat banyak.\r\n",
      "sample:  pembangunan gedung itu putus karena tidak lulus dengan layak ---- panin memutuskan tidak melanjutkan pembangunan gedung itu karena dinyatakan tidak lulus uji kelayakan.\r\n",
      "sample:  gedung itu dibangun dengan lulus terbaik dan pembangunan gedung itu ---- gedung tersebut dinyatakan tidak lulus uji sehingga pembangunan gedung diputuskan tidak dilanjutkan.\r\n",
      "sample:  kasat akp samian di polres tangerang selatan melakukan laku tahu di gedung itu saat ini ---- kasat reskrim polres tangerangg selatan, akp samian mengatakan saat ini pihaknya melakukan penyelidikan untuk mengetahui penyebab runtuhnya gedung.\r\n",
      "sample:  sejak ini, kami melihat peristiwa yang sebut ---- hingga kini kami masih mendalami peristiwa tersebut.\r\n",
      "sample:  kejadian ini terjadi ketika korban ---- kejadian tersebut tidak timbul korban.\r\n",
      "sample:  rugi material yang terjadi ---- terjadi kerugian material.\r\n",
      "sample:  di tangerang selatan, terjadi bongkar di gedung itu ---- pembongkaran gedung di sektor 7, bintaro, tangerang selatan, diduga menyalahi prosedur.\r\n",
      "bleu score on test dataset:  5.241249340945514\r\n",
      "evaluate on data d-indo-fuji\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 27\r\n",
      "  0%|                                                     | 0/7 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:03<00:00,  2.22it/s]\r\n",
      "sample:  sertifikat yang diberikan oleh indosat fujitsu indonesia kepada para mitra bangun rumah itu adalah dalam teknologi internet of things ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  sertifikat yang diberikan oleh indosat fujitsu indonesia kepada para mitra bangun rumah itu adalah dalam teknologi internet of things ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  teknologi iot yang mereka gunakan merupakan yang signifikan dan perbaikan akan berlangsung secara keseluruhan ---- smart mobility dan iot mengalami perkembangan cukup signifikan sehingga mengubah cara perusahaan dalam mengelola aset.\r\n",
      "sample:  sertifikat yang diberikan oleh indosat fujitsu indonesia kepada para mitra bangun rumah itu adalah dalam teknologi internet of things ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  di indonesia, kerja sama akan dilakukan di sektor industri itu ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  president sofwan memimpin layanan data realtime di indonesia dan mobilitas merupakan cara yang optimal dalam meningkatkan strategi usaha ---- mobilitas akan menyediakan cara untuk mengoptimalkan strategi perusahaan dengan memanfaatkan aplikasi dan layanan yang dapat memproses data real-time, tutur achmad sofwan, president director fujitsu indonesia.\r\n",
      "sample:  di indonesia, kerja sama akan dilakukan di sektor industri itu ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  indosat ooredoo yang hadir dalam solusi smart mobility di indonesia ---- indosat ooredoo bekerja sama dengan fujitsu indonesia menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  president sofwan merupakan director fujitsu dan mobilitas merupakan cara yang optimal dalam perbaikan data realtime ---- president director fujitsu, achmad sofwan mengungkap, mobilitas akan menyediakan cara mengoptimalisasikan strategi perusahaan dengan memanfaatkan aplikasi dan layanan dalam aktivitas memproses data real-time.\r\n",
      "sample:  di indonesia, kerja sama akan dilakukan di sektor industri itu ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "bleu score on test dataset:  1.7696994497955176\r\n",
      "evaluate on data f-bunuh-diri\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 23\r\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:03<00:00,  1.60it/s]\r\n",
      "sample:  di jembatan yang kira-rangi oleh 15 meter, ia melompat ---- ciptadi dengan nekat melompat dari megatron jembatan penyeberangan orang yang tingginya diperkirakan mencapai 15 meter.\r\n",
      "sample:  hal yang sebut terjadi di kasubag reny 01 di bandung pada saat temu jumat di mapolrestabes bandung ---- hal tersebut diungkapkan kabag ops polrestabes bandung kompol dede rojudin didampingi kasubag humas kompol reny marthaliana, saat ditemui di mapolrestabes bandung, jumat.\r\n",
      "sample:  di bandung, rezha warga kota bandung dibela oleh noviana yang telah bubar di masjid raya bandung jumat ini ---- rezha noviana, warga kota bandung mengatakan ia dan jemaat lainnya langsung memperhatikan megatron jembatan penyeberangan orang depan kantor pos besar bandung setelah bubar salat jumat di masjid raya bandung.\r\n",
      "sample:  satpam linmas berdiri di atas jpo ---- ketika itu, petugas linmas dan beberapa satpam berdiri di atas jpo.\r\n",
      "sample:  aksi bunuh diri terjadi di alun bandung pada saat ini ---- aksi bunuh diri di alun - alun bandung belum lama ini terjadi.\r\n",
      "sample:  di alun-alangi kerumunan orang tua, dirinya terjatuh ---- seorang pria tua nekat menjatuhkan diri dari jpo di alun - alun kota bandung.\r\n",
      "sample:  di alun bandung, seorang saksi mata meledakkan diri di anggota linmas di kota bandung ---- sudirman, anggota linmas satpol pp kota bandung yang menjadi saksi mata aksi bunuh diri di alun - alun bandung itu mengatakan, pria tersebut diperkirakan berusia 50 tahun.\r\n",
      "sample:  di depan bandung, seorang orang itu terjatuh di jembatan seberang ---- seorang lelaki menjatuhkan diri dari megatron jembatan penyeberangan di depan pos giro besar bandung, bandung, jumat.\r\n",
      "sample:  di tengah malam ini, ia mengakhiri hidup di jalan asia afrika ---- seorang lelaki tengah baya nekat mengakhiri hidupnya di tengah kepadatan lalu lintas jalan asia afrika.\r\n",
      "sample:  di bandung, seorang pria misterius membuat jembatan di bandung barat, ia membuat pesan di seberang orang-orang ---- pria misterius yang bunuh diri dari jembatan penyeberangan orang di jalan asia afrika, bandung, jawa barat, membuat pesan sebelum nekat terjun.\r\n",
      "bleu score on test dataset:  8.354429364529265\r\n",
      "evaluate on data g-gempa-dieng\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 19\r\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:02<00:00,  2.44it/s]\r\n",
      "sample:  stasiun banjarnegara jawa tengah yang dibangun oleh badan meteorologi klimatologi geofisika pada pukul 19 00 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara, jawa tengah, menyatakan bahwa gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 pada skala richter.\r\n",
      "sample:  ada pusat gempa di desa kecamatan batang di kabupaten batang ---- pusat gempa diperkirakan berada di desa tanji gugur, kecamatan bawang, kabupaten batang.\r\n",
      "sample:  tutup sementara yang pvmbg umumkan di jalan dekat kawah di desa sumberejo ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  kepala vulkanologi bencana geologi berkata ada banyak gempa yang terjadi pukul 19 00 wib di dataran tinggi dieng ---- kepala pusat vulkanologi dan mitigasi bencana geologi surono mengatakan bahwa gempa yang terjadi di dataran tinggi dieng pukul 19.00 wib terekam sebanyak 86 kali.\r\n",
      "sample:  tutup dilakukan oleh tim ukur di lapangan hingga ada gas bahaya ---- penutupan ini dilakukan hingga tim di lapangan yang mengukur konsentrasi gas menyatakan tidak ada gas yang berbahaya.\r\n",
      "sample:  tutup sementara yang pvmbg umumkan di jalan dekat kawah di desa sumberejo ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  di pusat vulkanologi bencana geologi, terjadi peningkatan aktivitas kawah timbang dan pemerintah memberikan informasi ---- petugas pusat vulkanologi dan mitigasi bencana geologi diterjunkan untuk memantau aktivitas kawah timbang dan memberikan informasi.\r\n",
      "sample:  di pusat vulkanologi mitigasi bencana geologi, terjadi gempa dan gempa terjadi sejak pukul 19 00 wib ---- data yang dikeluarkan dari pusat vulkanologi dan mitigasi bencana geologi menyebutkan gempa mulai terasa pukul 19.00 wib dan telah terekam gempa sebanyak 86 kali.\r\n",
      "sample:  pengungsi mengungsi ke kabupaten banjarnegara pada sabtu pagi ---- pengungsi di kabupaten banjarnegara, pada sabtu pagi mulai meninggalkan pengungsian.\r\n",
      "sample:  stasiun meteorologi klimatologi geofisika yang melakukan guncang gempa kuat 4 skala richter pada pukul 19 00 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara menyatakan gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 skala richter *(sr)*.\r\n",
      "bleu score on test dataset:  11.91103574344951\r\n"
     ]
    }
   ],
   "source": [
    "!./evaluate_indoBART_to_all.sh result linearized_penman_with_tree_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2279280",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T08:14:40.612665Z",
     "iopub.status.busy": "2022-05-18T08:14:40.607281Z",
     "iopub.status.idle": "2022-05-18T08:15:10.124341Z",
     "shell.execute_reply": "2022-05-18T08:15:10.123823Z"
    },
    "papermill": {
     "duration": 31.961268,
     "end_time": "2022-05-18T08:15:10.124478",
     "exception": false,
     "start_time": "2022-05-18T08:14:38.163210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result/ (stored 0%)\r\n",
      "  adding: result/amr_simple_test/ (stored 0%)\r\n",
      "  adding: result/amr_simple_test/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/amr_simple_test/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/amr_simple_test/test_label.txt (deflated 60%)\r\n",
      "  adding: result/b-salah-darat/ (stored 0%)\r\n",
      "  adding: result/b-salah-darat/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/b-salah-darat/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/b-salah-darat/test_label.txt (deflated 66%)\r\n",
      "  adding: result/model/ (stored 0%)\r\n",
      "  adding: result/model/config.json (deflated 63%)\r\n",
      "  adding: result/model/pytorch_model.bin (deflated 8%)\r\n",
      "  adding: result/g-gempa-dieng/ (stored 0%)\r\n",
      "  adding: result/g-gempa-dieng/test_generations.txt (deflated 64%)\r\n",
      "  adding: result/g-gempa-dieng/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/g-gempa-dieng/test_label.txt (deflated 67%)\r\n",
      "  adding: result/f-bunuh-diri/ (stored 0%)\r\n",
      "  adding: result/f-bunuh-diri/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/f-bunuh-diri/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/f-bunuh-diri/test_label.txt (deflated 68%)\r\n",
      "  adding: result/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/loss_data.tsv (deflated 34%)\r\n",
      "  adding: result/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/test_label.txt (deflated 60%)\r\n",
      "  adding: result/c-gedung-roboh/ (stored 0%)\r\n",
      "  adding: result/c-gedung-roboh/test_generations.txt (deflated 61%)\r\n",
      "  adding: result/c-gedung-roboh/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/c-gedung-roboh/test_label.txt (deflated 62%)\r\n",
      "  adding: result/d-indo-fuji/ (stored 0%)\r\n",
      "  adding: result/d-indo-fuji/test_generations.txt (deflated 82%)\r\n",
      "  adding: result/d-indo-fuji/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/d-indo-fuji/test_label.txt (deflated 86%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r result_without_sta.zip result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd1b80",
   "metadata": {
    "papermill": {
     "duration": 2.492477,
     "end_time": "2022-05-18T08:15:15.012778",
     "exception": false,
     "start_time": "2022-05-18T08:15:12.520301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Finetune indobart with linearized penman + supervised task adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8859c66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T08:15:19.566385Z",
     "iopub.status.busy": "2022-05-18T08:15:19.565654Z",
     "iopub.status.idle": "2022-05-18T08:15:19.571827Z",
     "shell.execute_reply": "2022-05-18T08:15:19.572499Z"
    },
    "papermill": {
     "duration": 2.383752,
     "end_time": "2022-05-18T08:15:19.572681",
     "exception": false,
     "start_time": "2022-05-18T08:15:17.188929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/train\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/amr-to-text-indonesia/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9adf503e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T08:15:24.470078Z",
     "iopub.status.busy": "2022-05-18T08:15:24.469120Z",
     "iopub.status.idle": "2022-05-18T08:15:25.303227Z",
     "shell.execute_reply": "2022-05-18T08:15:25.302222Z"
    },
    "papermill": {
     "duration": 3.339369,
     "end_time": "2022-05-18T08:15:25.303418",
     "exception": false,
     "start_time": "2022-05-18T08:15:21.964049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'result/model': Is a directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm result/* -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ceeb14c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T08:15:30.051416Z",
     "iopub.status.busy": "2022-05-18T08:15:30.050635Z",
     "iopub.status.idle": "2022-05-18T09:08:43.905654Z",
     "shell.execute_reply": "2022-05-18T09:08:43.906076Z"
    },
    "papermill": {
     "duration": 3196.03413,
     "end_time": "2022-05-18T09:08:43.906257",
     "exception": false,
     "start_time": "2022-05-18T08:15:27.872127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing method linearized_penman\r\n",
      "silver data folder ../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/preprocessed_silver_data/train.amr.txt', result_sent_path='data/preprocessed_silver_data/train.sent.txt', source_file_path=None, source_folder_path='../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news')\r\n",
      "100%|███████████████████████████████| 269258/269258 [00:01<00:00, 145088.49it/s]\r\n",
      "100%|███████████████████████████████| 264330/264330 [00:02<00:00, 124836.06it/s]\r\n",
      "100%|███████████████████████████████| 268572/268572 [00:01<00:00, 145327.77it/s]\r\n",
      "100%|███████████████████████████████| 263263/263263 [00:01<00:00, 145969.55it/s]\r\n",
      "100%|███████████████████████████████| 263508/263508 [00:01<00:00, 145806.48it/s]\r\n",
      "100%|███████████████████████████████| 265325/265325 [00:01<00:00, 146055.26it/s]\r\n",
      "100%|███████████████████████████████| 268262/268262 [00:01<00:00, 142445.28it/s]\r\n",
      "100%|███████████████████████████████| 266966/266966 [00:02<00:00, 127828.10it/s]\r\n",
      "100%|███████████████████████████████| 270251/270251 [00:01<00:00, 144651.92it/s]\r\n",
      "100%|███████████████████████████████| 264253/264253 [00:01<00:00, 146796.69it/s]\r\n",
      "total: 154892 pair sent-amr\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  154892\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  38723\r\n",
      "(Epoch 1) TRAIN LOSS:1.4159 LR:0.00010000: 100%|█| 38723/38723 [52:22<00:00, 12.\r\n",
      "(Epoch 1) DEV LOSS:3.6840 LR:0.00010000: 100%|████| 5/5 [00:00<00:00, 55.48it/s]\r\n",
      "bleu score on dev:  10.863122861332448\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:09<00:00,  7.85it/s]\r\n",
      "sample:  ilham tiup balon. ---- balon itu ditiup ilham\r\n",
      "sample:  obe menulis puisi. ---- obe menulis puisi\r\n",
      "sample:  saya ketik makalahnya. ---- saya mengetik makalah\r\n",
      "sample:  angga anak keajaiban. ---- angga anak ajaib\r\n",
      "sample:  ibu itu orang dosen. ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  24.907649860217376\r\n",
      "tensor([[    0,   382,  9338, 40007,   382,   475,   384, 40008,   382,  6353,\r\n",
      "           384,   384,   384,     2, 40002]])\r\n",
      "saya ketik makalahnya.\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x train_indoBART_on_silver_data.sh\n",
    "!./train_indoBART_on_silver_data.sh linearized_penman ../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b19c5a1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T09:09:20.124393Z",
     "iopub.status.busy": "2022-05-18T09:09:20.123518Z",
     "iopub.status.idle": "2022-05-18T09:09:20.838099Z",
     "shell.execute_reply": "2022-05-18T09:09:20.837618Z"
    },
    "papermill": {
     "duration": 19.20907,
     "end_time": "2022-05-18T09:09:20.838239",
     "exception": false,
     "start_time": "2022-05-18T09:09:01.629169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu_score_test.txt  loss_data.tsv  model  test_generations.txt  test_label.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls result/result_supervised_task_adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b92bde1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T09:09:56.687117Z",
     "iopub.status.busy": "2022-05-18T09:09:56.682236Z",
     "iopub.status.idle": "2022-05-18T09:09:57.341619Z",
     "shell.execute_reply": "2022-05-18T09:09:57.340975Z"
    },
    "papermill": {
     "duration": 19.014933,
     "end_time": "2022-05-18T09:09:57.341756",
     "exception": false,
     "start_time": "2022-05-18T09:09:38.326823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir result/result_linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c6657d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T09:10:33.335504Z",
     "iopub.status.busy": "2022-05-18T09:10:33.334728Z",
     "iopub.status.idle": "2022-05-18T09:13:47.681840Z",
     "shell.execute_reply": "2022-05-18T09:13:47.681251Z"
    },
    "papermill": {
     "duration": 212.56598,
     "end_time": "2022-05-18T09:13:47.682001",
     "exception": false,
     "start_time": "2022-05-18T09:10:15.116021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "resume from checkpoint\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:1.3457 LR:0.00003000: 100%|█| 662/662 [00:41<00:00, 16.02it\r\n",
      "(Epoch 1) DEV LOSS:1.5065 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 58.49it/s]\r\n",
      "bleu score on dev:  22.490499728424783\r\n",
      "(Epoch 2) TRAIN LOSS:0.6498 LR:0.00003000: 100%|█| 662/662 [00:40<00:00, 16.50it\r\n",
      "(Epoch 2) DEV LOSS:1.3144 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 58.62it/s]\r\n",
      "bleu score on dev:  20.62066927765074\r\n",
      "(Epoch 3) TRAIN LOSS:0.4803 LR:0.00003000: 100%|█| 662/662 [00:40<00:00, 16.21it\r\n",
      "(Epoch 3) DEV LOSS:1.2749 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 58.01it/s]\r\n",
      "bleu score on dev:  36.66851200800482\r\n",
      "(Epoch 4) TRAIN LOSS:0.4026 LR:0.00003000: 100%|█| 662/662 [00:40<00:00, 16.31it\r\n",
      "(Epoch 4) DEV LOSS:1.2579 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 56.77it/s]\r\n",
      "bleu score on dev:  37.50513973757383\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:12<00:00,  6.08it/s]\r\n",
      "sample:  balon dimainkan oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis obe ---- obe menulis puisi\r\n",
      "sample:  saya mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  kebhinekaan anak ajaib ---- angga anak ajaib\r\n",
      "sample:  ibu adalah seorang dosen ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  31.907815142378\r\n",
      "tensor([[    0,   382,  9338, 40007,   382,   475,   384, 40008,   382,  6353,\r\n",
      "           384,   384,   384,     2, 40002]])\r\n",
      "saya mengetik makalah\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoBART.py --model_type indo-bart --n_epochs 4 --lr 3e-5 --num_beams 10 --data_folder ../data/preprocessed_data/linearized_penman  --result_folder result/result_linearized_penman --resume_from_checkpoint True --saved_model_folder_path result/result_supervised_task_adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f684f11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T09:14:26.265939Z",
     "iopub.status.busy": "2022-05-18T09:14:26.265055Z",
     "iopub.status.idle": "2022-05-18T09:14:55.228229Z",
     "shell.execute_reply": "2022-05-18T09:14:55.228725Z"
    },
    "papermill": {
     "duration": 48.459867,
     "end_time": "2022-05-18T09:14:55.228902",
     "exception": false,
     "start_time": "2022-05-18T09:14:06.769035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result/result_linearized_penman/ (stored 0%)\r\n",
      "  adding: result/result_linearized_penman/model/ (stored 0%)\r\n",
      "  adding: result/result_linearized_penman/model/config.json (deflated 63%)\r\n",
      "  adding: result/result_linearized_penman/model/pytorch_model.bin (deflated 8%)\r\n",
      "  adding: result/result_linearized_penman/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/result_linearized_penman/loss_data.tsv (deflated 33%)\r\n",
      "  adding: result/result_linearized_penman/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/result_linearized_penman/test_label.txt (deflated 60%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r result_with_sta.zip result/result_linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1efe1cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T09:15:33.902794Z",
     "iopub.status.busy": "2022-05-18T09:15:33.901955Z",
     "iopub.status.idle": "2022-05-18T09:15:33.905049Z",
     "shell.execute_reply": "2022-05-18T09:15:33.905502Z"
    },
    "papermill": {
     "duration": 19.488953,
     "end_time": "2022-05-18T09:15:33.905645",
     "exception": false,
     "start_time": "2022-05-18T09:15:14.416692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/evaluate\n"
     ]
    }
   ],
   "source": [
    "%cd ../evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8806570",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T09:16:11.823378Z",
     "iopub.status.busy": "2022-05-18T09:16:11.822623Z",
     "iopub.status.idle": "2022-05-18T09:16:12.483936Z",
     "shell.execute_reply": "2022-05-18T09:16:12.483245Z"
    },
    "papermill": {
     "duration": 19.907625,
     "end_time": "2022-05-18T09:16:12.484077",
     "exception": false,
     "start_time": "2022-05-18T09:15:52.576452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'result/amr_simple_test': Is a directory\r\n",
      "rm: cannot remove 'result/b-salah-darat': Is a directory\r\n",
      "rm: cannot remove 'result/c-gedung-roboh': Is a directory\r\n",
      "rm: cannot remove 'result/d-indo-fuji': Is a directory\r\n",
      "rm: cannot remove 'result/f-bunuh-diri': Is a directory\r\n",
      "rm: cannot remove 'result/g-gempa-dieng': Is a directory\r\n",
      "rm: cannot remove 'result/linearized_penman': Is a directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm result/* -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "827e0748",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T09:16:50.877400Z",
     "iopub.status.busy": "2022-05-18T09:16:50.876610Z",
     "iopub.status.idle": "2022-05-18T09:18:54.896897Z",
     "shell.execute_reply": "2022-05-18T09:18:54.896414Z"
    },
    "papermill": {
     "duration": 142.670156,
     "end_time": "2022-05-18T09:18:54.897038",
     "exception": false,
     "start_time": "2022-05-18T09:16:32.226882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model folder ../train/result/result_linearized_penman\r\n",
      "preprocessing method linearized_penman\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data’: File exists\r\n",
      "preprocess amr_simple_test\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/amr_simple_test’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/amr_simple_test/test.amr.txt', result_sent_path='data/test/preprocessed_data/amr_simple_test/test.sent.txt', source_file_path='data/test/amr_simple_test.txt', source_folder_path=None)\r\n",
      "100%|███████████████████████████████████| 2618/2618 [00:00<00:00, 174907.42it/s]\r\n",
      "total: 306 pair sent-amr\r\n",
      "preprocess b-salah-darat\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/b-salah-darat’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/b-salah-darat/test.amr.txt', result_sent_path='data/test/preprocessed_data/b-salah-darat/test.sent.txt', source_file_path='data/test/b-salah-darat.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 676/676 [00:00<00:00, 123571.56it/s]\r\n",
      "total: 32 pair sent-amr\r\n",
      "preprocess c-gedung-roboh\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/c-gedung-roboh’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/c-gedung-roboh/test.amr.txt', result_sent_path='data/test/preprocessed_data/c-gedung-roboh/test.sent.txt', source_file_path='data/test/c-gedung-roboh.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 606/606 [00:00<00:00, 121099.06it/s]\r\n",
      "total: 29 pair sent-amr\r\n",
      "preprocess d-indo-fuji\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/d-indo-fuji’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/d-indo-fuji/test.amr.txt', result_sent_path='data/test/preprocessed_data/d-indo-fuji/test.sent.txt', source_file_path='data/test/d-indo-fuji.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 745/745 [00:00<00:00, 122113.27it/s]\r\n",
      "total: 27 pair sent-amr\r\n",
      "preprocess f-bunuh-diri\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/f-bunuh-diri’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/f-bunuh-diri/test.amr.txt', result_sent_path='data/test/preprocessed_data/f-bunuh-diri/test.sent.txt', source_file_path='data/test/f-bunuh-diri.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 468/468 [00:00<00:00, 125187.13it/s]\r\n",
      "total: 23 pair sent-amr\r\n",
      "preprocess g-gempa-dieng\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/g-gempa-dieng’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/g-gempa-dieng/test.amr.txt', result_sent_path='data/test/preprocessed_data/g-gempa-dieng/test.sent.txt', source_file_path='data/test/g-gempa-dieng.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 412/412 [00:00<00:00, 116997.51it/s]\r\n",
      "total: 19 pair sent-amr\r\n",
      "mkdir: cannot create directory ‘result’: File exists\r\n",
      "evaluate on data amr_simple_test\r\n",
      "mkdir: cannot create directory ‘result/amr_simple_test’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 306\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:12<00:00,  6.00it/s]\r\n",
      "sample:  balon dimainkan oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis obe ---- obe menulis puisi\r\n",
      "sample:  saya mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  kebhinekaan anak ajaib ---- angga anak ajaib\r\n",
      "sample:  ibu adalah seorang dosen ---- ibuku seorang dosen\r\n",
      "sample:  dia sedang berenang ---- dia sedang berenang\r\n",
      "sample:  buku yang hilang dicari oleh ilham ---- ilham sedang mencari bukunya yang hilang\r\n",
      "sample:  ibu menyapu di halaman rumah ---- ibu sedang menyapu halaman rumah\r\n",
      "sample:  di sawah, ayah sedang menangi padi ---- ayah sedang membajak padi di sawah\r\n",
      "sample:  andi sedang pergi ke tepi pantai ---- andi pergi berkemah di tepi pantai\r\n",
      "bleu score on test dataset:  31.907815142378\r\n",
      "evaluate on data b-salah-darat\r\n",
      "mkdir: cannot create directory ‘result/b-salah-darat’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 32\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:04<00:00,  1.88it/s]\r\n",
      "sample:  kantor kantor kantor sama dan pt angkasa pura ii otoritas bandara soekarno-hatta membahas insiden tumpang penerbangan pesawat yang tiba di jakarta, minggu (16 mei). ---- kantor imigrasi bandara soekarno-hatta bersama kantor otoritas bandara soekarno-hatta dan pt angkasa pura ii membahas insiden penumpang penerbangan pesawat lion air jt 161 yang tiba di jakarta dari singapura 10 mei 2016.\r\n",
      "sample:  kepala kantor imigrasi soekarnohattabumn suadi berkata, kami meeting persoalan kepada tempo sabtu malam ini (14/8) ---- kami meeting soal ini,\" ujar kepala kantor imigrasi soekarno-hatta alif suadi kepada tempo, sabtu malam ini, 14 mei 2016.\r\n",
      "sample:  anak teman natereng dan menggunakan pesawat lion air (16) pada pukul 18,5 berangkat ke singapura ---- anak temannya natalie berangkat dari singapura dan menggunakan pesawat lion air jt 161, pada pukul 18.50.\r\n",
      "sample:  sekretaris agus haryadi dari perusahaan pt angkasa pura ii mengatakan, akan dilakukan koordinasi kantor otoritas bandara internasional soekarnohatta dengan pihak terkait (bes) ---- sekretaris perusahaan pt angkasa pura ii agus haryadi mengatakan selaku pengelola bandara internasional soekarno-hatta, pihaknya akan berkoordinasi dengan kantor otoritas bandara wilayah i terkait dengan peristiwa ini.\r\n",
      "sample:  di remote area, kami menyiapkan bus antar penumpang di terminal ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "sample:  edward yang mendapat sopir jemput penumpang di padang, ujar edward, ---- terdapat sopir bus yang akan menjemput penumpang dari padang\" ujar edward.\r\n",
      "sample:  pihak otoritas bandara soetta memanggil pihak terkait dan melakukan investigasi ---- pihak otoritas bandara soetta akan memanggil pihak terkait dan dilakukan investigasi.\r\n",
      "sample:  dinda memberikan sanksi kepada pihak-pihak yang lalai ---- kemenhub memberikan sanksi kepada pihak-pihak yang lalai,\" ujarnya.\r\n",
      "sample:  mengutip cerita temannya,emuka mengatakan pesawat itu tidak mendarat di terminal ii ---- pesawat itu mendarat tidak di terminal ii, kata zara mengutip cerita temannya.\r\n",
      "sample:  di remote area, kami menyiapkan bus antar penumpang di terminal ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "bleu score on test dataset:  18.829865145579763\r\n",
      "evaluate on data c-gedung-roboh\r\n",
      "mkdir: cannot create directory ‘result/c-gedung-roboh’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 29\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:03<00:00,  2.16it/s]\r\n",
      "sample:  pada saat ini, kendaraan melaju ke arah mal bintaro xchance dan tol bintaro menuju jalan boulevard bintaro dilakukan dengan menggunakan fly-over di atas lampu indomobil merah ---- pengalihan dilakukan bagi pengendara yang melaju dari arah mal bintaro xchange dan tol bintaro menuju jalan boulevard bintaro dengan menggunakan flyover di atas lampu merah indomobil.\r\n",
      "sample:  kegagalan dan kesalahan dalam memperbaiki tes tanah terjadi pada struktur di gedung miring menara jalan jakarta timur, mt haryono ---- kegagalan struktur dan kesalahan tes tanah yang mengakibatkan gedung miring juga terjadi pada menara saidah di jalan mt haryono, jakarta timur.\r\n",
      "sample:  kapolres ayi supardan akbp di tangerang selatan membenarkan peristiwa bintaro yang menimpa gedung itu menarik hati banyak masyarakat ---- kapolres tangerang selatan, akbp ayi supardan membenarkan peristiwa robohnya gedung di bintaro yang menarik perhatian masyarakat banyak.\r\n",
      "sample:  pembangunan gedung tersebut putus oleh panin karena dinyatakan tidak lulus uji kelayakan ---- panin memutuskan tidak melanjutkan pembangunan gedung itu karena dinyatakan tidak lulus uji kelayakan.\r\n",
      "sample:  gedung yang disebut itu dinyatakan tidak lulus ujian, sehingga pembangunan yang memutuskan tidak lanjut gedung ---- gedung tersebut dinyatakan tidak lulus uji sehingga pembangunan gedung diputuskan tidak dilanjutkan.\r\n",
      "sample:  di tangerang selatan, kasat akp samian dari polres tangerang selatan medan mengatakan, pihaknya saat ini mengetahui penyebab runtuhnya gedung ---- kasat reskrim polres tangerangg selatan, akp samian mengatakan saat ini pihaknya melakukan penyelidikan untuk mengetahui penyebab runtuhnya gedung.\r\n",
      "sample:  sampai ini, kami masih dalam peristiwa yang kami dapatkan ---- hingga kini kami masih mendalami peristiwa tersebut.\r\n",
      "sample:  kejadian itu tidak jadi timbul oleh korban ---- kejadian tersebut tidak timbul korban.\r\n",
      "sample:  kerugian material terjadi ---- terjadi kerugian material.\r\n",
      "sample:  di sektor bintaro, tangerang selatan 7 gedung itu diduga menyalahi prosedur ---- pembongkaran gedung di sektor 7, bintaro, tangerang selatan, diduga menyalahi prosedur.\r\n",
      "bleu score on test dataset:  20.92396993462541\r\n",
      "evaluate on data d-indo-fuji\r\n",
      "mkdir: cannot create directory ‘result/d-indo-fuji’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 27\r\n",
      "  0%|                                                     | 0/7 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:04<00:00,  1.71it/s]\r\n",
      "sample:  indosat sama perlunya kesepahaman indonesia di dalam rangka memperbaiki mitra pembangunan yang telah dibangun secara fisik dan pelanggan bisnis menandatangani mobility untuk menghadiri solusi reformasi internet of watch ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  indosat sama perlunya kesepahaman indonesia di dalam rangka memperbaiki mitra pembangunan yang telah dibangun secara fisik dan pelanggan bisnis menandatangani mobility untuk menghadiri solusi reformasi internet of watch ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  teknologi yang ada di dalam mobil-ity dan perorangan itu pengalaman yang cukup signifikan dan sampai dengan cara perusahaan yang mengelola aset ---- smart mobility dan iot mengalami perkembangan cukup signifikan sehingga mengubah cara perusahaan dalam mengelola aset.\r\n",
      "sample:  indosat sama perlunya kesepahaman indonesia di dalam rangka memperbaiki mitra pembangunan yang telah dibangun secara fisik dan pelanggan bisnis menandatangani mobility untuk menghadiri solusi reformasi internet of watch ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  kerja sama sektor transportasi dan otomotif di indonesia sedang terfokus pada sektor industri dan sebagainya ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  oleh karena itu, president achmad sofwan, kepada directorite indonesia, menuturkan kesediaan layanan yang proses data realence dan mobilitas dimanfaatkan dengan cara mengoptimalkan strategi perusahaan ---- mobilitas akan menyediakan cara untuk mengoptimalkan strategi perusahaan dengan memanfaatkan aplikasi dan layanan yang dapat memproses data real-time, tutur achmad sofwan, president director fujitsu indonesia.\r\n",
      "sample:  kerja sama sektor transportasi dan otomotif di indonesia sedang terfokus pada sektor industri dan sebagainya ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  indosatoran bekerja untuk menghadiri solusi smart mobility internet of england dan kerja sama pembangunan indonesia ---- indosat ooredoo bekerja sama dengan fujitsu indonesia menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  oleh karena itu, president achmad sofwan sebagai director imf mengungkapkan kesediaannya untuk memanfaatkan layanan yang dikerjakan oleh data realence dan perpustakaan menggunakan cara yang mengoptimalkan strategi usaha ---- president director fujitsu, achmad sofwan mengungkap, mobilitas akan menyediakan cara mengoptimalisasikan strategi perusahaan dengan memanfaatkan aplikasi dan layanan dalam aktivitas memproses data real-time.\r\n",
      "sample:  kerja sama sektor transportasi dan otomotif di indonesia sedang terfokus pada sektor industri dan sebagainya ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "bleu score on test dataset:  4.162887280543866\r\n",
      "evaluate on data f-bunuh-diri\r\n",
      "mkdir: cannot create directory ‘result/f-bunuh-diri’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 23\r\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:03<00:00,  1.70it/s]\r\n",
      "sample:  di minimarketron, jembatan yang tinggi diperkirakan mencapai 15 meter itu melompat oleh orang-orang ---- ciptadi dengan nekat melompat dari megatron jembatan penyeberangan orang yang tingginya diperkirakan mencapai 15 meter.\r\n",
      "sample:  hal yang disebut damping kasubag reny marthaliana dari humas kompol dede taktisudin dari kompol di bandung, diungkapkan kasubag bandung ---- hal tersebut diungkapkan kabag ops polrestabes bandung kompol dede rojudin didampingi kasubag humas kompol reny marthaliana, saat ditemui di mapolrestabes bandung, jumat.\r\n",
      "sample:  glennha warga kota bandung kepada noviana mengatakan telah membubarkan salat di masjid raya bandung jumat dan di depan kantor besar di jembatan penyeberangan orang ---- rezha noviana, warga kota bandung mengatakan ia dan jemaat lainnya langsung memperhatikan megatron jembatan penyeberangan orang depan kantor pos besar bandung setelah bubar salat jumat di masjid raya bandung.\r\n",
      "sample:  tugas linmas dan seberapa satpam berdiri di atas jpo ---- ketika itu, petugas linmas dan beberapa satpam berdiri di atas jpo.\r\n",
      "sample:  aksi bunuh diri terjadi di bandung alun sejak lama ---- aksi bunuh diri di alun - alun bandung belum lama ini terjadi.\r\n",
      "sample:  orang tua itu menjatuhkan diri jpo di kota bandung ---- seorang pria tua nekat menjatuhkan diri dari jpo di alun - alun kota bandung.\r\n",
      "sample:  sudirman yang menjadi saksi mata pembunuh di alun bandung alun alun berkata diri sebagai anggota linmas di kota bandung satpol pp, pria yang disebut 30 tahun ---- sudirman, anggota linmas satpol pp kota bandung yang menjadi saksi mata aksi bunuh diri di alun - alun bandung itu mengatakan, pria tersebut diperkirakan berusia 50 tahun.\r\n",
      "sample:  di depan bandung, lelaki dari bandung pos giro terbesar bandung di jembatan penyeberangan, menjatuhkan diri jumat ---- seorang lelaki menjatuhkan diri dari megatron jembatan penyeberangan di depan pos giro besar bandung, bandung, jumat.\r\n",
      "sample:  di tengah baya, hidup diakhiri oleh lelaki itu ketika lalu lintas di tengah jalan asia afrika ---- seorang lelaki tengah baya nekat mengakhiri hidupnya di tengah kepadatan lalu lintas jalan asia afrika.\r\n",
      "sample:  pria misterius yang terbunuh di sekitar jembatan di bandung, jawa barat di asia barat di seberang orang membuat dirinya belum terjun nekat ---- pria misterius yang bunuh diri dari jembatan penyeberangan orang di jalan asia afrika, bandung, jawa barat, membuat pesan sebelum nekat terjun.\r\n",
      "bleu score on test dataset:  15.763465278398996\r\n",
      "evaluate on data g-gempa-dieng\r\n",
      "mkdir: cannot create directory ‘result/g-gempa-dieng’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 19\r\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:02<00:00,  1.86it/s]\r\n",
      "sample:  di banjarnegara, jawa tengah, badan meteorologi pemulihanatologi geofisika menyatakan gempa berkekuatan 48 skala richter mengguncang dataran tinggi tokyo pada pukul 19:01 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara, jawa tengah, menyatakan bahwa gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 pada skala richter.\r\n",
      "sample:  pusat gempa memperkirakan ada di desa di kecamatan batang, desa tanji gugur ---- pusat gempa diperkirakan berada di desa tanji gugur, kecamatan bawang, kabupaten batang.\r\n",
      "sample:  ashmbg merekomendasikan untuk menutup semua jalan yang mendekati kawah di desa sumberejo secara keseluruhan ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  kepala pusat vulkanologi pemulihan geologi mengatakan banyak 86 kali sejak gempa yang terjadi pukul 19,01 wib di dataran tinggi yogyakarta ---- kepala pusat vulkanologi dan mitigasi bencana geologi surono mengatakan bahwa gempa yang terjadi di dataran tinggi dieng pukul 19.00 wib terekam sebanyak 86 kali.\r\n",
      "sample:  pada saat ini, tim yang mengukur konsentrasi gas dilakukan di lapangan untuk tidak ada gas berbahaya ---- penutupan ini dilakukan hingga tim di lapangan yang mengukur konsentrasi gas menyatakan tidak ada gas yang berbahaya.\r\n",
      "sample:  ashmbg merekomendasikan untuk menutup semua jalan yang mendekati kawah di desa sumberejo secara keseluruhan ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  tugas pusat vulkanologiepsi bencana geologi dan memantau aktivitas kawah pertimbangan dan memberikan informasi ---- petugas pusat vulkanologi dan mitigasi bencana geologi diterjunkan untuk memantau aktivitas kawah timbang dan memberikan informasi.\r\n",
      "sample:  pada saat ini, data yang dikeluarkan pusat vulkanologi trading menyebutkan dan telah beberapa kali rekaman gempa sebanyak 18 kali akan dimulai pada pukul 19:0 wib ---- data yang dikeluarkan dari pusat vulkanologi dan mitigasi bencana geologi menyebutkan gempa mulai terasa pukul 19.00 wib dan telah terekam gempa sebanyak 86 kali.\r\n",
      "sample:  pengungsi yang mengungsi ke kabupaten banjarnegara sejak sabtu pagi ---- pengungsi di kabupaten banjarnegara, pada sabtu pagi mulai meninggalkan pengungsian.\r\n",
      "sample:  di banjarnegara, badan meteorologi sekurangatologi geofisika di geofisika menyatakan gempa berkekuatan 4,8 skala richter mengguncang dataran tinggi dieng pada pukul 19:0 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara menyatakan gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 skala richter *(sr)*.\r\n",
      "bleu score on test dataset:  18.21225749120622\r\n",
      "mkdir: cannot create directory ‘result/linearized_penman’: File exists\r\n",
      "mv: cannot move 'result/amr_simple_test' to 'result/linearized_penman/amr_simple_test': Directory not empty\r\n",
      "mv: cannot move 'result/b-salah-darat' to 'result/linearized_penman/b-salah-darat': Directory not empty\r\n",
      "mv: cannot move 'result/c-gedung-roboh' to 'result/linearized_penman/c-gedung-roboh': Directory not empty\r\n",
      "mv: cannot move 'result/d-indo-fuji' to 'result/linearized_penman/d-indo-fuji': Directory not empty\r\n",
      "mv: cannot move 'result/f-bunuh-diri' to 'result/linearized_penman/f-bunuh-diri': Directory not empty\r\n",
      "mv: cannot move 'result/g-gempa-dieng' to 'result/linearized_penman/g-gempa-dieng': Directory not empty\r\n",
      "mv: cannot move 'result/linearized_penman' to a subdirectory of itself, 'result/linearized_penman/linearized_penman'\r\n"
     ]
    }
   ],
   "source": [
    "!./evaluate_indoBART_to_all.sh ../train/result/result_linearized_penman linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84595ab7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T09:19:33.185841Z",
     "iopub.status.busy": "2022-05-18T09:19:33.185003Z",
     "iopub.status.idle": "2022-05-18T09:19:33.974779Z",
     "shell.execute_reply": "2022-05-18T09:19:33.974245Z"
    },
    "papermill": {
     "duration": 19.821716,
     "end_time": "2022-05-18T09:19:33.974916",
     "exception": false,
     "start_time": "2022-05-18T09:19:14.153200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result/ (stored 0%)\r\n",
      "  adding: result/amr_simple_test/ (stored 0%)\r\n",
      "  adding: result/amr_simple_test/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/amr_simple_test/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/amr_simple_test/test_label.txt (deflated 60%)\r\n",
      "  adding: result/b-salah-darat/ (stored 0%)\r\n",
      "  adding: result/b-salah-darat/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/b-salah-darat/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/b-salah-darat/test_label.txt (deflated 66%)\r\n",
      "  adding: result/g-gempa-dieng/ (stored 0%)\r\n",
      "  adding: result/g-gempa-dieng/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/g-gempa-dieng/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/g-gempa-dieng/test_label.txt (deflated 67%)\r\n",
      "  adding: result/f-bunuh-diri/ (stored 0%)\r\n",
      "  adding: result/f-bunuh-diri/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/f-bunuh-diri/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/f-bunuh-diri/test_label.txt (deflated 68%)\r\n",
      "  adding: result/linearized_penman/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/test_label.txt (deflated 60%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/test_generations.txt (deflated 66%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/test_label.txt (deflated 66%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/test_generations.txt (deflated 66%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/test_label.txt (deflated 67%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/test_generations.txt (deflated 65%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/test_label.txt (deflated 68%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/bleu_score_test.txt (deflated 12%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/test_label.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/test_generations.txt (deflated 86%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/test_label.txt (deflated 86%)\r\n",
      "  adding: result/c-gedung-roboh/ (stored 0%)\r\n",
      "  adding: result/c-gedung-roboh/test_generations.txt (deflated 60%)\r\n",
      "  adding: result/c-gedung-roboh/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/c-gedung-roboh/test_label.txt (deflated 62%)\r\n",
      "  adding: result/d-indo-fuji/ (stored 0%)\r\n",
      "  adding: result/d-indo-fuji/test_generations.txt (deflated 82%)\r\n",
      "  adding: result/d-indo-fuji/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/d-indo-fuji/test_label.txt (deflated 86%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r result_with_sta.zip result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1146bdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T09:20:11.853027Z",
     "iopub.status.busy": "2022-05-18T09:20:11.852290Z",
     "iopub.status.idle": "2022-05-18T09:20:12.582176Z",
     "shell.execute_reply": "2022-05-18T09:20:12.580547Z"
    },
    "papermill": {
     "duration": 19.526123,
     "end_time": "2022-05-18T09:20:12.582380",
     "exception": false,
     "start_time": "2022-05-18T09:19:53.056257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate_indoBART.py\t     result_without_sta.zip\r\n",
      "evaluate_indoBART_to_all.sh  zeroshot_evaluate_indoBART.py\r\n",
      "evaluate_indoT5.py\t     zeroshot_evaluate_indoBART_to_all.sh\r\n",
      "evaluate_indoT5_to_all.sh    zeroshot_evaluate_indoT5.py\r\n",
      "result\t\t\t     zeroshot_evaluate_indoT5_to_all.sh\r\n",
      "result_with_sta.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78234db",
   "metadata": {
    "papermill": {
     "duration": 19.341253,
     "end_time": "2022-05-18T09:20:51.270391",
     "exception": false,
     "start_time": "2022-05-18T09:20:31.929138",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## linearized penman + tree level embeddings + supervised task adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68555c9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T09:21:29.415739Z",
     "iopub.status.busy": "2022-05-18T09:21:29.414887Z",
     "iopub.status.idle": "2022-05-18T09:21:29.418251Z",
     "shell.execute_reply": "2022-05-18T09:21:29.418707Z"
    },
    "papermill": {
     "duration": 19.395143,
     "end_time": "2022-05-18T09:21:29.418847",
     "exception": false,
     "start_time": "2022-05-18T09:21:10.023704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/tree_level_embeddings\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/amr-to-text-indonesia/tree_level_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5771688f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T09:22:07.488725Z",
     "iopub.status.busy": "2022-05-18T09:22:07.487809Z",
     "iopub.status.idle": "2022-05-18T09:22:08.188472Z",
     "shell.execute_reply": "2022-05-18T09:22:08.187920Z"
    },
    "papermill": {
     "duration": 19.979515,
     "end_time": "2022-05-18T09:22:08.188642",
     "exception": false,
     "start_time": "2022-05-18T09:21:48.209127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'result/amr_simple_test': Is a directory\r\n",
      "rm: cannot remove 'result/b-salah-darat': Is a directory\r\n",
      "rm: cannot remove 'result/c-gedung-roboh': Is a directory\r\n",
      "rm: cannot remove 'result/d-indo-fuji': Is a directory\r\n",
      "rm: cannot remove 'result/f-bunuh-diri': Is a directory\r\n",
      "rm: cannot remove 'result/g-gempa-dieng': Is a directory\r\n",
      "rm: cannot remove 'result/model': Is a directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm result/* -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4709d31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T09:22:46.500683Z",
     "iopub.status.busy": "2022-05-18T09:22:46.499930Z",
     "iopub.status.idle": "2022-05-18T10:30:36.336314Z",
     "shell.execute_reply": "2022-05-18T10:30:36.335796Z"
    },
    "papermill": {
     "duration": 4088.871921,
     "end_time": "2022-05-18T10:30:36.336452",
     "exception": false,
     "start_time": "2022-05-18T09:22:27.464531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing method linearized_penman_with_tree_level\r\n",
      "silver data folder ../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news\r\n",
      "mkdir: cannot create directory ‘data/preprocessed_silver_data’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/preprocessed_silver_data/train.amr.txt', result_sent_path='data/preprocessed_silver_data/train.sent.txt', source_file_path=None, source_folder_path='../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news')\r\n",
      "100%|███████████████████████████████| 269258/269258 [00:01<00:00, 144704.58it/s]\r\n",
      "15062\r\n",
      "100%|███████████████████████████████| 264330/264330 [00:01<00:00, 139187.61it/s]\r\n",
      "15942\r\n",
      "100%|███████████████████████████████| 268572/268572 [00:01<00:00, 145314.93it/s]\r\n",
      "14937\r\n",
      "100%|███████████████████████████████| 263263/263263 [00:01<00:00, 145629.74it/s]\r\n",
      "15995\r\n",
      "100%|███████████████████████████████| 263508/263508 [00:02<00:00, 129176.36it/s]\r\n",
      "15926\r\n",
      "100%|███████████████████████████████| 265325/265325 [00:01<00:00, 145764.85it/s]\r\n",
      "16077\r\n",
      "100%|███████████████████████████████| 268262/268262 [00:01<00:00, 145253.59it/s]\r\n",
      "14991\r\n",
      "100%|███████████████████████████████| 266966/266966 [00:02<00:00, 119215.45it/s]\r\n",
      "14930\r\n",
      "100%|███████████████████████████████| 270251/270251 [00:01<00:00, 141126.46it/s]\r\n",
      "14986\r\n",
      "100%|███████████████████████████████| 264253/264253 [00:01<00:00, 140924.77it/s]\r\n",
      "16046\r\n",
      "total: 154892  tuple_sent_amr_level\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "Some weights of MBartForConditionalGeneration were not initialized from the model checkpoint at indobenchmark/indobart and are newly initialized: ['model.encoder.tree_embed.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  154892\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  38723\r\n",
      "(Epoch 1) TRAIN LOSS:1.0842 LR:0.00003000: 100%|█| 38723/38723 [1:06:39<00:00,  \r\n",
      "(Epoch 1) DEV LOSS:3.3892 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 36.64it/s]\r\n",
      "bleu score on dev:  17.203218455996083\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:14<00:00,  5.25it/s]\r\n",
      "sample:  ilham bergemuruh balonnya. ---- balon itu ditiup ilham\r\n",
      "sample:  obe menulis puisi. ---- obe menulis puisi\r\n",
      "sample:  saya ketik makalahnya. ---- saya mengetik makalah\r\n",
      "sample:  angga itu anak ajaib. ---- angga anak ajaib\r\n",
      "sample:  ibu mereka orang dosen. ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  30.169099211898043\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x train_indoBART_on_silver_data.sh\n",
    "!./train_indoBART_on_silver_data.sh linearized_penman_with_tree_level ../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7395706",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T10:31:49.275060Z",
     "iopub.status.busy": "2022-05-18T10:31:49.270215Z",
     "iopub.status.idle": "2022-05-18T10:35:27.316688Z",
     "shell.execute_reply": "2022-05-18T10:35:27.315467Z"
    },
    "papermill": {
     "duration": 254.968899,
     "end_time": "2022-05-18T10:35:27.316878",
     "exception": false,
     "start_time": "2022-05-18T10:31:12.347979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "resume from checkpoint\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:0.9251 LR:0.00003000: 100%|█| 662/662 [00:47<00:00, 14.05it\r\n",
      "(Epoch 1) DEV LOSS:1.0424 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 40.21it/s]\r\n",
      "bleu score on dev:  31.438792882152384\r\n",
      "(Epoch 2) TRAIN LOSS:0.4822 LR:0.00003000: 100%|█| 662/662 [00:47<00:00, 14.05it\r\n",
      "(Epoch 2) DEV LOSS:1.0753 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 41.22it/s]\r\n",
      "bleu score on dev:  33.28966422689121\r\n",
      "(Epoch 3) TRAIN LOSS:0.4044 LR:0.00003000: 100%|█| 662/662 [00:46<00:00, 14.14it\r\n",
      "(Epoch 3) DEV LOSS:0.8161 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 40.85it/s]\r\n",
      "bleu score on dev:  50.75455023324016\r\n",
      "(Epoch 4) TRAIN LOSS:0.3567 LR:0.00003000: 100%|█| 662/662 [00:46<00:00, 14.24it\r\n",
      "(Epoch 4) DEV LOSS:0.8409 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 39.80it/s]\r\n",
      "bleu score on dev:  48.176805119578084\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:13<00:00,  5.87it/s]\r\n",
      "sample:  balon dituang oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  saya sedang mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  badannya anak yang ajaib ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  33.457806366470486\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoBART.py --model_type indo-bart --n_epochs 4 --lr 3e-5 --num_beams 10 --data_folder ../data/preprocessed_data/linearized_penman_with_tree_level  --result_folder result --resume_from_checkpoint True --saved_model_folder_path result/result_supervised_task_adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "135241b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T10:36:40.987588Z",
     "iopub.status.busy": "2022-05-18T10:36:40.982297Z",
     "iopub.status.idle": "2022-05-18T10:38:46.158800Z",
     "shell.execute_reply": "2022-05-18T10:38:46.158148Z"
    },
    "papermill": {
     "duration": 161.951548,
     "end_time": "2022-05-18T10:38:46.158948",
     "exception": false,
     "start_time": "2022-05-18T10:36:04.207400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model folder result\r\n",
      "preprocessing method linearized_penman_with_tree_level\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data’: File exists\r\n",
      "preprocess amr_simple_test\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/amr_simple_test’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/amr_simple_test/test.amr.txt', result_sent_path='data/test/preprocessed_data/amr_simple_test/test.sent.txt', source_file_path='data/test/amr_simple_test.txt', source_folder_path=None)\r\n",
      "100%|███████████████████████████████████| 2618/2618 [00:00<00:00, 179440.60it/s]\r\n",
      "total: 306  tuple_sent_amr_level\r\n",
      "preprocess b-salah-darat\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/b-salah-darat’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/b-salah-darat/test.amr.txt', result_sent_path='data/test/preprocessed_data/b-salah-darat/test.sent.txt', source_file_path='data/test/b-salah-darat.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 676/676 [00:00<00:00, 126032.34it/s]\r\n",
      "total: 32  tuple_sent_amr_level\r\n",
      "preprocess c-gedung-roboh\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/c-gedung-roboh’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/c-gedung-roboh/test.amr.txt', result_sent_path='data/test/preprocessed_data/c-gedung-roboh/test.sent.txt', source_file_path='data/test/c-gedung-roboh.txt', source_folder_path=None)\r\n",
      "100%|██████████████████████████████████████| 606/606 [00:00<00:00, 99062.60it/s]\r\n",
      "total: 29  tuple_sent_amr_level\r\n",
      "preprocess d-indo-fuji\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/d-indo-fuji’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/d-indo-fuji/test.amr.txt', result_sent_path='data/test/preprocessed_data/d-indo-fuji/test.sent.txt', source_file_path='data/test/d-indo-fuji.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 745/745 [00:00<00:00, 123177.09it/s]\r\n",
      "total: 27  tuple_sent_amr_level\r\n",
      "preprocess f-bunuh-diri\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/f-bunuh-diri’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/f-bunuh-diri/test.amr.txt', result_sent_path='data/test/preprocessed_data/f-bunuh-diri/test.sent.txt', source_file_path='data/test/f-bunuh-diri.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 468/468 [00:00<00:00, 126047.28it/s]\r\n",
      "total: 23  tuple_sent_amr_level\r\n",
      "preprocess g-gempa-dieng\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/g-gempa-dieng’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/g-gempa-dieng/test.amr.txt', result_sent_path='data/test/preprocessed_data/g-gempa-dieng/test.sent.txt', source_file_path='data/test/g-gempa-dieng.txt', source_folder_path=None)\r\n",
      "100%|██████████████████████████████████████| 412/412 [00:00<00:00, 92059.73it/s]\r\n",
      "total: 19  tuple_sent_amr_level\r\n",
      "evaluate on data amr_simple_test\r\n",
      "mkdir: cannot create directory ‘result/amr_simple_test’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 306\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:12<00:00,  5.98it/s]\r\n",
      "sample:  balon dituang oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  saya sedang mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  badannya anak yang ajaib ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "sample:  dia sedang berenang ---- dia sedang berenang\r\n",
      "sample:  buku yang dicari oleh ilham ---- ilham sedang mencari bukunya yang hilang\r\n",
      "sample:  di halaman rumah, ibu sedang menyapu ---- ibu sedang menyapu halaman rumah\r\n",
      "sample:  padi yang ayah bajak di sawah ---- ayah sedang membajak padi di sawah\r\n",
      "sample:  andi pergi ke tepi pantai ---- andi pergi berkemah di tepi pantai\r\n",
      "bleu score on test dataset:  33.457806366470486\r\n",
      "evaluate on data b-salah-darat\r\n",
      "mkdir: cannot create directory ‘result/b-salah-darat’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 32\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:04<00:00,  1.86it/s]\r\n",
      "sample:  kantor bersama pt angkasa pura ii dan otoritas bandara soekarno-hatta membahas insiden penumpang penerbangan pesawat yang tiba di singapura 10 juni 2016 di lion air jt 161 ---- kantor imigrasi bandara soekarno-hatta bersama kantor otoritas bandara soekarno-hatta dan pt angkasa pura ii membahas insiden penumpang penerbangan pesawat lion air jt 161 yang tiba di jakarta dari singapura 10 mei 2016.\r\n",
      "sample:  kepala kantor imigrasi soekarnohatta alif suadi berkata, kami meeting soal kepada tempo sabtu malam ini (14 mei) 2016 ---- kami meeting soal ini,\" ujar kepala kantor imigrasi soekarno-hatta alif suadi kepada tempo, sabtu malam ini, 14 mei 2016.\r\n",
      "sample:  anak teman natalie berangkat dan pukul 18 (5) menggunakan pesawat lion air jt 161 ---- anak temannya natalie berangkat dari singapura dan menggunakan pesawat lion air jt 161, pada pukul 18.50.\r\n",
      "sample:  kata sekretaris pt angkasa pura ii, melakukan pengelolaan bandara internasional soekarnohatta, pihaknya akan berkoordinasi dengan kantor otoritas bandara wilayah 1 berkaitan perisitiwa ---- sekretaris perusahaan pt angkasa pura ii agus haryadi mengatakan selaku pengelola bandara internasional soekarno-hatta, pihaknya akan berkoordinasi dengan kantor otoritas bandara wilayah i terkait dengan peristiwa ini.\r\n",
      "sample:  di remote area, kami menyiapkan bus antar penumpang ke terminal ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "sample:  kata edward, terdapat beberapa sopir yang menjemput penumpang di padang ---- terdapat sopir bus yang akan menjemput penumpang dari padang\" ujar edward.\r\n",
      "sample:  pihak otoritas bandara soetta memanggil pihak terkait dan melakukan investigasi ---- pihak otoritas bandara soetta akan memanggil pihak terkait dan dilakukan investigasi.\r\n",
      "sample:  syifa memberikan sanksi kepada pihak-pihak yang lalai ---- kemenhub memberikan sanksi kepada pihak-pihak yang lalai,\" ujarnya.\r\n",
      "sample:  zara berkata, cerita temannya kepada temannya bahwa pesawat itu tidak mendarat di terminal ii ---- pesawat itu mendarat tidak di terminal ii, kata zara mengutip cerita temannya.\r\n",
      "sample:  di remote area, kami menyiapkan bus antar penumpang ke terminal ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "bleu score on test dataset:  22.738899721614107\r\n",
      "evaluate on data c-gedung-roboh\r\n",
      "mkdir: cannot create directory ‘result/c-gedung-roboh’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 29\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:03<00:00,  2.25it/s]\r\n",
      "sample:  pengalihan dilakukan kendaraan melaju ke arah mal bintaro dan tol bintaro menuju jalan boulevard bintaro dan bintaro dengan menggunakan flyover di atas lampu indomobil merah ---- pengalihan dilakukan bagi pengendara yang melaju dari arah mal bintaro xchange dan tol bintaro menuju jalan boulevard bintaro dengan menggunakan flyover di atas lampu merah indomobil.\r\n",
      "sample:  kegagalan dan kesalahan struktur terjadi di gedung miring di jakarta timur dan mt haryono di saidah ---- kegagalan struktur dan kesalahan tes tanah yang mengakibatkan gedung miring juga terjadi pada menara saidah di jalan mt haryono, jakarta timur.\r\n",
      "sample:  kapolres ayi supardan akbp di tangerang selatan membenarkan peristiwa di bintaro yang menimpa gedung itu menarik hati masyarakat banyak ---- kapolres tangerang selatan, akbp ayi supardan membenarkan peristiwa robohnya gedung di bintaro yang menarik perhatian masyarakat banyak.\r\n",
      "sample:  pembangunan gedung tersebut diputuskan oleh panin karena dinyatakan tidak lulus ujian kelayakan ---- panin memutuskan tidak melanjutkan pembangunan gedung itu karena dinyatakan tidak lulus uji kelayakan.\r\n",
      "sample:  gedung itu dinyatakan tidak lulus ujian sehingga pembangunan gedung itu memutuskan untuk tidak melanjutkan gedung itu ---- gedung tersebut dinyatakan tidak lulus uji sehingga pembangunan gedung diputuskan tidak dilanjutkan.\r\n",
      "sample:  kasat akp samian mengatakan, polres tangerang selatan melakukan penyidikan saat ini untuk mengetahui penyebab runtuhnya gedung tersebut ---- kasat reskrim polres tangerangg selatan, akp samian mengatakan saat ini pihaknya melakukan penyelidikan untuk mengetahui penyebab runtuhnya gedung.\r\n",
      "sample:  sampai ini, kami akan mendalami peristiwa yang disebutnya ---- hingga kini kami masih mendalami peristiwa tersebut.\r\n",
      "sample:  yang disebutnya, kejadian tersebut menimbulkan korban ---- kejadian tersebut tidak timbul korban.\r\n",
      "sample:  terjadi kerugian material ---- terjadi kerugian material.\r\n",
      "sample:  di sektor bintaro, tangerang selatan, gedung itu diduga bersalah oleh prosedur ---- pembongkaran gedung di sektor 7, bintaro, tangerang selatan, diduga menyalahi prosedur.\r\n",
      "bleu score on test dataset:  22.914456912345535\r\n",
      "evaluate on data d-indo-fuji\r\n",
      "mkdir: cannot create directory ‘result/d-indo-fuji’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 27\r\n",
      "  0%|                                                     | 0/7 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:03<00:00,  1.78it/s]\r\n",
      "sample:  indosat bersama fujitsu indonesia menandatangani nota kesepahaman di dalam rangka meningkatkan mitra kerja dan pembangunan internet of things di mobility ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  indosat bersama fujitsu indonesia menandatangani nota kesepahaman di dalam rangka meningkatkan mitra kerja dan pembangunan internet of things di mobility ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  teknologi mobility dan iot yang berkembang cukup signifikan hingga cara perusahaan yang mengelola aset ---- smart mobility dan iot mengalami perkembangan cukup signifikan sehingga mengubah cara perusahaan dalam mengelola aset.\r\n",
      "sample:  indosat bersama fujitsu indonesia menandatangani nota kesepahaman di dalam rangka meningkatkan mitra kerja dan pembangunan internet of things di mobility ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  kerja sama sektor transportasi dan otomotif yang terfokus pada sektor industri semata dan memenuhi kebutuhan pelanggan korporasi di indonesia ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  president achmad sofwan menuturkan, director fujitsu indonesia kesediaan untuk memanfaatkan teknologi realtime dan mobilitas dengan cara mengoptimalkan strategi perusahaan itu ---- mobilitas akan menyediakan cara untuk mengoptimalkan strategi perusahaan dengan memanfaatkan aplikasi dan layanan yang dapat memproses data real-time, tutur achmad sofwan, president director fujitsu indonesia.\r\n",
      "sample:  kerja sama sektor transportasi dan otomotif yang terfokus pada sektor industri semata dan memenuhi kebutuhan pelanggan korporasi di indonesia ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  indosat ooredoo hadir dalam solusi smart mobility dan internet of things bersama dengan fujitsu di indonesia ---- indosat ooredoo bekerja sama dengan fujitsu indonesia menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  president achmad sofwan mengungkapkan, director fujitsu bersedia memanfaatkan teknologi realtime dan mobilitas dengan cara mengoptimalkan strategi perusahaan itu ---- president director fujitsu, achmad sofwan mengungkap, mobilitas akan menyediakan cara mengoptimalisasikan strategi perusahaan dengan memanfaatkan aplikasi dan layanan dalam aktivitas memproses data real-time.\r\n",
      "sample:  kerja sama sektor transportasi dan otomotif yang terfokus pada sektor industri semata dan memenuhi kebutuhan pelanggan korporasi di indonesia ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "bleu score on test dataset:  16.24178880481732\r\n",
      "evaluate on data f-bunuh-diri\r\n",
      "mkdir: cannot create directory ‘result/f-bunuh-diri’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 23\r\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:03<00:00,  1.90it/s]\r\n",
      "sample:  syifaron melompat dari jembatan yang tinggi hingga mencapai 15 meter sebrang orang ---- ciptadi dengan nekat melompat dari megatron jembatan penyeberangan orang yang tingginya diperkirakan mencapai 15 meter.\r\n",
      "sample:  hal yang disebutnya damping kasubag reny marthaliana kepada humas kompol rojudin kompol ditemui di polrestabes bandung pada jumat di mapolrestabes di bandung ---- hal tersebut diungkapkan kabag ops polrestabes bandung kompol dede rojudin didampingi kasubag humas kompol reny marthaliana, saat ditemui di mapolrestabes bandung, jumat.\r\n",
      "sample:  rezha warga kota bandung kepada noviana mengatakan, ia telah salat di masjid raya bandung dan para jemaat lainnya di depan kantor pos besar di seberang orang-orang ---- rezha noviana, warga kota bandung mengatakan ia dan jemaat lainnya langsung memperhatikan megatron jembatan penyeberangan orang depan kantor pos besar bandung setelah bubar salat jumat di masjid raya bandung.\r\n",
      "sample:  di atas jpo, petugas linmas berdiri dan satpam ---- ketika itu, petugas linmas dan beberapa satpam berdiri di atas jpo.\r\n",
      "sample:  aksi bunuh diri terjadi di alun alun belum lama ---- aksi bunuh diri di alun - alun bandung belum lama ini terjadi.\r\n",
      "sample:  di alun alun, orang tua terjatuh oleh jpo ---- seorang pria tua nekat menjatuhkan diri dari jpo di alun - alun kota bandung.\r\n",
      "sample:  di alun, seorang saksi mata berkata, dirinya dibunuh oleh seorang anggota linmas di kota bandung oleh satpol pp ketika itu, pria yang disebutnya berusia 30 tahun ---- sudirman, anggota linmas satpol pp kota bandung yang menjadi saksi mata aksi bunuh diri di alun - alun bandung itu mengatakan, pria tersebut diperkirakan berusia 50 tahun.\r\n",
      "sample:  di depan bandung, lelaki itu terjatuh oleh dirinya di pos giro besar bandung di jembatan penyeberangan pada jumat ---- seorang lelaki menjatuhkan diri dari megatron jembatan penyeberangan di depan pos giro besar bandung, bandung, jumat.\r\n",
      "sample:  di tengah baya, lelaki itu mengakhiri hidup di tengah kepadatan lalu lintas di jalan asia afrika ---- seorang lelaki tengah baya nekat mengakhiri hidupnya di tengah kepadatan lalu lintas jalan asia afrika.\r\n",
      "sample:  pria misterius yang membunuh di jembatan di bandung, jawa barat, di seberang orang-orang membuat pesanan ketika belum terjun nekat ---- pria misterius yang bunuh diri dari jembatan penyeberangan orang di jalan asia afrika, bandung, jawa barat, membuat pesan sebelum nekat terjun.\r\n",
      "bleu score on test dataset:  16.862525258324677\r\n",
      "evaluate on data g-gempa-dieng\r\n",
      "mkdir: cannot create directory ‘result/g-gempa-dieng’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 19\r\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:02<00:00,  1.82it/s]\r\n",
      "sample:  di banjarnegara, jawa tengah, badan meteorologi klimatologi geofisika menyatakan, gempa berkekuatan 4 skala richter mengguncang dataran tinggi dieng pukul 19 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara, jawa tengah, menyatakan bahwa gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 pada skala richter.\r\n",
      "sample:  di kecamatan batang, pusat gempa diperkirakan ada di desa tanji gugur ---- pusat gempa diperkirakan berada di desa tanji gugur, kecamatan bawang, kabupaten batang.\r\n",
      "sample:  pvmbg memilih untuk menutup sementara jalan yang berdekatan dengan kawah di desa sumberejo secara keseluruhan ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  kepala pusat vulkanologi mitigasi bencana geologi mengatakan, sebanyak 86 kali gempa terjadi pada pukul 19 wib di dataran tinggi dieng ---- kepala pusat vulkanologi dan mitigasi bencana geologi surono mengatakan bahwa gempa yang terjadi di dataran tinggi dieng pukul 19.00 wib terekam sebanyak 86 kali.\r\n",
      "sample:  pada penutupan dilakukan hingga tim ukur gas di lapangan, tidak ada gas berbahaya ---- penutupan ini dilakukan hingga tim di lapangan yang mengukur konsentrasi gas menyatakan tidak ada gas yang berbahaya.\r\n",
      "sample:  pvmbg memilih untuk menutup sementara jalan yang berdekatan dengan kawah di desa sumberejo secara keseluruhan ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  petugas pusat vulkanologi mitigasi bencana geologi menerjunkan untuk memantau aktivitas kawah timbang dan memberikan informasi ---- petugas pusat vulkanologi dan mitigasi bencana geologi diterjunkan untuk memantau aktivitas kawah timbang dan memberikan informasi.\r\n",
      "sample:  data yang dikeluarkan pusat vulkanologi mitigasi bencana geologi menyebutkan, gempa terasa banyak kali dan gempa dirasakan mulai pukul 19 00 wib ---- data yang dikeluarkan dari pusat vulkanologi dan mitigasi bencana geologi menyebutkan gempa mulai terasa pukul 19.00 wib dan telah terekam gempa sebanyak 86 kali.\r\n",
      "sample:  pengungsi meninggal di kabupaten banjarnegara pada sabtu pagi ---- pengungsi di kabupaten banjarnegara, pada sabtu pagi mulai meninggalkan pengungsian.\r\n",
      "sample:  di banjarnegara, badan meteorologi geofisika menyatakan, gempa berkekuatan 4 - 8 skala richter mengguncang dataran tinggi dieng pukul 19 00 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara menyatakan gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 skala richter *(sr)*.\r\n",
      "bleu score on test dataset:  26.295325038055655\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x ./evaluate_indoBART_to_all.sh\n",
    "!./evaluate_indoBART_to_all.sh result linearized_penman_with_tree_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4aa212e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T10:40:00.153561Z",
     "iopub.status.busy": "2022-05-18T10:40:00.152776Z",
     "iopub.status.idle": "2022-05-18T10:40:58.179281Z",
     "shell.execute_reply": "2022-05-18T10:40:58.180353Z"
    },
    "papermill": {
     "duration": 94.718911,
     "end_time": "2022-05-18T10:40:58.180738",
     "exception": false,
     "start_time": "2022-05-18T10:39:23.461827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result/ (stored 0%)\r\n",
      "  adding: result/amr_simple_test/ (stored 0%)\r\n",
      "  adding: result/amr_simple_test/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/amr_simple_test/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/amr_simple_test/test_label.txt (deflated 60%)\r\n",
      "  adding: result/b-salah-darat/ (stored 0%)\r\n",
      "  adding: result/b-salah-darat/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/b-salah-darat/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/b-salah-darat/test_label.txt (deflated 66%)\r\n",
      "  adding: result/model/ (stored 0%)\r\n",
      "  adding: result/model/config.json (deflated 63%)\r\n",
      "  adding: result/model/pytorch_model.bin (deflated 8%)\r\n",
      "  adding: result/g-gempa-dieng/ (stored 0%)\r\n",
      "  adding: result/g-gempa-dieng/test_generations.txt (deflated 64%)\r\n",
      "  adding: result/g-gempa-dieng/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/g-gempa-dieng/test_label.txt (deflated 67%)\r\n",
      "  adding: result/f-bunuh-diri/ (stored 0%)\r\n",
      "  adding: result/f-bunuh-diri/test_generations.txt (deflated 64%)\r\n",
      "  adding: result/f-bunuh-diri/bleu_score_test.txt (deflated 6%)\r\n",
      "  adding: result/f-bunuh-diri/test_label.txt (deflated 68%)\r\n",
      "  adding: result/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/loss_data.tsv (deflated 34%)\r\n",
      "  adding: result/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/test_label.txt (deflated 60%)\r\n",
      "  adding: result/result_supervised_task_adaptation/ (stored 0%)\r\n",
      "  adding: result/result_supervised_task_adaptation/model/ (stored 0%)\r\n",
      "  adding: result/result_supervised_task_adaptation/model/config.json (deflated 63%)\r\n",
      "  adding: result/result_supervised_task_adaptation/model/pytorch_model.bin (deflated 8%)\r\n",
      "  adding: result/result_supervised_task_adaptation/test_generations.txt (deflated 64%)\r\n",
      "  adding: result/result_supervised_task_adaptation/loss_data.tsv (deflated 4%)\r\n",
      "  adding: result/result_supervised_task_adaptation/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/result_supervised_task_adaptation/test_label.txt (deflated 60%)\r\n",
      "  adding: result/c-gedung-roboh/ (stored 0%)\r\n",
      "  adding: result/c-gedung-roboh/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/c-gedung-roboh/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/c-gedung-roboh/test_label.txt (deflated 62%)\r\n",
      "  adding: result/d-indo-fuji/ (stored 0%)\r\n",
      "  adding: result/d-indo-fuji/test_generations.txt (deflated 84%)\r\n",
      "  adding: result/d-indo-fuji/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/d-indo-fuji/test_label.txt (deflated 86%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r result_with_sta.zip result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9630.27951,
   "end_time": "2022-05-18T10:41:36.016182",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-18T08:01:05.736672",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
