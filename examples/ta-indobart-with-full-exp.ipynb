{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa9c7aaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T01:45:42.569221Z",
     "iopub.status.busy": "2022-03-15T01:45:42.563017Z",
     "iopub.status.idle": "2022-03-15T01:46:11.275382Z",
     "shell.execute_reply": "2022-03-15T01:46:11.274631Z",
     "shell.execute_reply.started": "2022-03-14T06:35:28.661684Z"
    },
    "papermill": {
     "duration": 28.744989,
     "end_time": "2022-03-15T01:46:11.275553",
     "exception": false,
     "start_time": "2022-03-15T01:45:42.530564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.15.0)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.96)\r\n",
      "Collecting indobenchmark-toolkit==0.0.4\r\n",
      "  Downloading indobenchmark_toolkit-0.0.4-py3-none-any.whl (8.0 kB)\r\n",
      "Collecting sacrebleu\r\n",
      "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\r\n",
      "     |████████████████████████████████| 90 kB 1.0 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.7.1 in /opt/conda/lib/python3.7/site-packages (from indobenchmark-toolkit==0.0.4) (1.9.1)\r\n",
      "Collecting sentencepiece\r\n",
      "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\r\n",
      "     |████████████████████████████████| 1.2 MB 4.0 MB/s            \r\n",
      "\u001b[?25hCollecting datasets==1.4.1\r\n",
      "  Downloading datasets-1.4.1-py3-none-any.whl (186 kB)\r\n",
      "     |████████████████████████████████| 186 kB 57.5 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2022.2.0)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.3.5)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.20.3)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.26.0)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (4.11.2)\r\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (6.0.1)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (0.3.4)\r\n",
      "Collecting tqdm<4.50.0,>=4.27\r\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\r\n",
      "     |████████████████████████████████| 69 kB 6.1 MB/s             \r\n",
      "\u001b[?25hCollecting huggingface-hub==0.0.2\r\n",
      "  Downloading huggingface_hub-0.0.2-py3-none-any.whl (24 kB)\r\n",
      "Collecting xxhash\r\n",
      "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\r\n",
      "     |████████████████████████████████| 212 kB 53.3 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (0.70.12.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub==0.0.2->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (3.4.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\r\n",
      "Collecting transformers\r\n",
      "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\r\n",
      "     |████████████████████████████████| 3.8 MB 55.9 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\r\n",
      "     |████████████████████████████████| 3.5 MB 55.0 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\r\n",
      "  Downloading transformers-4.16.1-py3-none-any.whl (3.5 MB)\r\n",
      "     |████████████████████████████████| 3.5 MB 39.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.16.0-py3-none-any.whl (3.5 MB)\r\n",
      "     |████████████████████████████████| 3.5 MB 44.9 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.14.1-py3-none-any.whl (3.4 MB)\r\n",
      "     |████████████████████████████████| 3.4 MB 48.2 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\r\n",
      "     |████████████████████████████████| 3.3 MB 52.4 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 32.6 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.4-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 51.4 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 49.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.2-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 50.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.1-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 27.6 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.0-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 48.2 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 46.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.2-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 46.2 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.1-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 44.9 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.0-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 50.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 45.9 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 40.3 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.1-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 51.6 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 43.9 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\r\n",
      "     |████████████████████████████████| 2.6 MB 39.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\r\n",
      "     |████████████████████████████████| 2.6 MB 43.6 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.9.0-py3-none-any.whl (2.6 MB)\r\n",
      "     |████████████████████████████████| 2.6 MB 49.4 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 44.9 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\r\n",
      "  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 51.4 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.8.0-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 48.9 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.7.0-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 43.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\r\n",
      "     |████████████████████████████████| 2.2 MB 44.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.6.0-py3-none-any.whl (2.3 MB)\r\n",
      "     |████████████████████████████████| 2.3 MB 44.9 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\r\n",
      "     |████████████████████████████████| 2.1 MB 49.7 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.47)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\r\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.4.4)\r\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (2.4.0)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.8.9)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.26.7)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.0.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2021.10.8)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (3.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.7.1->indobenchmark-toolkit==0.0.4) (4.1.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (3.6.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2021.3)\r\n",
      "Installing collected packages: tqdm, xxhash, huggingface-hub, transformers, sentencepiece, datasets, sacrebleu, indobenchmark-toolkit\r\n",
      "  Attempting uninstall: tqdm\r\n",
      "    Found existing installation: tqdm 4.62.3\r\n",
      "    Uninstalling tqdm-4.62.3:\r\n",
      "      Successfully uninstalled tqdm-4.62.3\r\n",
      "  Attempting uninstall: huggingface-hub\r\n",
      "    Found existing installation: huggingface-hub 0.2.1\r\n",
      "    Uninstalling huggingface-hub-0.2.1:\r\n",
      "      Successfully uninstalled huggingface-hub-0.2.1\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.15.0\r\n",
      "    Uninstalling transformers-4.15.0:\r\n",
      "      Successfully uninstalled transformers-4.15.0\r\n",
      "  Attempting uninstall: sentencepiece\r\n",
      "    Found existing installation: sentencepiece 0.1.96\r\n",
      "    Uninstalling sentencepiece-0.1.96:\r\n",
      "      Successfully uninstalled sentencepiece-0.1.96\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "beatrix-jupyterlab 3.1.6 requires google-cloud-bigquery-storage, which is not installed.\r\n",
      "spacy 3.2.2 requires typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.1.1 which is incompatible.\r\n",
      "featuretools 1.6.0 requires numpy>=1.21.0, but you have numpy 1.20.3 which is incompatible.\r\n",
      "cached-path 1.0.2 requires huggingface-hub<0.3.0,>=0.0.12, but you have huggingface-hub 0.0.2 which is incompatible.\r\n",
      "cached-path 1.0.2 requires tqdm<4.63,>=4.62, but you have tqdm 4.49.0 which is incompatible.\r\n",
      "allennlp 2.9.0 requires huggingface-hub>=0.0.16, but you have huggingface-hub 0.0.2 which is incompatible.\r\n",
      "allennlp 2.9.0 requires tqdm>=4.62, but you have tqdm 4.49.0 which is incompatible.\u001b[0m\r\n",
      "Successfully installed datasets-1.4.1 huggingface-hub-0.0.2 indobenchmark-toolkit-0.0.4 sacrebleu-2.0.0 sentencepiece-0.1.95 tqdm-4.49.0 transformers-4.5.1 xxhash-3.0.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece indobenchmark-toolkit==0.0.4 sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e3071ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T01:46:11.447917Z",
     "iopub.status.busy": "2022-03-15T01:46:11.445228Z",
     "iopub.status.idle": "2022-03-15T01:46:13.865147Z",
     "shell.execute_reply": "2022-03-15T01:46:13.864628Z",
     "shell.execute_reply.started": "2022-03-14T06:36:04.472712Z"
    },
    "papermill": {
     "duration": 2.506542,
     "end_time": "2022-03-15T01:46:13.865290",
     "exception": false,
     "start_time": "2022-03-15T01:46:11.358748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'amr-to-text-indonesia'...\r\n",
      "remote: Enumerating objects: 1698, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (1698/1698), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (1097/1097), done.\u001b[K\r\n",
      "remote: Total 1698 (delta 684), reused 1383 (delta 374), pack-reused 0\u001b[K\r\n",
      "Receiving objects: 100% (1698/1698), 4.17 MiB | 9.89 MiB/s, done.\r\n",
      "Resolving deltas: 100% (684/684), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://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@github.com/taufiqhusada/amr-to-text-indonesia.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd9ef240",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T01:46:14.046077Z",
     "iopub.status.busy": "2022-03-15T01:46:14.045299Z",
     "iopub.status.idle": "2022-03-15T01:46:25.570716Z",
     "shell.execute_reply": "2022-03-15T01:46:25.571206Z",
     "shell.execute_reply.started": "2022-03-14T06:36:07.592912Z"
    },
    "papermill": {
     "duration": 11.616968,
     "end_time": "2022-03-15T01:46:25.571411",
     "exception": false,
     "start_time": "2022-03-15T01:46:13.954443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'amr-indo-dataset'...\r\n",
      "remote: Enumerating objects: 57, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (57/57), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (43/43), done.\u001b[K\r\n",
      "remote: Total 57 (delta 12), reused 53 (delta 11), pack-reused 0\u001b[K\r\n",
      "Unpacking objects: 100% (57/57), 55.48 MiB | 8.03 MiB/s, done.\r\n",
      "Updating files: 100% (50/50), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://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@github.com/taufiqhusada/amr-indo-dataset.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb771b03",
   "metadata": {
    "papermill": {
     "duration": 0.102767,
     "end_time": "2022-03-15T01:46:25.782911",
     "exception": false,
     "start_time": "2022-03-15T01:46:25.680144",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Finetune indobart with linearized penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d080ad9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T01:46:25.988443Z",
     "iopub.status.busy": "2022-03-15T01:46:25.986729Z",
     "iopub.status.idle": "2022-03-15T01:46:25.990795Z",
     "shell.execute_reply": "2022-03-15T01:46:25.990322Z",
     "shell.execute_reply.started": "2022-03-14T06:38:45.630235Z"
    },
    "papermill": {
     "duration": 0.108783,
     "end_time": "2022-03-15T01:46:25.990912",
     "exception": false,
     "start_time": "2022-03-15T01:46:25.882129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/train\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/amr-to-text-indonesia/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d176f65c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T01:46:26.195285Z",
     "iopub.status.busy": "2022-03-15T01:46:26.190602Z",
     "iopub.status.idle": "2022-03-15T01:46:26.849041Z",
     "shell.execute_reply": "2022-03-15T01:46:26.851453Z",
     "shell.execute_reply.started": "2022-03-14T06:38:45.864254Z"
    },
    "papermill": {
     "duration": 0.763516,
     "end_time": "2022-03-15T01:46:26.851646",
     "exception": false,
     "start_time": "2022-03-15T01:46:26.088130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetuning_indoBART.sh\ttrain_indoBART_on_silver_data.sh\r\n",
      "finetuning_indoT5.sh\ttrain_indoT5.py\r\n",
      "result\t\t\ttrain_indoT5_on_silver_data.sh\r\n",
      "train_indoBART.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0d41ad4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T01:46:27.059558Z",
     "iopub.status.busy": "2022-03-15T01:46:27.058755Z",
     "iopub.status.idle": "2022-03-15T01:50:27.707065Z",
     "shell.execute_reply": "2022-03-15T01:50:27.706350Z",
     "shell.execute_reply.started": "2022-03-14T06:38:46.955663Z"
    },
    "papermill": {
     "duration": 240.75358,
     "end_time": "2022-03-15T01:50:27.707202",
     "exception": false,
     "start_time": "2022-03-15T01:46:26.953622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "Downloading: 100%|███████████████████████████| 932k/932k [00:00<00:00, 3.35MB/s]\r\n",
      "Downloading: 100%|██████████████████████████████| 315/315 [00:00<00:00, 239kB/s]\r\n",
      "Downloading: 100%|██████████████████████████████| 339/339 [00:00<00:00, 266kB/s]\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "Downloading: 100%|█████████████████████████| 1.71k/1.71k [00:00<00:00, 1.45MB/s]\r\n",
      "Downloading: 100%|███████████████████████████| 526M/526M [00:26<00:00, 19.9MB/s]\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:1.3601 LR:0.00003000: 100%|█| 662/662 [00:44<00:00, 14.95it\r\n",
      "(Epoch 1) DEV LOSS:0.5929 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 50.76it/s]\r\n",
      "bleu score on dev:  57.38434026338936\r\n",
      "(Epoch 2) TRAIN LOSS:0.5418 LR:0.00003000: 100%|█| 662/662 [00:43<00:00, 15.27it\r\n",
      "(Epoch 2) DEV LOSS:0.6770 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 55.45it/s]\r\n",
      "bleu score on dev:  56.57797910100631\r\n",
      "(Epoch 3) TRAIN LOSS:0.4376 LR:0.00003000: 100%|█| 662/662 [00:42<00:00, 15.40it\r\n",
      "(Epoch 3) DEV LOSS:0.4651 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 54.83it/s]\r\n",
      "bleu score on dev:  59.79251743606556\r\n",
      "(Epoch 4) TRAIN LOSS:0.3903 LR:0.00003000: 100%|█| 662/662 [00:43<00:00, 15.06it\r\n",
      "(Epoch 4) DEV LOSS:0.5798 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 52.69it/s]\r\n",
      "bleu score on dev:  56.06612825420897\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:12<00:00,  6.28it/s]\r\n",
      "sample:  balon tersebut ditiup oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  obe sedang menulis puisi ---- obe menulis puisi\r\n",
      "sample:  saya sedang mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  anak yang ajaib itu bernama angga ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  32.137943149928766\r\n",
      "tensor([[    0,   382,  9338, 40007,   382,   475,   384, 40008,   382,  6353,\r\n",
      "           384,   384,   384,     2, 40002]])\r\n",
      "saya sedang mengetik makalah\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoBART.py --model_type indo-bart --n_epochs 4 --lr 3e-5 --num_beams 10 --data_folder ../data/preprocessed_data/linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6418d955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T01:50:30.149500Z",
     "iopub.status.busy": "2022-03-15T01:50:30.148668Z",
     "iopub.status.idle": "2022-03-15T01:50:30.881002Z",
     "shell.execute_reply": "2022-03-15T01:50:30.880115Z",
     "shell.execute_reply.started": "2022-03-14T06:44:57.875847Z"
    },
    "papermill": {
     "duration": 2.010125,
     "end_time": "2022-03-15T01:50:30.881136",
     "exception": false,
     "start_time": "2022-03-15T01:50:28.871011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu_score_test.txt  loss_data.tsv  model  test_generations.txt  test_label.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74a0e1ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T01:50:33.127079Z",
     "iopub.status.busy": "2022-03-15T01:50:33.126249Z",
     "iopub.status.idle": "2022-03-15T01:51:02.545921Z",
     "shell.execute_reply": "2022-03-15T01:51:02.546440Z",
     "shell.execute_reply.started": "2022-03-14T06:44:58.623221Z"
    },
    "papermill": {
     "duration": 30.529205,
     "end_time": "2022-03-15T01:51:02.546622",
     "exception": false,
     "start_time": "2022-03-15T01:50:32.017417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result/ (stored 0%)\r\n",
      "  adding: result/test_label.txt (deflated 60%)\r\n",
      "  adding: result/.gitignore (stored 0%)\r\n",
      "  adding: result/test_generations.txt (deflated 64%)\r\n",
      "  adding: result/loss_data.tsv (deflated 33%)\r\n",
      "  adding: result/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/model/ (stored 0%)\r\n",
      "  adding: result/model/pytorch_model.bin (deflated 8%)\r\n",
      "  adding: result/model/config.json (deflated 63%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r result_without_sta.zip result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b664725b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T01:51:05.047033Z",
     "iopub.status.busy": "2022-03-15T01:51:05.046157Z",
     "iopub.status.idle": "2022-03-15T01:51:05.049091Z",
     "shell.execute_reply": "2022-03-15T01:51:05.049547Z",
     "shell.execute_reply.started": "2022-03-14T06:45:31.282452Z"
    },
    "papermill": {
     "duration": 1.111405,
     "end_time": "2022-03-15T01:51:05.049698",
     "exception": false,
     "start_time": "2022-03-15T01:51:03.938293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/evaluate\n"
     ]
    }
   ],
   "source": [
    "%cd ../evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28895a7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T01:51:07.273652Z",
     "iopub.status.busy": "2022-03-15T01:51:07.272878Z",
     "iopub.status.idle": "2022-03-15T01:53:05.111702Z",
     "shell.execute_reply": "2022-03-15T01:53:05.110561Z",
     "shell.execute_reply.started": "2022-03-14T06:45:31.330294Z"
    },
    "papermill": {
     "duration": 118.938322,
     "end_time": "2022-03-15T01:53:05.111882",
     "exception": false,
     "start_time": "2022-03-15T01:51:06.173560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘result’: File exists\r\n",
      "saved model folder ../train/result\r\n",
      "preprocessing method linearized_penman\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data’: File exists\r\n",
      "preprocess amr_simple_test\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/amr_simple_test’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/amr_simple_test/test.amr.txt', result_sent_path='data/test/preprocessed_data/amr_simple_test/test.sent.txt', source_file_path='data/test/amr_simple_test.txt', source_folder_path=None)\r\n",
      "100%|███████████████████████████████████| 2618/2618 [00:00<00:00, 177099.30it/s]\r\n",
      "total: 306 pair sent-amr\r\n",
      "preprocess b-salah-darat\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/b-salah-darat’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/b-salah-darat/test.amr.txt', result_sent_path='data/test/preprocessed_data/b-salah-darat/test.sent.txt', source_file_path='data/test/b-salah-darat.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 676/676 [00:00<00:00, 123776.55it/s]\r\n",
      "total: 32 pair sent-amr\r\n",
      "preprocess c-gedung-roboh\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/c-gedung-roboh’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/c-gedung-roboh/test.amr.txt', result_sent_path='data/test/preprocessed_data/c-gedung-roboh/test.sent.txt', source_file_path='data/test/c-gedung-roboh.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 606/606 [00:00<00:00, 125320.39it/s]\r\n",
      "total: 29 pair sent-amr\r\n",
      "preprocess d-indo-fuji\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/d-indo-fuji’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/d-indo-fuji/test.amr.txt', result_sent_path='data/test/preprocessed_data/d-indo-fuji/test.sent.txt', source_file_path='data/test/d-indo-fuji.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 745/745 [00:00<00:00, 123167.38it/s]\r\n",
      "total: 27 pair sent-amr\r\n",
      "preprocess f-bunuh-diri\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/f-bunuh-diri’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/f-bunuh-diri/test.amr.txt', result_sent_path='data/test/preprocessed_data/f-bunuh-diri/test.sent.txt', source_file_path='data/test/f-bunuh-diri.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 468/468 [00:00<00:00, 128229.31it/s]\r\n",
      "total: 23 pair sent-amr\r\n",
      "preprocess g-gempa-dieng\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/g-gempa-dieng’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/g-gempa-dieng/test.amr.txt', result_sent_path='data/test/preprocessed_data/g-gempa-dieng/test.sent.txt', source_file_path='data/test/g-gempa-dieng.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 412/412 [00:00<00:00, 124553.36it/s]\r\n",
      "total: 19 pair sent-amr\r\n",
      "mkdir: cannot create directory ‘result’: File exists\r\n",
      "evaluate on data amr_simple_test\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 306\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:12<00:00,  6.16it/s]\r\n",
      "sample:  balon tersebut ditiup oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  obe sedang menulis puisi ---- obe menulis puisi\r\n",
      "sample:  saya sedang mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  anak yang ajaib itu bernama angga ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "sample:  dia berenang ---- dia sedang berenang\r\n",
      "sample:  buku yang hilang dicari oleh ilham ---- ilham sedang mencari bukunya yang hilang\r\n",
      "sample:  ibu sedang menyapu halaman rumah ---- ibu sedang menyapu halaman rumah\r\n",
      "sample:  padi yang ayah bajak di sawah ---- ayah sedang membajak padi di sawah\r\n",
      "sample:  di tepi pantai, andi pergi ---- andi pergi berkemah di tepi pantai\r\n",
      "bleu score on test dataset:  32.137943149928766\r\n",
      "evaluate on data b-salah-darat\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 32\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:03<00:00,  2.54it/s]\r\n",
      "sample:  kantor kantor sama itu sedang berdiskusi di bandara soekarnohatta mei 10 2016 ---- kantor imigrasi bandara soekarno-hatta bersama kantor otoritas bandara soekarno-hatta dan pt angkasa pura ii membahas insiden penumpang penerbangan pesawat lion air jt 161 yang tiba di jakarta dari singapura 10 mei 2016.\r\n",
      "sample:  kepala kantor imigrasi soekarnohatta alif siang ini sedang meeting soal kepada sabtu 14 mei ---- kami meeting soal ini,\" ujar kepala kantor imigrasi soekarno-hatta alif suadi kepada tempo, sabtu malam ini, 14 mei 2016.\r\n",
      "sample:  pada pukul 18, anak natalie berangkat ke singapura ---- anak temannya natalie berangkat dari singapura dan menggunakan pesawat lion air jt 161, pada pukul 18.50.\r\n",
      "sample:  kata sekretaris agus haryadi di perusahaan pt angkasa ii, akan melakukan laku kelola bandara internasional soekarnohatta ---- sekretaris perusahaan pt angkasa pura ii agus haryadi mengatakan selaku pengelola bandara internasional soekarno-hatta, pihaknya akan berkoordinasi dengan kantor otoritas bandara wilayah i terkait dengan peristiwa ini.\r\n",
      "sample:  di remote area, pesawat parkir bersiap-siap ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "sample:  di padang, sopir jemput oleh edward ---- terdapat sopir bus yang akan menjemput penumpang dari padang\" ujar edward.\r\n",
      "sample:  pihak otoritas bandara soetta memanggil pihak yang melakukan investigasi ---- pihak otoritas bandara soetta akan memanggil pihak terkait dan dilakukan investigasi.\r\n",
      "sample:  kemenhub memberikan sanksi kepada pihak yang lalai ---- kemenhub memberikan sanksi kepada pihak-pihak yang lalai,\" ujarnya.\r\n",
      "sample:  cerita teman tersebut ditulis oleh zara di terminal ii ---- pesawat itu mendarat tidak di terminal ii, kata zara mengutip cerita temannya.\r\n",
      "sample:  di remote area, pesawat parkir bersiap-siap ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "bleu score on test dataset:  7.779637653277772\r\n",
      "evaluate on data c-gedung-roboh\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 29\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:03<00:00,  2.61it/s]\r\n",
      "sample:  kendara laju arah mal bintaro xchange akan melakukan flyover di atas lampu merah ---- pengalihan dilakukan bagi pengendara yang melaju dari arah mal bintaro xchange dan tol bintaro menuju jalan boulevard bintaro dengan menggunakan flyover di atas lampu merah indomobil.\r\n",
      "sample:  terjadi kecelakan tanah dan gedung miring itu terjadi di menara jakarta timur ---- kegagalan struktur dan kesalahan tes tanah yang mengakibatkan gedung miring juga terjadi pada menara saidah di jalan mt haryono, jakarta timur.\r\n",
      "sample:  di tangerang selatan, terjadi kecelakan gedung itu ---- kapolres tangerang selatan, akbp ayi supardan membenarkan peristiwa robohnya gedung di bintaro yang menarik perhatian masyarakat banyak.\r\n",
      "sample:  pembangunan gedung itu putus oleh panin karena lulus yang layak ---- panin memutuskan tidak melanjutkan pembangunan gedung itu karena dinyatakan tidak lulus uji kelayakan.\r\n",
      "sample:  gedung itu dibangun oleh lulus tidak sehingga pembangunan gedung itu putus ---- gedung tersebut dinyatakan tidak lulus uji sehingga pembangunan gedung diputuskan tidak dilanjutkan.\r\n",
      "sample:  kata kasat akp samian di polres tangerang selatan, pembangunan gedung itu dilakukan oleh pihak yang tahu saat ini ---- kasat reskrim polres tangerangg selatan, akp samian mengatakan saat ini pihaknya melakukan penyelidikan untuk mengetahui penyebab runtuhnya gedung.\r\n",
      "sample:  kami sedang meliput peristiwa yang sebut tadi ---- hingga kini kami masih mendalami peristiwa tersebut.\r\n",
      "sample:  terjadi kecelekaan oleh korban ---- kejadian tersebut tidak timbul korban.\r\n",
      "sample:  rugi material yang terjadi ---- terjadi kerugian material.\r\n",
      "sample:  di tangerang selatan, terjadi kecelakan gedung ---- pembongkaran gedung di sektor 7, bintaro, tangerang selatan, diduga menyalahi prosedur.\r\n",
      "bleu score on test dataset:  7.49138735703375\r\n",
      "evaluate on data d-indo-fuji\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 27\r\n",
      "  0%|                                                     | 0/7 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:02<00:00,  2.70it/s]\r\n",
      "sample:  kejadian ini ditandatangani oleh indosat ooredoo indonesia dan langgan bisnis itu hadir dalam bentuk smart internet ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  kejadian ini ditandatangani oleh indosat ooredoo indonesia dan langgan bisnis itu hadir dalam bentuk smart internet ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  smart mobility iot dan peningkatan aset akan dilakukan oleh perusahaan itu ---- smart mobility dan iot mengalami perkembangan cukup signifikan sehingga mengubah cara perusahaan dalam mengelola aset.\r\n",
      "sample:  kejadian ini ditandatangani oleh indosat ooredoo indonesia dan langgan bisnis itu hadir dalam bentuk smart internet ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  di indonesia, kerja sama akan dilakukan oleh perusahaan itu ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  president achmad sofwan memimpin perusahaan realtime dan mobilitas akan dilakukan dengan cara yang optimal ---- mobilitas akan menyediakan cara untuk mengoptimalkan strategi perusahaan dengan memanfaatkan aplikasi dan layanan yang dapat memproses data real-time, tutur achmad sofwan, president director fujitsu indonesia.\r\n",
      "sample:  di indonesia, kerja sama akan dilakukan oleh perusahaan itu ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  kejadian ini dikerjakan oleh indosat ooredoo dan fujitsu indonesia ---- indosat ooredoo bekerja sama dengan fujitsu indonesia menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  president achmad sofwan memimpin perusahaan realtime dan mobilitas dilakukan dengan cara optimal ---- president director fujitsu, achmad sofwan mengungkap, mobilitas akan menyediakan cara mengoptimalisasikan strategi perusahaan dengan memanfaatkan aplikasi dan layanan dalam aktivitas memproses data real-time.\r\n",
      "sample:  di indonesia, kerja sama akan dilakukan oleh perusahaan itu ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "bleu score on test dataset:  0.6374410792131114\r\n",
      "evaluate on data f-bunuh-diri\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 23\r\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:02<00:00,  2.69it/s]\r\n",
      "sample:  ciptadi melompat di jembatan yang tinggi 15 meter ---- ciptadi dengan nekat melompat dari megatron jembatan penyeberangan orang yang tingginya diperkirakan mencapai 15 meter.\r\n",
      "sample:  hal yang disampaikan oleh kasubag reny kompol di mapolrestabes bandung ---- hal tersebut diungkapkan kabag ops polrestabes bandung kompol dede rojudin didampingi kasubag humas kompol reny marthaliana, saat ditemui di mapolrestabes bandung, jumat.\r\n",
      "sample:  di kota bandung, rezha telah bubar salat di masjid raya bandung jumat dan ia meledakkan megatron di depan kantor ---- rezha noviana, warga kota bandung mengatakan ia dan jemaat lainnya langsung memperhatikan megatron jembatan penyeberangan orang depan kantor pos besar bandung setelah bubar salat jumat di masjid raya bandung.\r\n",
      "sample:  di atas jpo, tugas linmas ---- ketika itu, petugas linmas dan beberapa satpam berdiri di atas jpo.\r\n",
      "sample:  aksi bunuh diri dilakukan di alun alun ---- aksi bunuh diri di alun - alun bandung belum lama ini terjadi.\r\n",
      "sample:  orang tua terjatuh dalam diri di alun bandung ---- seorang pria tua nekat menjatuhkan diri dari jpo di alun - alun kota bandung.\r\n",
      "sample:  di alun alun, sudirman menjadi saksi mata anggota linmas di kota bandung ---- sudirman, anggota linmas satpol pp kota bandung yang menjadi saksi mata aksi bunuh diri di alun - alun bandung itu mengatakan, pria tersebut diperkirakan berusia 50 tahun.\r\n",
      "sample:  di depan bandung, lelaki itu terjatuh di jembatan seberang jumat ---- seorang lelaki menjatuhkan diri dari megatron jembatan penyeberangan di depan pos giro besar bandung, bandung, jumat.\r\n",
      "sample:  hidup itu diakhiri oleh lelaki tengah baya di tengah jalan asia afrika ---- seorang lelaki tengah baya nekat mengakhiri hidupnya di tengah kepadatan lalu lintas jalan asia afrika.\r\n",
      "sample:  di bandung, seorang pria misterius membuat bom di seberang jalan ---- pria misterius yang bunuh diri dari jembatan penyeberangan orang di jalan asia afrika, bandung, jawa barat, membuat pesan sebelum nekat terjun.\r\n",
      "bleu score on test dataset:  6.409873137202585\r\n",
      "evaluate on data g-gempa-dieng\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 19\r\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:01<00:00,  2.61it/s]\r\n",
      "sample:  stasiun banjarnegara berada di jawa tengah badan meteorologi geofisika pukul 19 00 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara, jawa tengah, menyatakan bahwa gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 pada skala richter.\r\n",
      "sample:  di desa kecamatan batang, pusat gempa berada tanji gugur ---- pusat gempa diperkirakan berada di desa tanji gugur, kecamatan bawang, kabupaten batang.\r\n",
      "sample:  di desa sumberejo, terdapat tutup sementara ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  kepala surono memimpin pusat vulkanologi mitigasi bencana geologi pukul 19 00 wib di dataran tinggi dieng ---- kepala pusat vulkanologi dan mitigasi bencana geologi surono mengatakan bahwa gempa yang terjadi di dataran tinggi dieng pukul 19.00 wib terekam sebanyak 86 kali.\r\n",
      "sample:  tutup dilakukan oleh tim ukur di lapangan ---- penutupan ini dilakukan hingga tim di lapangan yang mengukur konsentrasi gas menyatakan tidak ada gas yang berbahaya.\r\n",
      "sample:  di desa sumberejo, terdapat tutup sementara ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  tugas pusat vulkanologi mitigasi bencana geologi yang diberikan oleh pantau kawah timbang ---- petugas pusat vulkanologi dan mitigasi bencana geologi diterjunkan untuk memantau aktivitas kawah timbang dan memberikan informasi.\r\n",
      "sample:  di pusat vulkanologi mitigasi bencana geologi, terjadi gempa pukul 19 00 wib ---- data yang dikeluarkan dari pusat vulkanologi dan mitigasi bencana geologi menyebutkan gempa mulai terasa pukul 19.00 wib dan telah terekam gempa sebanyak 86 kali.\r\n",
      "sample:  pengungsi tinggal di kabupaten banjarnegara sabtu pagi ---- pengungsi di kabupaten banjarnegara, pada sabtu pagi mulai meninggalkan pengungsian.\r\n",
      "sample:  stasiun banjarnegara diresmikan oleh badan meteorologi klimatologi geofisika pukul 19 ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara menyatakan gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 skala richter *(sr)*.\r\n",
      "bleu score on test dataset:  7.697201928691179\r\n",
      "mkdir: cannot create directory ‘result/linearized_penman’: File exists\r\n",
      "mv: cannot move 'result/amr_simple_test' to 'result/linearized_penman/amr_simple_test': Directory not empty\r\n",
      "mv: cannot move 'result/b-salah-darat' to 'result/linearized_penman/b-salah-darat': Directory not empty\r\n",
      "mv: cannot move 'result/c-gedung-roboh' to 'result/linearized_penman/c-gedung-roboh': Directory not empty\r\n",
      "mv: cannot move 'result/d-indo-fuji' to 'result/linearized_penman/d-indo-fuji': Directory not empty\r\n",
      "mv: cannot move 'result/f-bunuh-diri' to 'result/linearized_penman/f-bunuh-diri': Directory not empty\r\n",
      "mv: cannot move 'result/g-gempa-dieng' to 'result/linearized_penman/g-gempa-dieng': Directory not empty\r\n",
      "mv: cannot move 'result/linearized_penman' to a subdirectory of itself, 'result/linearized_penman/linearized_penman'\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x ./evaluate_indoBART_to_all.sh\n",
    "!mkdir result\n",
    "!./evaluate_indoBART_to_all.sh ../train/result linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cee3877d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T01:53:07.465935Z",
     "iopub.status.busy": "2022-03-15T01:53:07.465100Z",
     "iopub.status.idle": "2022-03-15T01:53:08.116885Z",
     "shell.execute_reply": "2022-03-15T01:53:08.116241Z",
     "shell.execute_reply.started": "2022-03-14T06:46:21.715669Z"
    },
    "papermill": {
     "duration": 1.810961,
     "end_time": "2022-03-15T01:53:08.117008",
     "exception": false,
     "start_time": "2022-03-15T01:53:06.306047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result/ (stored 0%)\r\n",
      "  adding: result/c-gedung-roboh/ (stored 0%)\r\n",
      "  adding: result/c-gedung-roboh/test_label.txt (deflated 62%)\r\n",
      "  adding: result/c-gedung-roboh/test_generations.txt (deflated 61%)\r\n",
      "  adding: result/c-gedung-roboh/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/g-gempa-dieng/ (stored 0%)\r\n",
      "  adding: result/g-gempa-dieng/test_label.txt (deflated 67%)\r\n",
      "  adding: result/g-gempa-dieng/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/g-gempa-dieng/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/test_label.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/bleu_score_test.txt (deflated 12%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/test_label.txt (deflated 67%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/test_generations.txt (deflated 66%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/test_label.txt (deflated 68%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/test_generations.txt (deflated 65%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/test_label.txt (deflated 86%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/test_generations.txt (deflated 86%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/test_label.txt (deflated 66%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/test_generations.txt (deflated 66%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/test_label.txt (deflated 60%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/f-bunuh-diri/ (stored 0%)\r\n",
      "  adding: result/f-bunuh-diri/test_label.txt (deflated 68%)\r\n",
      "  adding: result/f-bunuh-diri/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/f-bunuh-diri/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/d-indo-fuji/ (stored 0%)\r\n",
      "  adding: result/d-indo-fuji/test_label.txt (deflated 86%)\r\n",
      "  adding: result/d-indo-fuji/test_generations.txt (deflated 84%)\r\n",
      "  adding: result/d-indo-fuji/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/b-salah-darat/ (stored 0%)\r\n",
      "  adding: result/b-salah-darat/test_label.txt (deflated 66%)\r\n",
      "  adding: result/b-salah-darat/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/b-salah-darat/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/amr_simple_test/ (stored 0%)\r\n",
      "  adding: result/amr_simple_test/test_label.txt (deflated 60%)\r\n",
      "  adding: result/amr_simple_test/test_generations.txt (deflated 64%)\r\n",
      "  adding: result/amr_simple_test/bleu_score_test.txt (stored 0%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r result_without_sta.zip result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652cf923",
   "metadata": {
    "papermill": {
     "duration": 1.146088,
     "end_time": "2022-03-15T01:53:10.421403",
     "exception": false,
     "start_time": "2022-03-15T01:53:09.275315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Finetune indobart with linearized penman + tree level embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c0186c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T01:53:12.853959Z",
     "iopub.status.busy": "2022-03-15T01:53:12.853225Z",
     "iopub.status.idle": "2022-03-15T01:53:12.856429Z",
     "shell.execute_reply": "2022-03-15T01:53:12.856843Z"
    },
    "papermill": {
     "duration": 1.271988,
     "end_time": "2022-03-15T01:53:12.856985",
     "exception": false,
     "start_time": "2022-03-15T01:53:11.584997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/tree_level_embeddings\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/amr-to-text-indonesia/tree_level_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6d1a340",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T01:53:15.509250Z",
     "iopub.status.busy": "2022-03-15T01:53:15.508389Z",
     "iopub.status.idle": "2022-03-15T01:53:16.197823Z",
     "shell.execute_reply": "2022-03-15T01:53:16.197332Z"
    },
    "papermill": {
     "duration": 2.152799,
     "end_time": "2022-03-15T01:53:16.197955",
     "exception": false,
     "start_time": "2022-03-15T01:53:14.045156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3bb5db7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T01:53:18.530727Z",
     "iopub.status.busy": "2022-03-15T01:53:18.529924Z",
     "iopub.status.idle": "2022-03-15T01:57:10.253638Z",
     "shell.execute_reply": "2022-03-15T01:57:10.254155Z"
    },
    "papermill": {
     "duration": 232.905354,
     "end_time": "2022-03-15T01:57:10.254311",
     "exception": false,
     "start_time": "2022-03-15T01:53:17.348957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "Some weights of MBartForConditionalGeneration were not initialized from the model checkpoint at indobenchmark/indobart and are newly initialized: ['model.encoder.tree_embed.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:1.3870 LR:0.00003000: 100%|█| 662/662 [00:50<00:00, 13.11it\r\n",
      "(Epoch 1) DEV LOSS:0.4111 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 42.02it/s]\r\n",
      "bleu score on dev:  58.45206044614471\r\n",
      "(Epoch 2) TRAIN LOSS:0.5565 LR:0.00003000: 100%|█| 662/662 [00:48<00:00, 13.51it\r\n",
      "(Epoch 2) DEV LOSS:0.5936 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 41.03it/s]\r\n",
      "bleu score on dev:  52.416784417341034\r\n",
      "(Epoch 3) TRAIN LOSS:0.4405 LR:0.00003000: 100%|█| 662/662 [00:50<00:00, 13.11it\r\n",
      "(Epoch 3) DEV LOSS:0.5785 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 41.23it/s]\r\n",
      "bleu score on dev:  40.23496367556997\r\n",
      "(Epoch 4) TRAIN LOSS:0.3927 LR:0.00003000: 100%|█| 662/662 [00:49<00:00, 13.35it\r\n",
      "(Epoch 4) DEV LOSS:0.5856 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 42.01it/s]\r\n",
      "bleu score on dev:  57.13385738234364\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:12<00:00,  6.01it/s]\r\n",
      "sample:  balon ditiup oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  saya sedang mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  anak ajaib itu bernama ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  32.03378873198283\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoBART.py --model_type indo-bart --n_epochs 4 --lr 3e-5 --num_beams 10 --data_folder ../data/preprocessed_data/linearized_penman_with_tree_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aff9d4aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T01:57:14.646003Z",
     "iopub.status.busy": "2022-03-15T01:57:14.645191Z",
     "iopub.status.idle": "2022-03-15T01:57:15.298518Z",
     "shell.execute_reply": "2022-03-15T01:57:15.297973Z"
    },
    "papermill": {
     "duration": 2.877231,
     "end_time": "2022-03-15T01:57:15.298662",
     "exception": false,
     "start_time": "2022-03-15T01:57:12.421431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!chmod +x ./evaluate_indoBART_to_all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55a2e1da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T01:57:20.057567Z",
     "iopub.status.busy": "2022-03-15T01:57:20.056792Z",
     "iopub.status.idle": "2022-03-15T01:59:18.656540Z",
     "shell.execute_reply": "2022-03-15T01:59:18.656029Z"
    },
    "papermill": {
     "duration": 120.875658,
     "end_time": "2022-03-15T01:59:18.656673",
     "exception": false,
     "start_time": "2022-03-15T01:57:17.781015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model folder result\r\n",
      "preprocessing method linearized_penman_with_tree_level\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data’: File exists\r\n",
      "preprocess amr_simple_test\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/amr_simple_test’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/amr_simple_test/test.amr.txt', result_sent_path='data/test/preprocessed_data/amr_simple_test/test.sent.txt', source_file_path='data/test/amr_simple_test.txt', source_folder_path=None)\r\n",
      "100%|███████████████████████████████████| 2618/2618 [00:00<00:00, 182327.74it/s]\r\n",
      "total: 306  tuple_sent_amr_level\r\n",
      "preprocess b-salah-darat\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/b-salah-darat’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/b-salah-darat/test.amr.txt', result_sent_path='data/test/preprocessed_data/b-salah-darat/test.sent.txt', source_file_path='data/test/b-salah-darat.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 676/676 [00:00<00:00, 126301.82it/s]\r\n",
      "total: 32  tuple_sent_amr_level\r\n",
      "preprocess c-gedung-roboh\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/c-gedung-roboh’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/c-gedung-roboh/test.amr.txt', result_sent_path='data/test/preprocessed_data/c-gedung-roboh/test.sent.txt', source_file_path='data/test/c-gedung-roboh.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 606/606 [00:00<00:00, 101706.54it/s]\r\n",
      "total: 29  tuple_sent_amr_level\r\n",
      "preprocess d-indo-fuji\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/d-indo-fuji’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/d-indo-fuji/test.amr.txt', result_sent_path='data/test/preprocessed_data/d-indo-fuji/test.sent.txt', source_file_path='data/test/d-indo-fuji.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 745/745 [00:00<00:00, 122881.61it/s]\r\n",
      "total: 27  tuple_sent_amr_level\r\n",
      "preprocess f-bunuh-diri\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/f-bunuh-diri’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/f-bunuh-diri/test.amr.txt', result_sent_path='data/test/preprocessed_data/f-bunuh-diri/test.sent.txt', source_file_path='data/test/f-bunuh-diri.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 468/468 [00:00<00:00, 127182.47it/s]\r\n",
      "total: 23  tuple_sent_amr_level\r\n",
      "preprocess g-gempa-dieng\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/g-gempa-dieng’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/g-gempa-dieng/test.amr.txt', result_sent_path='data/test/preprocessed_data/g-gempa-dieng/test.sent.txt', source_file_path='data/test/g-gempa-dieng.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 412/412 [00:00<00:00, 123963.65it/s]\r\n",
      "total: 19  tuple_sent_amr_level\r\n",
      "evaluate on data amr_simple_test\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 306\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:13<00:00,  5.91it/s]\r\n",
      "sample:  balon ditiup oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  saya sedang mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  anak ajaib itu bernama ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "sample:  dia berenang ---- dia sedang berenang\r\n",
      "sample:  buku hilang dicari ilham ---- ilham sedang mencari bukunya yang hilang\r\n",
      "sample:  ibu sedang menyapu halaman rumah ---- ibu sedang menyapu halaman rumah\r\n",
      "sample:  padi ditanam ayah di sawah ---- ayah sedang membajak padi di sawah\r\n",
      "sample:  di tepi pantai, andi pergi ---- andi pergi berkemah di tepi pantai\r\n",
      "bleu score on test dataset:  32.03378873198283\r\n",
      "evaluate on data b-salah-darat\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 32\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:03<00:00,  2.44it/s]\r\n",
      "sample:  di kantor pt pt angkasa pura ii, di bandara soekarnohatta, terjadi insiden terbang yang tiba di jakarta dari mei 2016 ---- kantor imigrasi bandara soekarno-hatta bersama kantor otoritas bandara soekarno-hatta dan pt angkasa pura ii membahas insiden penumpang penerbangan pesawat lion air jt 161 yang tiba di jakarta dari singapura 10 mei 2016.\r\n",
      "sample:  di kantor imigrasi soekarnohatta, kami bertemu pada sabtu ini ---- kami meeting soal ini,\" ujar kepala kantor imigrasi soekarno-hatta alif suadi kepada tempo, sabtu malam ini, 14 mei 2016.\r\n",
      "sample:  anak natalie berangkat ke pesawat lion air jt 161 pada pukul 18 55 ---- anak temannya natalie berangkat dari singapura dan menggunakan pesawat lion air jt 161, pada pukul 18.50.\r\n",
      "sample:  direktur haryadi seorang perusahaan pt angkasa pura ii yang melakukan laku kelola bandara internasional soekarnohatta ---- sekretaris perusahaan pt angkasa pura ii agus haryadi mengatakan selaku pengelola bandara internasional soekarno-hatta, pihaknya akan berkoordinasi dengan kantor otoritas bandara wilayah i terkait dengan peristiwa ini.\r\n",
      "sample:  pesawat parkir itu disiapkan oleh kami di remote area ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "sample:  dari padang, edward mendapatkan pekerjaan sebagai sopir jemput bus ---- terdapat sopir bus yang akan menjemput penumpang dari padang\" ujar edward.\r\n",
      "sample:  pihak bandara soetta yang memanggil pihak kait-nasional ---- pihak otoritas bandara soetta akan memanggil pihak terkait dan dilakukan investigasi.\r\n",
      "sample:  kemenhub memberikan sanksi kepada pihakpihak yang lalai ---- kemenhub memberikan sanksi kepada pihak-pihak yang lalai,\" ujarnya.\r\n",
      "sample:  zara menulis cerita temannya di terminal ii ---- pesawat itu mendarat tidak di terminal ii, kata zara mengutip cerita temannya.\r\n",
      "sample:  pesawat parkir itu disiapkan oleh kami di remote area ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "bleu score on test dataset:  9.879791521839332\r\n",
      "evaluate on data c-gedung-roboh\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 29\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:03<00:00,  2.66it/s]\r\n",
      "sample:  kendara itu bergerak semakin arah mal bintaro xchange di atas jalan boulevard bintaro ---- pengalihan dilakukan bagi pengendara yang melaju dari arah mal bintaro xchange dan tol bintaro menuju jalan boulevard bintaro dengan menggunakan flyover di atas lampu merah indomobil.\r\n",
      "sample:  kejadian ini terjadi ketika pembangunan gedung miring dan di jalan jakarta timur, gedung itu roboh ---- kegagalan struktur dan kesalahan tes tanah yang mengakibatkan gedung miring juga terjadi pada menara saidah di jalan mt haryono, jakarta timur.\r\n",
      "sample:  di tangerang selatan, terjadi peristiwa roboh gedung itu ---- kapolres tangerang selatan, akbp ayi supardan membenarkan peristiwa robohnya gedung di bintaro yang menarik perhatian masyarakat banyak.\r\n",
      "sample:  pembangunan gedung itu putus karena tidak lulus dengan layak ---- panin memutuskan tidak melanjutkan pembangunan gedung itu karena dinyatakan tidak lulus uji kelayakan.\r\n",
      "sample:  gedung itu dibangun dengan lulus terbaik dan pembangunan gedung itu ---- gedung tersebut dinyatakan tidak lulus uji sehingga pembangunan gedung diputuskan tidak dilanjutkan.\r\n",
      "sample:  kasat akp samian di polres tangerang selatan melakukan laku tahu di gedung itu saat ini ---- kasat reskrim polres tangerangg selatan, akp samian mengatakan saat ini pihaknya melakukan penyelidikan untuk mengetahui penyebab runtuhnya gedung.\r\n",
      "sample:  sejak ini, kami melihat peristiwa yang sebut ---- hingga kini kami masih mendalami peristiwa tersebut.\r\n",
      "sample:  kejadian ini terjadi ketika korban ---- kejadian tersebut tidak timbul korban.\r\n",
      "sample:  rugi material yang terjadi ---- terjadi kerugian material.\r\n",
      "sample:  di tangerang selatan, terjadi bongkar di gedung itu ---- pembongkaran gedung di sektor 7, bintaro, tangerang selatan, diduga menyalahi prosedur.\r\n",
      "bleu score on test dataset:  5.241249340945514\r\n",
      "evaluate on data d-indo-fuji\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 27\r\n",
      "  0%|                                                     | 0/7 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:03<00:00,  2.13it/s]\r\n",
      "sample:  sertifikat yang diberikan oleh indosat fujitsu indonesia kepada para mitra bangun rumah itu adalah dalam teknologi internet of things ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  sertifikat yang diberikan oleh indosat fujitsu indonesia kepada para mitra bangun rumah itu adalah dalam teknologi internet of things ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  teknologi iot yang mereka gunakan merupakan yang signifikan dan perbaikan akan berlangsung secara keseluruhan ---- smart mobility dan iot mengalami perkembangan cukup signifikan sehingga mengubah cara perusahaan dalam mengelola aset.\r\n",
      "sample:  sertifikat yang diberikan oleh indosat fujitsu indonesia kepada para mitra bangun rumah itu adalah dalam teknologi internet of things ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  di indonesia, kerja sama akan dilakukan di sektor industri itu ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  president sofwan memimpin layanan data realtime di indonesia dan mobilitas merupakan cara yang optimal dalam meningkatkan strategi usaha ---- mobilitas akan menyediakan cara untuk mengoptimalkan strategi perusahaan dengan memanfaatkan aplikasi dan layanan yang dapat memproses data real-time, tutur achmad sofwan, president director fujitsu indonesia.\r\n",
      "sample:  di indonesia, kerja sama akan dilakukan di sektor industri itu ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  indosat ooredoo yang hadir dalam solusi smart mobility di indonesia ---- indosat ooredoo bekerja sama dengan fujitsu indonesia menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  president sofwan merupakan director fujitsu dan mobilitas merupakan cara yang optimal dalam perbaikan data realtime ---- president director fujitsu, achmad sofwan mengungkap, mobilitas akan menyediakan cara mengoptimalisasikan strategi perusahaan dengan memanfaatkan aplikasi dan layanan dalam aktivitas memproses data real-time.\r\n",
      "sample:  di indonesia, kerja sama akan dilakukan di sektor industri itu ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "bleu score on test dataset:  1.7696994497955176\r\n",
      "evaluate on data f-bunuh-diri\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 23\r\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:02<00:00,  2.29it/s]\r\n",
      "sample:  di jembatan yang kira-rangi oleh 15 meter, ia melompat ---- ciptadi dengan nekat melompat dari megatron jembatan penyeberangan orang yang tingginya diperkirakan mencapai 15 meter.\r\n",
      "sample:  hal yang sebut terjadi di kasubag reny 01 di bandung pada saat temu jumat di mapolrestabes bandung ---- hal tersebut diungkapkan kabag ops polrestabes bandung kompol dede rojudin didampingi kasubag humas kompol reny marthaliana, saat ditemui di mapolrestabes bandung, jumat.\r\n",
      "sample:  di bandung, rezha warga kota bandung dibela oleh noviana yang telah bubar di masjid raya bandung jumat ini ---- rezha noviana, warga kota bandung mengatakan ia dan jemaat lainnya langsung memperhatikan megatron jembatan penyeberangan orang depan kantor pos besar bandung setelah bubar salat jumat di masjid raya bandung.\r\n",
      "sample:  satpam linmas berdiri di atas jpo ---- ketika itu, petugas linmas dan beberapa satpam berdiri di atas jpo.\r\n",
      "sample:  aksi bunuh diri terjadi di alun bandung pada saat ini ---- aksi bunuh diri di alun - alun bandung belum lama ini terjadi.\r\n",
      "sample:  di alun-alangi kerumunan orang tua, dirinya terjatuh ---- seorang pria tua nekat menjatuhkan diri dari jpo di alun - alun kota bandung.\r\n",
      "sample:  di alun bandung, seorang saksi mata meledakkan diri di anggota linmas di kota bandung ---- sudirman, anggota linmas satpol pp kota bandung yang menjadi saksi mata aksi bunuh diri di alun - alun bandung itu mengatakan, pria tersebut diperkirakan berusia 50 tahun.\r\n",
      "sample:  di depan bandung, seorang orang itu terjatuh di jembatan seberang ---- seorang lelaki menjatuhkan diri dari megatron jembatan penyeberangan di depan pos giro besar bandung, bandung, jumat.\r\n",
      "sample:  di tengah malam ini, ia mengakhiri hidup di jalan asia afrika ---- seorang lelaki tengah baya nekat mengakhiri hidupnya di tengah kepadatan lalu lintas jalan asia afrika.\r\n",
      "sample:  di bandung, seorang pria misterius membuat jembatan di bandung barat, ia membuat pesan di seberang orang-orang ---- pria misterius yang bunuh diri dari jembatan penyeberangan orang di jalan asia afrika, bandung, jawa barat, membuat pesan sebelum nekat terjun.\r\n",
      "bleu score on test dataset:  8.354429364529265\r\n",
      "evaluate on data g-gempa-dieng\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 19\r\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:02<00:00,  2.34it/s]\r\n",
      "sample:  stasiun banjarnegara jawa tengah yang dibangun oleh badan meteorologi klimatologi geofisika pada pukul 19 00 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara, jawa tengah, menyatakan bahwa gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 pada skala richter.\r\n",
      "sample:  ada pusat gempa di desa kecamatan batang di kabupaten batang ---- pusat gempa diperkirakan berada di desa tanji gugur, kecamatan bawang, kabupaten batang.\r\n",
      "sample:  tutup sementara yang pvmbg umumkan di jalan dekat kawah di desa sumberejo ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  kepala vulkanologi bencana geologi berkata ada banyak gempa yang terjadi pukul 19 00 wib di dataran tinggi dieng ---- kepala pusat vulkanologi dan mitigasi bencana geologi surono mengatakan bahwa gempa yang terjadi di dataran tinggi dieng pukul 19.00 wib terekam sebanyak 86 kali.\r\n",
      "sample:  tutup dilakukan oleh tim ukur di lapangan hingga ada gas bahaya ---- penutupan ini dilakukan hingga tim di lapangan yang mengukur konsentrasi gas menyatakan tidak ada gas yang berbahaya.\r\n",
      "sample:  tutup sementara yang pvmbg umumkan di jalan dekat kawah di desa sumberejo ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  di pusat vulkanologi bencana geologi, terjadi peningkatan aktivitas kawah timbang dan pemerintah memberikan informasi ---- petugas pusat vulkanologi dan mitigasi bencana geologi diterjunkan untuk memantau aktivitas kawah timbang dan memberikan informasi.\r\n",
      "sample:  di pusat vulkanologi mitigasi bencana geologi, terjadi gempa dan gempa terjadi sejak pukul 19 00 wib ---- data yang dikeluarkan dari pusat vulkanologi dan mitigasi bencana geologi menyebutkan gempa mulai terasa pukul 19.00 wib dan telah terekam gempa sebanyak 86 kali.\r\n",
      "sample:  pengungsi mengungsi ke kabupaten banjarnegara pada sabtu pagi ---- pengungsi di kabupaten banjarnegara, pada sabtu pagi mulai meninggalkan pengungsian.\r\n",
      "sample:  stasiun meteorologi klimatologi geofisika yang melakukan guncang gempa kuat 4 skala richter pada pukul 19 00 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara menyatakan gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 skala richter *(sr)*.\r\n",
      "bleu score on test dataset:  11.91103574344951\r\n"
     ]
    }
   ],
   "source": [
    "!./evaluate_indoBART_to_all.sh result linearized_penman_with_tree_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94096796",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T01:59:23.098029Z",
     "iopub.status.busy": "2022-03-15T01:59:23.097175Z",
     "iopub.status.idle": "2022-03-15T01:59:52.472616Z",
     "shell.execute_reply": "2022-03-15T01:59:52.472094Z"
    },
    "papermill": {
     "duration": 31.607444,
     "end_time": "2022-03-15T01:59:52.472741",
     "exception": false,
     "start_time": "2022-03-15T01:59:20.865297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result/ (stored 0%)\r\n",
      "  adding: result/test_label.txt (deflated 60%)\r\n",
      "  adding: result/c-gedung-roboh/ (stored 0%)\r\n",
      "  adding: result/c-gedung-roboh/test_label.txt (deflated 62%)\r\n",
      "  adding: result/c-gedung-roboh/test_generations.txt (deflated 61%)\r\n",
      "  adding: result/c-gedung-roboh/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/g-gempa-dieng/ (stored 0%)\r\n",
      "  adding: result/g-gempa-dieng/test_label.txt (deflated 67%)\r\n",
      "  adding: result/g-gempa-dieng/test_generations.txt (deflated 64%)\r\n",
      "  adding: result/g-gempa-dieng/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/f-bunuh-diri/ (stored 0%)\r\n",
      "  adding: result/f-bunuh-diri/test_label.txt (deflated 68%)\r\n",
      "  adding: result/f-bunuh-diri/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/f-bunuh-diri/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/loss_data.tsv (deflated 34%)\r\n",
      "  adding: result/d-indo-fuji/ (stored 0%)\r\n",
      "  adding: result/d-indo-fuji/test_label.txt (deflated 86%)\r\n",
      "  adding: result/d-indo-fuji/test_generations.txt (deflated 82%)\r\n",
      "  adding: result/d-indo-fuji/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/model/ (stored 0%)\r\n",
      "  adding: result/model/pytorch_model.bin (deflated 8%)\r\n",
      "  adding: result/model/config.json (deflated 63%)\r\n",
      "  adding: result/b-salah-darat/ (stored 0%)\r\n",
      "  adding: result/b-salah-darat/test_label.txt (deflated 66%)\r\n",
      "  adding: result/b-salah-darat/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/b-salah-darat/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/amr_simple_test/ (stored 0%)\r\n",
      "  adding: result/amr_simple_test/test_label.txt (deflated 60%)\r\n",
      "  adding: result/amr_simple_test/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/amr_simple_test/bleu_score_test.txt (stored 0%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r result_without_sta.zip result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85707748",
   "metadata": {
    "papermill": {
     "duration": 2.267559,
     "end_time": "2022-03-15T01:59:57.241040",
     "exception": false,
     "start_time": "2022-03-15T01:59:54.973481",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Finetune indobart with linearized penman + supervised task adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a085fc01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T02:00:02.071187Z",
     "iopub.status.busy": "2022-03-15T02:00:02.069810Z",
     "iopub.status.idle": "2022-03-15T02:00:02.077155Z",
     "shell.execute_reply": "2022-03-15T02:00:02.078057Z"
    },
    "papermill": {
     "duration": 2.415796,
     "end_time": "2022-03-15T02:00:02.078321",
     "exception": false,
     "start_time": "2022-03-15T01:59:59.662525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/train\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/amr-to-text-indonesia/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b080e9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T02:00:06.798774Z",
     "iopub.status.busy": "2022-03-15T02:00:06.797969Z",
     "iopub.status.idle": "2022-03-15T02:00:07.462989Z",
     "shell.execute_reply": "2022-03-15T02:00:07.461587Z"
    },
    "papermill": {
     "duration": 2.908472,
     "end_time": "2022-03-15T02:00:07.463185",
     "exception": false,
     "start_time": "2022-03-15T02:00:04.554713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'result/model': Is a directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm result/* -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "583b8e22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T02:00:12.399985Z",
     "iopub.status.busy": "2022-03-15T02:00:12.399157Z",
     "iopub.status.idle": "2022-03-15T02:55:37.326746Z",
     "shell.execute_reply": "2022-03-15T02:55:37.326163Z"
    },
    "papermill": {
     "duration": 3327.176874,
     "end_time": "2022-03-15T02:55:37.326894",
     "exception": false,
     "start_time": "2022-03-15T02:00:10.150020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing method linearized_penman\r\n",
      "silver data folder ../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/preprocessed_silver_data/train.amr.txt', result_sent_path='data/preprocessed_silver_data/train.sent.txt', source_file_path=None, source_folder_path='../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news')\r\n",
      "100%|███████████████████████████████| 263263/263263 [00:01<00:00, 143807.88it/s]\r\n",
      "100%|███████████████████████████████| 265325/265325 [00:01<00:00, 143553.63it/s]\r\n",
      "100%|███████████████████████████████| 266966/266966 [00:01<00:00, 142496.89it/s]\r\n",
      "100%|███████████████████████████████| 264330/264330 [00:01<00:00, 140457.34it/s]\r\n",
      "100%|███████████████████████████████| 270251/270251 [00:02<00:00, 131774.19it/s]\r\n",
      "100%|███████████████████████████████| 268262/268262 [00:02<00:00, 124443.99it/s]\r\n",
      "100%|███████████████████████████████| 264253/264253 [00:01<00:00, 142240.09it/s]\r\n",
      "100%|███████████████████████████████| 268572/268572 [00:01<00:00, 141634.10it/s]\r\n",
      "100%|███████████████████████████████| 263508/263508 [00:01<00:00, 139312.29it/s]\r\n",
      "100%|███████████████████████████████| 269258/269258 [00:02<00:00, 133014.45it/s]\r\n",
      "total: 154892 pair sent-amr\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  154892\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  38723\r\n",
      "(Epoch 1) TRAIN LOSS:1.4148 LR:0.00010000: 100%|█| 38723/38723 [54:34<00:00, 11.\r\n",
      "(Epoch 1) DEV LOSS:3.4578 LR:0.00010000: 100%|████| 5/5 [00:00<00:00, 34.95it/s]\r\n",
      "bleu score on dev:  8.926911530659217\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:09<00:00,  8.05it/s]\r\n",
      "sample:  ilham meni balon. ---- balon itu ditiup ilham\r\n",
      "sample:  obe menulis puisi. ---- obe menulis puisi\r\n",
      "sample:  saya ketik makalah. ---- saya mengetik makalah\r\n",
      "sample:  timing anak keajaiban. ---- angga anak ajaib\r\n",
      "sample:  ibu itu orang dosen. ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  27.034488093563688\r\n",
      "tensor([[    0,   382,  9338, 40007,   382,   475,   384, 40008,   382,  6353,\r\n",
      "           384,   384,   384,     2, 40002]])\r\n",
      "saya ketik makalah.\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x train_indoBART_on_silver_data.sh\n",
    "!./train_indoBART_on_silver_data.sh linearized_penman ../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85bcd3a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T02:56:15.003875Z",
     "iopub.status.busy": "2022-03-15T02:56:15.003051Z",
     "iopub.status.idle": "2022-03-15T02:56:15.683601Z",
     "shell.execute_reply": "2022-03-15T02:56:15.683078Z"
    },
    "papermill": {
     "duration": 19.883192,
     "end_time": "2022-03-15T02:56:15.683739",
     "exception": false,
     "start_time": "2022-03-15T02:55:55.800547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu_score_test.txt  loss_data.tsv  model  test_generations.txt  test_label.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls result/result_supervised_task_adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bde0e5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T02:56:52.720280Z",
     "iopub.status.busy": "2022-03-15T02:56:52.719500Z",
     "iopub.status.idle": "2022-03-15T02:56:53.410503Z",
     "shell.execute_reply": "2022-03-15T02:56:53.410019Z"
    },
    "papermill": {
     "duration": 19.32622,
     "end_time": "2022-03-15T02:56:53.410639",
     "exception": false,
     "start_time": "2022-03-15T02:56:34.084419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir result/result_linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2e252f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T02:57:30.403436Z",
     "iopub.status.busy": "2022-03-15T02:57:30.402637Z",
     "iopub.status.idle": "2022-03-15T03:00:55.887325Z",
     "shell.execute_reply": "2022-03-15T03:00:55.886814Z"
    },
    "papermill": {
     "duration": 224.26287,
     "end_time": "2022-03-15T03:00:55.887491",
     "exception": false,
     "start_time": "2022-03-15T02:57:11.624621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "resume from checkpoint\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:1.3664 LR:0.00003000: 100%|█| 662/662 [00:43<00:00, 15.07it\r\n",
      "(Epoch 1) DEV LOSS:1.3719 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 54.09it/s]\r\n",
      "bleu score on dev:  15.850247304246803\r\n",
      "(Epoch 2) TRAIN LOSS:0.6530 LR:0.00003000: 100%|█| 662/662 [00:43<00:00, 15.25it\r\n",
      "(Epoch 2) DEV LOSS:1.2040 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 54.56it/s]\r\n",
      "bleu score on dev:  25.825448022682053\r\n",
      "(Epoch 3) TRAIN LOSS:0.4798 LR:0.00003000: 100%|█| 662/662 [00:43<00:00, 15.33it\r\n",
      "(Epoch 3) DEV LOSS:1.3210 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 54.13it/s]\r\n",
      "bleu score on dev:  34.96835291798967\r\n",
      "(Epoch 4) TRAIN LOSS:0.3999 LR:0.00003000: 100%|█| 662/662 [00:43<00:00, 15.22it\r\n",
      "(Epoch 4) DEV LOSS:1.2310 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 53.64it/s]\r\n",
      "bleu score on dev:  37.319852071487816\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:13<00:00,  5.88it/s]\r\n",
      "sample:  balon tersebut berputar oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  saya sedang ketik makalah ---- saya mengetik makalah\r\n",
      "sample:  pacarnya anak keajaiban ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  32.50941738604406\r\n",
      "tensor([[    0,   382,  9338, 40007,   382,   475,   384, 40008,   382,  6353,\r\n",
      "           384,   384,   384,     2, 40002]])\r\n",
      "saya mengetik makalah\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoBART.py --model_type indo-bart --n_epochs 4 --lr 3e-5 --num_beams 10 --data_folder ../data/preprocessed_data/linearized_penman  --result_folder result/result_linearized_penman --resume_from_checkpoint True --saved_model_folder_path result/result_supervised_task_adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e36e0443",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T03:01:34.944226Z",
     "iopub.status.busy": "2022-03-15T03:01:34.943464Z",
     "iopub.status.idle": "2022-03-15T03:02:04.468141Z",
     "shell.execute_reply": "2022-03-15T03:02:04.467643Z"
    },
    "papermill": {
     "duration": 48.700395,
     "end_time": "2022-03-15T03:02:04.468262",
     "exception": false,
     "start_time": "2022-03-15T03:01:15.767867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result/result_linearized_penman/ (stored 0%)\r\n",
      "  adding: result/result_linearized_penman/test_label.txt (deflated 60%)\r\n",
      "  adding: result/result_linearized_penman/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/result_linearized_penman/loss_data.tsv (deflated 34%)\r\n",
      "  adding: result/result_linearized_penman/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/result_linearized_penman/model/ (stored 0%)\r\n",
      "  adding: result/result_linearized_penman/model/pytorch_model.bin (deflated 8%)\r\n",
      "  adding: result/result_linearized_penman/model/config.json (deflated 63%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r result_with_sta.zip result/result_linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b433965f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T03:02:43.091672Z",
     "iopub.status.busy": "2022-03-15T03:02:43.090853Z",
     "iopub.status.idle": "2022-03-15T03:02:43.093908Z",
     "shell.execute_reply": "2022-03-15T03:02:43.094266Z"
    },
    "papermill": {
     "duration": 19.14924,
     "end_time": "2022-03-15T03:02:43.094471",
     "exception": false,
     "start_time": "2022-03-15T03:02:23.945231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/evaluate\n"
     ]
    }
   ],
   "source": [
    "%cd ../evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7cb8b508",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T03:03:22.556192Z",
     "iopub.status.busy": "2022-03-15T03:03:22.555445Z",
     "iopub.status.idle": "2022-03-15T03:03:23.224851Z",
     "shell.execute_reply": "2022-03-15T03:03:23.224324Z"
    },
    "papermill": {
     "duration": 20.367513,
     "end_time": "2022-03-15T03:03:23.224981",
     "exception": false,
     "start_time": "2022-03-15T03:03:02.857468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'result/amr_simple_test': Is a directory\r\n",
      "rm: cannot remove 'result/b-salah-darat': Is a directory\r\n",
      "rm: cannot remove 'result/c-gedung-roboh': Is a directory\r\n",
      "rm: cannot remove 'result/d-indo-fuji': Is a directory\r\n",
      "rm: cannot remove 'result/f-bunuh-diri': Is a directory\r\n",
      "rm: cannot remove 'result/g-gempa-dieng': Is a directory\r\n",
      "rm: cannot remove 'result/linearized_penman': Is a directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm result/* -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "945182d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T03:04:02.079590Z",
     "iopub.status.busy": "2022-03-15T03:04:02.078809Z",
     "iopub.status.idle": "2022-03-15T03:06:03.675487Z",
     "shell.execute_reply": "2022-03-15T03:06:03.674994Z"
    },
    "papermill": {
     "duration": 141.029001,
     "end_time": "2022-03-15T03:06:03.675630",
     "exception": false,
     "start_time": "2022-03-15T03:03:42.646629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model folder ../train/result/result_linearized_penman\r\n",
      "preprocessing method linearized_penman\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data’: File exists\r\n",
      "preprocess amr_simple_test\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/amr_simple_test’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/amr_simple_test/test.amr.txt', result_sent_path='data/test/preprocessed_data/amr_simple_test/test.sent.txt', source_file_path='data/test/amr_simple_test.txt', source_folder_path=None)\r\n",
      "100%|███████████████████████████████████| 2618/2618 [00:00<00:00, 176410.76it/s]\r\n",
      "total: 306 pair sent-amr\r\n",
      "preprocess b-salah-darat\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/b-salah-darat’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/b-salah-darat/test.amr.txt', result_sent_path='data/test/preprocessed_data/b-salah-darat/test.sent.txt', source_file_path='data/test/b-salah-darat.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 676/676 [00:00<00:00, 125836.57it/s]\r\n",
      "total: 32 pair sent-amr\r\n",
      "preprocess c-gedung-roboh\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/c-gedung-roboh’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/c-gedung-roboh/test.amr.txt', result_sent_path='data/test/preprocessed_data/c-gedung-roboh/test.sent.txt', source_file_path='data/test/c-gedung-roboh.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 606/606 [00:00<00:00, 128002.63it/s]\r\n",
      "total: 29 pair sent-amr\r\n",
      "preprocess d-indo-fuji\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/d-indo-fuji’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/d-indo-fuji/test.amr.txt', result_sent_path='data/test/preprocessed_data/d-indo-fuji/test.sent.txt', source_file_path='data/test/d-indo-fuji.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 745/745 [00:00<00:00, 122333.18it/s]\r\n",
      "total: 27 pair sent-amr\r\n",
      "preprocess f-bunuh-diri\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/f-bunuh-diri’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/f-bunuh-diri/test.amr.txt', result_sent_path='data/test/preprocessed_data/f-bunuh-diri/test.sent.txt', source_file_path='data/test/f-bunuh-diri.txt', source_folder_path=None)\r\n",
      "100%|██████████████████████████████████████| 468/468 [00:00<00:00, 89297.35it/s]\r\n",
      "total: 23 pair sent-amr\r\n",
      "preprocess g-gempa-dieng\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/g-gempa-dieng’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/g-gempa-dieng/test.amr.txt', result_sent_path='data/test/preprocessed_data/g-gempa-dieng/test.sent.txt', source_file_path='data/test/g-gempa-dieng.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 412/412 [00:00<00:00, 117267.46it/s]\r\n",
      "total: 19 pair sent-amr\r\n",
      "mkdir: cannot create directory ‘result’: File exists\r\n",
      "evaluate on data amr_simple_test\r\n",
      "mkdir: cannot create directory ‘result/amr_simple_test’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 306\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:13<00:00,  5.80it/s]\r\n",
      "sample:  balon tersebut berputar oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  saya sedang ketik makalah ---- saya mengetik makalah\r\n",
      "sample:  pacarnya anak keajaiban ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "sample:  dia sedang berenang ---- dia sedang berenang\r\n",
      "sample:  buku yang hilang dicari ilham ---- ilham sedang mencari bukunya yang hilang\r\n",
      "sample:  ibu sedang menyapu di halaman rumah ---- ibu sedang menyapu halaman rumah\r\n",
      "sample:  di sawah, ayah sedang membenamkan padi ---- ayah sedang membajak padi di sawah\r\n",
      "sample:  di tepi pantai, andi pergi perkemahan ---- andi pergi berkemah di tepi pantai\r\n",
      "bleu score on test dataset:  32.50941738604406\r\n",
      "evaluate on data b-salah-darat\r\n",
      "mkdir: cannot create directory ‘result/b-salah-darat’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 32\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:04<00:00,  1.68it/s]\r\n",
      "sample:  kantor kantor kantor sama dan pt angkasa pura ii otoritas bandara soekarnoatta dan imigrasi bandara soekarno-hatta untuk membahas insiden penumpang pesawat yang tiba di jakarta, 10 mei (10/1). ---- kantor imigrasi bandara soekarno-hatta bersama kantor otoritas bandara soekarno-hatta dan pt angkasa pura ii membahas insiden penumpang penerbangan pesawat lion air jt 161 yang tiba di jakarta dari singapura 10 mei 2016.\r\n",
      "sample:  kepala kantor imigrasi soekarnohatta alif suadi berkata, kami meeting soal kepada tempo sabtu malam ini (14 mei). ---- kami meeting soal ini,\" ujar kepala kantor imigrasi soekarno-hatta alif suadi kepada tempo, sabtu malam ini, 14 mei 2016.\r\n",
      "sample:  pada pukul 18, anak teman natabilitas dan menggunakan pesawat lion air jt 161 berangkat ke singapura ---- anak temannya natalie berangkat dari singapura dan menggunakan pesawat lion air jt 161, pada pukul 18.50.\r\n",
      "sample:  sekretaris agus haryadi di perusahaan pt angkasa pura ii, mengatakan, pelaku yang mengelola bandara internasional soekarnohatta, pihaknya akan melakukan koordinasi kantor otoritas bandara wilayah (1) berkaitan tbk tbkitiwa ---- sekretaris perusahaan pt angkasa pura ii agus haryadi mengatakan selaku pengelola bandara internasional soekarno-hatta, pihaknya akan berkoordinasi dengan kantor otoritas bandara wilayah i terkait dengan peristiwa ini.\r\n",
      "sample:  di remote area, kami menyiapkan bus antar penumpang di terminal ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "sample:  di padang, edward mengatakan terdapat sopir yang menjemput penumpang dari bus ---- terdapat sopir bus yang akan menjemput penumpang dari padang\" ujar edward.\r\n",
      "sample:  pihak otoritas bandara soetta memanggil pihak yang terkait dan investigasi ---- pihak otoritas bandara soetta akan memanggil pihak terkait dan dilakukan investigasi.\r\n",
      "sample:  sanksi yang diberikan oleh pihak-pihak yang lalai tersebut, ujarnya ---- kemenhub memberikan sanksi kepada pihak-pihak yang lalai,\" ujarnya.\r\n",
      "sample:  dinda berkata, cerita teman yang mengatakan, pesawat itu tidak mendarat di terminal ii ---- pesawat itu mendarat tidak di terminal ii, kata zara mengutip cerita temannya.\r\n",
      "sample:  di remote area, kami menyiapkan bus antar penumpang di terminal ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "bleu score on test dataset:  17.392535680439025\r\n",
      "evaluate on data c-gedung-roboh\r\n",
      "mkdir: cannot create directory ‘result/c-gedung-roboh’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 29\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:03<00:00,  2.04it/s]\r\n",
      "sample:  di atas lampu indomobil, kendaraan yang melaju ke arah bintaro dan tol bintaro menuju jalan boulevard bintaro itu akan dilakukan dengan menggunakan flyover bintaro ---- pengalihan dilakukan bagi pengendara yang melaju dari arah mal bintaro xchange dan tol bintaro menuju jalan boulevard bintaro dengan menggunakan flyover di atas lampu merah indomobil.\r\n",
      "sample:  kegagalan dan kegagalan dalam melakukan tes tanah secara struktur di gedung miring itu terjadi di menara jakarta timur, mt haryono ---- kegagalan struktur dan kesalahan tes tanah yang mengakibatkan gedung miring juga terjadi pada menara saidah di jalan mt haryono, jakarta timur.\r\n",
      "sample:  kapolres ayi supardan akbp tangerang selatan membenarkan peristiwa bintaro roboh gedung yang menarik hati masyarakat banyak ---- kapolres tangerang selatan, akbp ayi supardan membenarkan peristiwa robohnya gedung di bintaro yang menarik perhatian masyarakat banyak.\r\n",
      "sample:  panin memutuskan untuk melanjutkan pembangunan gedung itu, dikarenakan tidak lulus ujian kelayakan ---- panin memutuskan tidak melanjutkan pembangunan gedung itu karena dinyatakan tidak lulus uji kelayakan.\r\n",
      "sample:  di gedung, disebutkan tidak lulus ujian, gedung itu dinyatakan tidak melanjutkan ujian ---- gedung tersebut dinyatakan tidak lulus uji sehingga pembangunan gedung diputuskan tidak dilanjutkan.\r\n",
      "sample:  di tangerang selatan, kasat akp samian dari polres tangerang selatan mengatakan, pelaku yang mengetahui penyebab runtuhnya gedung itu saat ini ---- kasat reskrim polres tangerangg selatan, akp samian mengatakan saat ini pihaknya melakukan penyelidikan untuk mengetahui penyebab runtuhnya gedung.\r\n",
      "sample:  kami masih dalam peristiwa yang disebutkan sampai ini ---- hingga kini kami masih mendalami peristiwa tersebut.\r\n",
      "sample:  jadi, kejadian itu tidak menimbulkan korban ---- kejadian tersebut tidak timbul korban.\r\n",
      "sample:  menjadi rugi material ---- terjadi kerugian material.\r\n",
      "sample:  di sektor bintaro, tangerang selatan 7 gedung itu diduga menyalahi prosedur ---- pembongkaran gedung di sektor 7, bintaro, tangerang selatan, diduga menyalahi prosedur.\r\n",
      "bleu score on test dataset:  16.95549772561969\r\n",
      "evaluate on data d-indo-fuji\r\n",
      "mkdir: cannot create directory ‘result/d-indo-fuji’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 27\r\n",
      "  0%|                                                     | 0/7 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:04<00:00,  1.74it/s]\r\n",
      "sample:  di indonesia, indosat juga menandatangani nota kesepahaman mendalam dalam rangka memantapkan mitra pembangunan yang telah diperbaiki oleh pelanggan kepada bisnis, hadir dalam solusi zahir dan internet of delivery mobility ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  di indonesia, indosat juga menandatangani nota kesepahaman mendalam dalam rangka memantapkan mitra pembangunan yang telah diperbaiki oleh pelanggan kepada bisnis, hadir dalam solusi zahir dan internet of delivery mobility ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  sejauh ini, peralatan perusahaan dalam mengelola aset yang berjalan cukup signifikan dan mengubah cara usaha yang mengubah aset ---- smart mobility dan iot mengalami perkembangan cukup signifikan sehingga mengubah cara perusahaan dalam mengelola aset.\r\n",
      "sample:  di indonesia, indosat juga menandatangani nota kesepahaman mendalam dalam rangka memantapkan mitra pembangunan yang telah diperbaiki oleh pelanggan kepada bisnis, hadir dalam solusi zahir dan internet of delivery mobility ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  kerja sama itu memfokuskan pada sektor transportasi dan otomotif secara lebih luas dan pelanggan yang korporasi di indonesia membutuhkan sektor industri bagai dan ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  president achmad sofwan menuturkan, persediaan peralatan proses data real madrid dan aplikasi mobilitasnya dimanfaatkan dengan cara mengoptimalkan strategi perusahaan itu ---- mobilitas akan menyediakan cara untuk mengoptimalkan strategi perusahaan dengan memanfaatkan aplikasi dan layanan yang dapat memproses data real-time, tutur achmad sofwan, president director fujitsu indonesia.\r\n",
      "sample:  kerja sama itu memfokuskan pada sektor transportasi dan otomotif secara lebih luas dan pelanggan yang korporasi di indonesia membutuhkan sektor industri bagai dan ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  di indonesia, jamali mengerjakan menghadirkan solusi terobosan mobility and internet of peace dan bekerja sama dengannya dengannya ---- indosat ooredoo bekerja sama dengan fujitsu indonesia menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  president achmad sofwan mengungkapkan kesediaannya untuk memanfaatkan layanan proses data real listing dan mobilitasnya dimanfaatkan dengan cara mengoptimalkan strategi usaha ---- president director fujitsu, achmad sofwan mengungkap, mobilitas akan menyediakan cara mengoptimalisasikan strategi perusahaan dengan memanfaatkan aplikasi dan layanan dalam aktivitas memproses data real-time.\r\n",
      "sample:  kerja sama itu memfokuskan pada sektor transportasi dan otomotif secara lebih luas dan pelanggan yang korporasi di indonesia membutuhkan sektor industri bagai dan ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "bleu score on test dataset:  6.044932056702022\r\n",
      "evaluate on data f-bunuh-diri\r\n",
      "mkdir: cannot create directory ‘result/f-bunuh-diri’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 23\r\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:03<00:00,  1.77it/s]\r\n",
      "sample:  di minimarketron jembatan yang tinggi diperkirakan mencapai 1,5 meter itu melompat sebrang orang ---- ciptadi dengan nekat melompat dari megatron jembatan penyeberangan orang yang tingginya diperkirakan mencapai 15 meter.\r\n",
      "sample:  hal itu dalam hal yang disebutkan oleh kasubag reny marthaliana kepada humas kompol yeudin, sabtu (30/9), mengungkapkan, kabaghara di bandung, ops di bandung ---- hal tersebut diungkapkan kabag ops polrestabes bandung kompol dede rojudin didampingi kasubag humas kompol reny marthaliana, saat ditemui di mapolrestabes bandung, jumat.\r\n",
      "sample:  di kota bandung, rezha noviana mengatakan, dirinya juga telah melakukan salat di masjid raya bandung pada jumat dan jemaat lainnya di depan kantor pos besar seberang orang ---- rezha noviana, warga kota bandung mengatakan ia dan jemaat lainnya langsung memperhatikan megatron jembatan penyeberangan orang depan kantor pos besar bandung setelah bubar salat jumat di masjid raya bandung.\r\n",
      "sample:  tugas linmas dan seberapa satpam berdiri di atas jpo ---- ketika itu, petugas linmas dan beberapa satpam berdiri di atas jpo.\r\n",
      "sample:  aksi bunuh diri terjadi di alun bandung sejak lama ---- aksi bunuh diri di alun - alun bandung belum lama ini terjadi.\r\n",
      "sample:  orang tua terjatuh diri dari jpo di kota bandung di alun ---- seorang pria tua nekat menjatuhkan diri dari jpo di alun - alun kota bandung.\r\n",
      "sample:  di bandung, sudirman yang menjadi saksi mata pembunuhan di alun alun kediri mengatakan, dirinya seorang anggota linmas kota bandung satpol pp memperkirakan pria yang disebutnya sebagai 30 tahun ---- sudirman, anggota linmas satpol pp kota bandung yang menjadi saksi mata aksi bunuh diri di alun - alun bandung itu mengatakan, pria tersebut diperkirakan berusia 50 tahun.\r\n",
      "sample:  pada jumat, seorang lelaki itu, dirinya terjatuh di giro besar bandung di jembatan seberang ---- seorang lelaki menjatuhkan diri dari megatron jembatan penyeberangan di depan pos giro besar bandung, bandung, jumat.\r\n",
      "sample:  di tengah baya, hidup diakhiri di tengah lalu lintas lalu lintas di jalan asia afrika ---- seorang lelaki tengah baya nekat mengakhiri hidupnya di tengah kepadatan lalu lintas jalan asia afrika.\r\n",
      "sample:  pria misterius bunuh di jalanan bandung, jawa barat, di asia barat, afrika di seberang orang membuat pesanan belum terjun nekat ---- pria misterius yang bunuh diri dari jembatan penyeberangan orang di jalan asia afrika, bandung, jawa barat, membuat pesan sebelum nekat terjun.\r\n",
      "bleu score on test dataset:  16.791518205042827\r\n",
      "evaluate on data g-gempa-dieng\r\n",
      "mkdir: cannot create directory ‘result/g-gempa-dieng’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 19\r\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:03<00:00,  1.63it/s]\r\n",
      "sample:  di banjarnegara, jawa tengah (badan meteorologi andologi geofisika), sebuah gempa berkekuatan 4 8 skala richter di dataran tinggi dieng pada pukul 19/0 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara, jawa tengah, menyatakan bahwa gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 pada skala richter.\r\n",
      "sample:  di desa kabupaten batang, pusat gempa itu diperkirakan ada di tanji gugur ---- pusat gempa diperkirakan berada di desa tanji gugur, kecamatan bawang, kabupaten batang.\r\n",
      "sample:  bibimbg merekomendasikan untuk menutup sementara jalan yang berdekatan dengan desa sumberejo dengan pertimbangan secara keseluruhan ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  kepala surono pusat vulkanologi mitigasi bencana geologi mengatakan, banyak kali rekam gempa terjadi pukul 19/0 wib di dataran tinggi dieng ---- kepala pusat vulkanologi dan mitigasi bencana geologi surono mengatakan bahwa gempa yang terjadi di dataran tinggi dieng pukul 19.00 wib terekam sebanyak 86 kali.\r\n",
      "sample:  pada saat ini, dilakukan suatu tim yang mengukur konsentrasi gas berbahaya di lapangan karena tidak ada gas berbahaya ---- penutupan ini dilakukan hingga tim di lapangan yang mengukur konsentrasi gas menyatakan tidak ada gas yang berbahaya.\r\n",
      "sample:  bibimbg merekomendasikan untuk menutup sementara jalan yang berdekatan dengan desa sumberejo dengan pertimbangan secara keseluruhan ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  petugas pusat vulkanologi mitigasi bencana geologi menerjunkan informasi untuk memantau aktivitas kawah timbang dan memberikan informasi ---- petugas pusat vulkanologi dan mitigasi bencana geologi diterjunkan untuk memantau aktivitas kawah timbang dan memberikan informasi.\r\n",
      "sample:  data yang dikeluarkan pusat vulkanologi mitigasi bencana geologi dan telah rekam gempa sebanyak 86 kali menyebutkan, gempa pukul 19/0 wib ---- data yang dikeluarkan dari pusat vulkanologi dan mitigasi bencana geologi menyebutkan gempa mulai terasa pukul 19.00 wib dan telah terekam gempa sebanyak 86 kali.\r\n",
      "sample:  pengungsi meninggal di kabupaten banjarnegara pada sabtu pagi ---- pengungsi di kabupaten banjarnegara, pada sabtu pagi mulai meninggalkan pengungsian.\r\n",
      "sample:  stasiun banjarnegara (badan meteorologi cipto mangunatologi geofisika) menyatakan bahwa gempa berkekuatan 4 8 skala richter di dataran tinggi dieng pukul 19/0 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara menyatakan gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 skala richter *(sr)*.\r\n",
      "bleu score on test dataset:  21.847520052209163\r\n",
      "mkdir: cannot create directory ‘result/linearized_penman’: File exists\r\n",
      "mv: cannot move 'result/amr_simple_test' to 'result/linearized_penman/amr_simple_test': Directory not empty\r\n",
      "mv: cannot move 'result/b-salah-darat' to 'result/linearized_penman/b-salah-darat': Directory not empty\r\n",
      "mv: cannot move 'result/c-gedung-roboh' to 'result/linearized_penman/c-gedung-roboh': Directory not empty\r\n",
      "mv: cannot move 'result/d-indo-fuji' to 'result/linearized_penman/d-indo-fuji': Directory not empty\r\n",
      "mv: cannot move 'result/f-bunuh-diri' to 'result/linearized_penman/f-bunuh-diri': Directory not empty\r\n",
      "mv: cannot move 'result/g-gempa-dieng' to 'result/linearized_penman/g-gempa-dieng': Directory not empty\r\n",
      "mv: cannot move 'result/linearized_penman' to a subdirectory of itself, 'result/linearized_penman/linearized_penman'\r\n"
     ]
    }
   ],
   "source": [
    "!./evaluate_indoBART_to_all.sh ../train/result/result_linearized_penman linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05396f0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T03:06:42.841915Z",
     "iopub.status.busy": "2022-03-15T03:06:42.841112Z",
     "iopub.status.idle": "2022-03-15T03:06:43.509110Z",
     "shell.execute_reply": "2022-03-15T03:06:43.508204Z"
    },
    "papermill": {
     "duration": 20.484965,
     "end_time": "2022-03-15T03:06:43.509255",
     "exception": false,
     "start_time": "2022-03-15T03:06:23.024290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result/ (stored 0%)\r\n",
      "  adding: result/c-gedung-roboh/ (stored 0%)\r\n",
      "  adding: result/c-gedung-roboh/test_label.txt (deflated 62%)\r\n",
      "  adding: result/c-gedung-roboh/test_generations.txt (deflated 61%)\r\n",
      "  adding: result/c-gedung-roboh/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/g-gempa-dieng/ (stored 0%)\r\n",
      "  adding: result/g-gempa-dieng/test_label.txt (deflated 67%)\r\n",
      "  adding: result/g-gempa-dieng/test_generations.txt (deflated 65%)\r\n",
      "  adding: result/g-gempa-dieng/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/test_label.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/bleu_score_test.txt (deflated 12%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/test_label.txt (deflated 67%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/test_generations.txt (deflated 66%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/test_label.txt (deflated 68%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/test_generations.txt (deflated 65%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/test_label.txt (deflated 86%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/test_generations.txt (deflated 86%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/test_label.txt (deflated 66%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/test_generations.txt (deflated 66%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/test_label.txt (deflated 60%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/f-bunuh-diri/ (stored 0%)\r\n",
      "  adding: result/f-bunuh-diri/test_label.txt (deflated 68%)\r\n",
      "  adding: result/f-bunuh-diri/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/f-bunuh-diri/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/d-indo-fuji/ (stored 0%)\r\n",
      "  adding: result/d-indo-fuji/test_label.txt (deflated 86%)\r\n",
      "  adding: result/d-indo-fuji/test_generations.txt (deflated 83%)\r\n",
      "  adding: result/d-indo-fuji/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/b-salah-darat/ (stored 0%)\r\n",
      "  adding: result/b-salah-darat/test_label.txt (deflated 66%)\r\n",
      "  adding: result/b-salah-darat/test_generations.txt (deflated 64%)\r\n",
      "  adding: result/b-salah-darat/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/amr_simple_test/ (stored 0%)\r\n",
      "  adding: result/amr_simple_test/test_label.txt (deflated 60%)\r\n",
      "  adding: result/amr_simple_test/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/amr_simple_test/bleu_score_test.txt (stored 0%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r result_with_sta.zip result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f6358c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T03:07:22.817667Z",
     "iopub.status.busy": "2022-03-15T03:07:22.816872Z",
     "iopub.status.idle": "2022-03-15T03:07:23.471999Z",
     "shell.execute_reply": "2022-03-15T03:07:23.471532Z"
    },
    "papermill": {
     "duration": 20.444321,
     "end_time": "2022-03-15T03:07:23.472128",
     "exception": false,
     "start_time": "2022-03-15T03:07:03.027807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate_indoBART.py\t     result_without_sta.zip\r\n",
      "evaluate_indoBART_to_all.sh  zeroshot_evaluate_indoBART.py\r\n",
      "evaluate_indoT5.py\t     zeroshot_evaluate_indoBART_to_all.sh\r\n",
      "evaluate_indoT5_to_all.sh    zeroshot_evaluate_indoT5.py\r\n",
      "result\t\t\t     zeroshot_evaluate_indoT5_to_all.sh\r\n",
      "result_with_sta.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146a9d3b",
   "metadata": {
    "papermill": {
     "duration": 19.610286,
     "end_time": "2022-03-15T03:08:02.314561",
     "exception": false,
     "start_time": "2022-03-15T03:07:42.704275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## linearized penman + tree level embeddings + supervised task adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64b5d611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T03:08:41.962150Z",
     "iopub.status.busy": "2022-03-15T03:08:41.961223Z",
     "iopub.status.idle": "2022-03-15T03:08:41.965292Z",
     "shell.execute_reply": "2022-03-15T03:08:41.964871Z"
    },
    "papermill": {
     "duration": 19.690586,
     "end_time": "2022-03-15T03:08:41.965427",
     "exception": false,
     "start_time": "2022-03-15T03:08:22.274841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/tree_level_embeddings\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/amr-to-text-indonesia/tree_level_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcc63a99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T03:09:20.559253Z",
     "iopub.status.busy": "2022-03-15T03:09:20.558234Z",
     "iopub.status.idle": "2022-03-15T03:09:21.225451Z",
     "shell.execute_reply": "2022-03-15T03:09:21.224866Z"
    },
    "papermill": {
     "duration": 19.676283,
     "end_time": "2022-03-15T03:09:21.225605",
     "exception": false,
     "start_time": "2022-03-15T03:09:01.549322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'result/amr_simple_test': Is a directory\r\n",
      "rm: cannot remove 'result/b-salah-darat': Is a directory\r\n",
      "rm: cannot remove 'result/c-gedung-roboh': Is a directory\r\n",
      "rm: cannot remove 'result/d-indo-fuji': Is a directory\r\n",
      "rm: cannot remove 'result/f-bunuh-diri': Is a directory\r\n",
      "rm: cannot remove 'result/g-gempa-dieng': Is a directory\r\n",
      "rm: cannot remove 'result/model': Is a directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm result/* -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c96cb951",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T03:10:01.039063Z",
     "iopub.status.busy": "2022-03-15T03:10:01.038228Z",
     "iopub.status.idle": "2022-03-15T04:20:36.659722Z",
     "shell.execute_reply": "2022-03-15T04:20:36.658907Z"
    },
    "papermill": {
     "duration": 4255.707315,
     "end_time": "2022-03-15T04:20:36.659883",
     "exception": false,
     "start_time": "2022-03-15T03:09:40.952568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing method linearized_penman_with_tree_level\r\n",
      "silver data folder ../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news\r\n",
      "mkdir: cannot create directory ‘data/preprocessed_silver_data’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/preprocessed_silver_data/train.amr.txt', result_sent_path='data/preprocessed_silver_data/train.sent.txt', source_file_path=None, source_folder_path='../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news')\r\n",
      "100%|███████████████████████████████| 263263/263263 [00:02<00:00, 124081.65it/s]\r\n",
      "15995\r\n",
      "100%|███████████████████████████████| 265325/265325 [00:01<00:00, 140518.15it/s]\r\n",
      "16077\r\n",
      "100%|███████████████████████████████| 266966/266966 [00:01<00:00, 141333.87it/s]\r\n",
      "14930\r\n",
      "100%|███████████████████████████████| 264330/264330 [00:01<00:00, 136485.66it/s]\r\n",
      "15942\r\n",
      "100%|███████████████████████████████| 270251/270251 [00:01<00:00, 139245.15it/s]\r\n",
      "14986\r\n",
      "100%|███████████████████████████████| 268262/268262 [00:01<00:00, 137674.36it/s]\r\n",
      "14991\r\n",
      "100%|███████████████████████████████| 264253/264253 [00:01<00:00, 137695.54it/s]\r\n",
      "16046\r\n",
      "100%|███████████████████████████████| 268572/268572 [00:01<00:00, 140746.02it/s]\r\n",
      "14937\r\n",
      "100%|███████████████████████████████| 263508/263508 [00:01<00:00, 135772.76it/s]\r\n",
      "15926\r\n",
      "100%|███████████████████████████████| 269258/269258 [00:02<00:00, 130010.89it/s]\r\n",
      "15062\r\n",
      "total: 154892  tuple_sent_amr_level\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "Some weights of MBartForConditionalGeneration were not initialized from the model checkpoint at indobenchmark/indobart and are newly initialized: ['model.encoder.tree_embed.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  154892\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  38723\r\n",
      "(Epoch 1) TRAIN LOSS:1.0846 LR:0.00003000: 100%|█| 38723/38723 [1:09:25<00:00,  \r\n",
      "(Epoch 1) DEV LOSS:3.3207 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 37.77it/s]\r\n",
      "bleu score on dev:  15.516440234491993\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:14<00:00,  5.15it/s]\r\n",
      "sample:  ilham tiup balon. ---- balon itu ditiup ilham\r\n",
      "sample:  obe menulis puisi. ---- obe menulis puisi\r\n",
      "sample:  saya ketik makalahnya. ---- saya mengetik makalah\r\n",
      "sample:  angga anak ajaib. ---- angga anak ajaib\r\n",
      "sample:  ibu itu orang dosen. ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  32.30589986700964\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x train_indoBART_on_silver_data.sh\n",
    "!./train_indoBART_on_silver_data.sh linearized_penman_with_tree_level ../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21e81d51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T04:21:51.645491Z",
     "iopub.status.busy": "2022-03-15T04:21:51.644678Z",
     "iopub.status.idle": "2022-03-15T04:25:42.443327Z",
     "shell.execute_reply": "2022-03-15T04:25:42.442855Z"
    },
    "papermill": {
     "duration": 267.575828,
     "end_time": "2022-03-15T04:25:42.443501",
     "exception": false,
     "start_time": "2022-03-15T04:21:14.867673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "resume from checkpoint\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:0.9047 LR:0.00003000: 100%|█| 662/662 [00:49<00:00, 13.25it\r\n",
      "(Epoch 1) DEV LOSS:0.9938 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 40.16it/s]\r\n",
      "bleu score on dev:  27.526968946317435\r\n",
      "(Epoch 2) TRAIN LOSS:0.4812 LR:0.00003000: 100%|█| 662/662 [00:49<00:00, 13.41it\r\n",
      "(Epoch 2) DEV LOSS:1.1800 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 37.16it/s]\r\n",
      "bleu score on dev:  27.17507557678638\r\n",
      "(Epoch 3) TRAIN LOSS:0.3940 LR:0.00003000: 100%|█| 662/662 [00:50<00:00, 13.16it\r\n",
      "(Epoch 3) DEV LOSS:1.0179 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 40.38it/s]\r\n",
      "bleu score on dev:  29.330503346163088\r\n",
      "(Epoch 4) TRAIN LOSS:0.3510 LR:0.00003000: 100%|█| 662/662 [00:50<00:00, 13.20it\r\n",
      "(Epoch 4) DEV LOSS:1.0130 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 36.73it/s]\r\n",
      "bleu score on dev:  29.069134738343084\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:13<00:00,  5.55it/s]\r\n",
      "sample:  balon dituang oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  saya sedang mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  badannya anak yang ajaib ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  31.884585478751454\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoBART.py --model_type indo-bart --n_epochs 4 --lr 3e-5 --num_beams 10 --data_folder ../data/preprocessed_data/linearized_penman_with_tree_level  --result_folder result --resume_from_checkpoint True --saved_model_folder_path result/result_supervised_task_adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca3c9391",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T04:26:58.136969Z",
     "iopub.status.busy": "2022-03-15T04:26:58.136174Z",
     "iopub.status.idle": "2022-03-15T04:29:02.803006Z",
     "shell.execute_reply": "2022-03-15T04:29:02.802282Z"
    },
    "papermill": {
     "duration": 162.161246,
     "end_time": "2022-03-15T04:29:02.803143",
     "exception": false,
     "start_time": "2022-03-15T04:26:20.641897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model folder result\r\n",
      "preprocessing method linearized_penman_with_tree_level\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data’: File exists\r\n",
      "preprocess amr_simple_test\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/amr_simple_test’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/amr_simple_test/test.amr.txt', result_sent_path='data/test/preprocessed_data/amr_simple_test/test.sent.txt', source_file_path='data/test/amr_simple_test.txt', source_folder_path=None)\r\n",
      "100%|███████████████████████████████████| 2618/2618 [00:00<00:00, 180927.78it/s]\r\n",
      "total: 306  tuple_sent_amr_level\r\n",
      "preprocess b-salah-darat\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/b-salah-darat’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/b-salah-darat/test.amr.txt', result_sent_path='data/test/preprocessed_data/b-salah-darat/test.sent.txt', source_file_path='data/test/b-salah-darat.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 676/676 [00:00<00:00, 120330.58it/s]\r\n",
      "total: 32  tuple_sent_amr_level\r\n",
      "preprocess c-gedung-roboh\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/c-gedung-roboh’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/c-gedung-roboh/test.amr.txt', result_sent_path='data/test/preprocessed_data/c-gedung-roboh/test.sent.txt', source_file_path='data/test/c-gedung-roboh.txt', source_folder_path=None)\r\n",
      "100%|██████████████████████████████████████| 606/606 [00:00<00:00, 98182.49it/s]\r\n",
      "total: 29  tuple_sent_amr_level\r\n",
      "preprocess d-indo-fuji\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/d-indo-fuji’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/d-indo-fuji/test.amr.txt', result_sent_path='data/test/preprocessed_data/d-indo-fuji/test.sent.txt', source_file_path='data/test/d-indo-fuji.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 745/745 [00:00<00:00, 120866.30it/s]\r\n",
      "total: 27  tuple_sent_amr_level\r\n",
      "preprocess f-bunuh-diri\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/f-bunuh-diri’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/f-bunuh-diri/test.amr.txt', result_sent_path='data/test/preprocessed_data/f-bunuh-diri/test.sent.txt', source_file_path='data/test/f-bunuh-diri.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 468/468 [00:00<00:00, 128556.83it/s]\r\n",
      "total: 23  tuple_sent_amr_level\r\n",
      "preprocess g-gempa-dieng\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/g-gempa-dieng’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/g-gempa-dieng/test.amr.txt', result_sent_path='data/test/preprocessed_data/g-gempa-dieng/test.sent.txt', source_file_path='data/test/g-gempa-dieng.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 412/412 [00:00<00:00, 103013.61it/s]\r\n",
      "total: 19  tuple_sent_amr_level\r\n",
      "evaluate on data amr_simple_test\r\n",
      "mkdir: cannot create directory ‘result/amr_simple_test’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 306\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:14<00:00,  5.36it/s]\r\n",
      "sample:  balon dituang oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  saya sedang mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  badannya anak yang ajaib ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "sample:  dia sedang berenang ---- dia sedang berenang\r\n",
      "sample:  buku yang dicari oleh ilham ---- ilham sedang mencari bukunya yang hilang\r\n",
      "sample:  di halaman rumah, ibu sedang menyapu ---- ibu sedang menyapu halaman rumah\r\n",
      "sample:  padi ditanam oleh ayah di sawah ---- ayah sedang membajak padi di sawah\r\n",
      "sample:  andi pergi ke tepi pantai ---- andi pergi berkemah di tepi pantai\r\n",
      "bleu score on test dataset:  31.884585478751454\r\n",
      "evaluate on data b-salah-darat\r\n",
      "mkdir: cannot create directory ‘result/b-salah-darat’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 32\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:04<00:00,  1.68it/s]\r\n",
      "sample:  kantor bersama pt angkasa pura ii dan otoritas bandara soekarno-hatta membahas insiden penerbangan pesawat yang tiba di jakarta dari singapura 10 mei 2016 di lion air jt 161 ---- kantor imigrasi bandara soekarno-hatta bersama kantor otoritas bandara soekarno-hatta dan pt angkasa pura ii membahas insiden penumpang penerbangan pesawat lion air jt 161 yang tiba di jakarta dari singapura 10 mei 2016.\r\n",
      "sample:  kepala kantor imigrasi soekarnohatta alif suadi berkata, kami meeting soal kepada tempo sabtu malam ini, 14 mei 2016 ---- kami meeting soal ini,\" ujar kepala kantor imigrasi soekarno-hatta alif suadi kepada tempo, sabtu malam ini, 14 mei 2016.\r\n",
      "sample:  anak temannya natalie berangkat dan menggunakan pesawat lion air jt 161 pukul 18 55 di singapura ---- anak temannya natalie berangkat dari singapura dan menggunakan pesawat lion air jt 161, pada pukul 18.50.\r\n",
      "sample:  direktur agus haryadi kepada perusahaan pt angkasa pura ii mengatakan, pelaku yang mengelola bandara internasional soekarno-hatta akan melakukan koordinasi di kantor otoritas bandara wilayah 1 berkaitan perisitiwa ---- sekretaris perusahaan pt angkasa pura ii agus haryadi mengatakan selaku pengelola bandara internasional soekarno-hatta, pihaknya akan berkoordinasi dengan kantor otoritas bandara wilayah i terkait dengan peristiwa ini.\r\n",
      "sample:  di remote area, kami menyiapkan bus pengantar penumpang ke terminal ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "sample:  kata edward, terdapat sopir yang menjemput penumpang di padang dari bus ---- terdapat sopir bus yang akan menjemput penumpang dari padang\" ujar edward.\r\n",
      "sample:  pihak otoritas bandara soetta memanggil pihak terkait dan melakukan investigasi ---- pihak otoritas bandara soetta akan memanggil pihak terkait dan dilakukan investigasi.\r\n",
      "sample:  kripik memberikan sanksi kepada pihak-pihak yang lalai ---- kemenhub memberikan sanksi kepada pihak-pihak yang lalai,\" ujarnya.\r\n",
      "sample:  zara berkata mengutip cerita temannya, pesawat itu tidak mendarat di terminal ii ---- pesawat itu mendarat tidak di terminal ii, kata zara mengutip cerita temannya.\r\n",
      "sample:  di remote area, kami menyiapkan bus pengantar penumpang ke terminal ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "bleu score on test dataset:  22.070899856828632\r\n",
      "evaluate on data c-gedung-roboh\r\n",
      "mkdir: cannot create directory ‘result/c-gedung-roboh’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 29\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:03<00:00,  2.04it/s]\r\n",
      "sample:  kendara melaju ke arah mal bintaro xchange dan tol bintaro menuju jalan boulevard bintaro dilakukan dengan flyover di atas lampu indomobil merah ---- pengalihan dilakukan bagi pengendara yang melaju dari arah mal bintaro xchange dan tol bintaro menuju jalan boulevard bintaro dengan menggunakan flyover di atas lampu merah indomobil.\r\n",
      "sample:  kegagalan dan salah satu tes tanah menjadi struktur di gedung miring di jalan mt haryono oleh saidah ---- kegagalan struktur dan kesalahan tes tanah yang mengakibatkan gedung miring juga terjadi pada menara saidah di jalan mt haryono, jakarta timur.\r\n",
      "sample:  kapolres ayi supardan akbp di tangerang selatan membenarkan peristiwa di bintaro yang roboh gedung itu menarik hati masyarakat banyak ---- kapolres tangerang selatan, akbp ayi supardan membenarkan peristiwa robohnya gedung di bintaro yang menarik perhatian masyarakat banyak.\r\n",
      "sample:  panin memutuskan untuk melanjutkan pembangunan gedung itu dikarenakan tidak lulus uji kelayakan ---- panin memutuskan tidak melanjutkan pembangunan gedung itu karena dinyatakan tidak lulus uji kelayakan.\r\n",
      "sample:  gedung itu disebutnya dinyatakan tidak lulus ujian sehingga gedung itu putus ---- gedung tersebut dinyatakan tidak lulus uji sehingga pembangunan gedung diputuskan tidak dilanjutkan.\r\n",
      "sample:  di tangerang selatan, kasat akp sam sam samian mengatakan, pihaknya melakukan penyelidikan saat ini dan mengetahui penyebab runtuhnya gedung tersebut ---- kasat reskrim polres tangerangg selatan, akp samian mengatakan saat ini pihaknya melakukan penyelidikan untuk mengetahui penyebab runtuhnya gedung.\r\n",
      "sample:  sampai ini, kami masih dalam peristiwa yang disebut ---- hingga kini kami masih mendalami peristiwa tersebut.\r\n",
      "sample:  kejadian tersebut tidak menimbulkan korban ---- kejadian tersebut tidak timbul korban.\r\n",
      "sample:  terjadi kerugian material ---- terjadi kerugian material.\r\n",
      "sample:  di sektor bintaro, gedung itu dibongkar oleh 7 orang-orang ---- pembongkaran gedung di sektor 7, bintaro, tangerang selatan, diduga menyalahi prosedur.\r\n",
      "bleu score on test dataset:  22.367020443875255\r\n",
      "evaluate on data d-indo-fuji\r\n",
      "mkdir: cannot create directory ‘result/d-indo-fuji’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 27\r\n",
      "  0%|                                                     | 0/7 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:05<00:00,  1.38it/s]\r\n",
      "sample:  indosat bersama fujitsu indonesia menandatangani nota kesepahaman mendalam dalam rangka memperkuat mitra pembangunan yang telah didukung oleh langgan bisnis itu dalam rangka menghadirkan solusi smart dan internet of things kepada mobility ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  indosat bersama fujitsu indonesia menandatangani nota kesepahaman mendalam dalam rangka memperkuat mitra pembangunan yang telah didukung oleh langgan bisnis itu dalam rangka menghadirkan solusi smart dan internet of things kepada mobility ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  teknologi mobility iot dan pengembangan cukup signifikan dari peserta yang mengubah cara usaha yang dikelola oleh aset ---- smart mobility dan iot mengalami perkembangan cukup signifikan sehingga mengubah cara perusahaan dalam mengelola aset.\r\n",
      "sample:  indosat bersama fujitsu indonesia menandatangani nota kesepahaman mendalam dalam rangka memperkuat mitra pembangunan yang telah didukung oleh langgan bisnis itu dalam rangka menghadirkan solusi smart dan internet of things kepada mobility ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  kerja sama sektor transportasi dan otomotif terfokus pada perluasan sektor industri bagai dan kebutuhan pelanggan korporasi indonesia ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  president achmad sofwan kepada director fujitsu indonesia menuturkan, layanan memproses data realtime dan mobilitasnya dimanfaatkan sebagai cara optimal strategi usaha ---- mobilitas akan menyediakan cara untuk mengoptimalkan strategi perusahaan dengan memanfaatkan aplikasi dan layanan yang dapat memproses data real-time, tutur achmad sofwan, president director fujitsu indonesia.\r\n",
      "sample:  kerja sama sektor transportasi dan otomotif terfokus pada perluasan sektor industri bagai dan kebutuhan pelanggan korporasi indonesia ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  indosat ooredoo mengerjakan solusi smart mobility dan internet of things bekerja sama dengan fujitsu indonesia ---- indosat ooredoo bekerja sama dengan fujitsu indonesia menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  president achmad sofwan kepada director fujitsu mengungkapkan, layanan yang memproses data realtime dan aplikasi tersebut diberdayakan oleh mobilitas dengan cara optimal strategi usaha ---- president director fujitsu, achmad sofwan mengungkap, mobilitas akan menyediakan cara mengoptimalisasikan strategi perusahaan dengan memanfaatkan aplikasi dan layanan dalam aktivitas memproses data real-time.\r\n",
      "sample:  kerja sama sektor transportasi dan otomotif terfokus pada perluasan sektor industri bagai dan kebutuhan pelanggan korporasi indonesia ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "bleu score on test dataset:  14.002691307560962\r\n",
      "evaluate on data f-bunuh-diri\r\n",
      "mkdir: cannot create directory ‘result/f-bunuh-diri’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 23\r\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:03<00:00,  1.64it/s]\r\n",
      "sample:  syifaron melompat di jembatan yang tinggi mencapai 15 meter sebrang orang ---- ciptadi dengan nekat melompat dari megatron jembatan penyeberangan orang yang tingginya diperkirakan mencapai 15 meter.\r\n",
      "sample:  hal yang disebutnya diungkapkan oleh kasubag reny marthaliana kepada humas akp rojudin di kompol ops polrestabes bandung pada jumat ini, ditemui di mapolrestabes bandung ---- hal tersebut diungkapkan kabag ops polrestabes bandung kompol dede rojudin didampingi kasubag humas kompol reny marthaliana, saat ditemui di mapolrestabes bandung, jumat.\r\n",
      "sample:  rezha warga kota bandung kepada noviana berkata telah salat di masjid raya bandung jumat dan ia juga jemaat lain di depan kantor pos besar di jembatan penyeberangan orang ---- rezha noviana, warga kota bandung mengatakan ia dan jemaat lainnya langsung memperhatikan megatron jembatan penyeberangan orang depan kantor pos besar bandung setelah bubar salat jumat di masjid raya bandung.\r\n",
      "sample:  petugas linmas berdiri dan satpam yang berdiri di atas jpo ---- ketika itu, petugas linmas dan beberapa satpam berdiri di atas jpo.\r\n",
      "sample:  aksi bunuh diri terjadi di alun bandung dari alun belum lama ---- aksi bunuh diri di alun - alun bandung belum lama ini terjadi.\r\n",
      "sample:  orang tua terjatuh oleh dirinya dari jpo di alun kota bandung ---- seorang pria tua nekat menjatuhkan diri dari jpo di alun - alun kota bandung.\r\n",
      "sample:  sudirman yang menjadi saksi mata yang dibunuh di alun bandung oleh satpol pp mengatakan, dirinya memperkirakan pria yang disebutnya 30 tahun ---- sudirman, anggota linmas satpol pp kota bandung yang menjadi saksi mata aksi bunuh diri di alun - alun bandung itu mengatakan, pria tersebut diperkirakan berusia 50 tahun.\r\n",
      "sample:  di depan bandung, seorang lelaki itu terjatuh di pos giro besar bandung di jembatan penyeberangan pada jumat ini ---- seorang lelaki menjatuhkan diri dari megatron jembatan penyeberangan di depan pos giro besar bandung, bandung, jumat.\r\n",
      "sample:  di tengah baya, ia mengakhiri hidup di tengah lalu lintas di jalan asia afrika ---- seorang lelaki tengah baya nekat mengakhiri hidupnya di tengah kepadatan lalu lintas jalan asia afrika.\r\n",
      "sample:  pria misterius yang membunuh di jembatan di jalan bandung, jawa barat, di asia afrika di seberang orang, membuat dirinya belum terjun nekat ---- pria misterius yang bunuh diri dari jembatan penyeberangan orang di jalan asia afrika, bandung, jawa barat, membuat pesan sebelum nekat terjun.\r\n",
      "bleu score on test dataset:  21.81696368139144\r\n",
      "evaluate on data g-gempa-dieng\r\n",
      "mkdir: cannot create directory ‘result/g-gempa-dieng’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 19\r\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:02<00:00,  1.84it/s]\r\n",
      "sample:  stasiun di banjarnegara, jawa tengah, badan meteorologi klimatologi geofisika, menyatakan gempa berkekuatan 4 8 skala richter di dataran tinggi dieng pukul 19 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara, jawa tengah, menyatakan bahwa gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 pada skala richter.\r\n",
      "sample:  dari kecamatan batang, pusat gempa memperkirakan ada di desa tanji gugur ---- pusat gempa diperkirakan berada di desa tanji gugur, kecamatan bawang, kabupaten batang.\r\n",
      "sample:  pvmbg merekomendasikan untuk menutup sementara jalan yang berdekatan dengan kawah di desa sumberejo secara keseluruhan ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  kepala surono mengatakan, pusat vulkanologi mitigasi bencana geologi sebanyak 86 kali merekam gempa yang terjadi pukul 19 wib di dataran tinggi dieng ---- kepala pusat vulkanologi dan mitigasi bencana geologi surono mengatakan bahwa gempa yang terjadi di dataran tinggi dieng pukul 19.00 wib terekam sebanyak 86 kali.\r\n",
      "sample:  sampai saat ini, tim ukur konsentrasi gas dilakukan di lapangan untuk tidak ada gas berbahaya ---- penutupan ini dilakukan hingga tim di lapangan yang mengukur konsentrasi gas menyatakan tidak ada gas yang berbahaya.\r\n",
      "sample:  pvmbg merekomendasikan untuk menutup sementara jalan yang berdekatan dengan kawah di desa sumberejo secara keseluruhan ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  petugas pusat vulkanologi mitigasi bencana geologi menerjunkan untuk memantau aktivitas kawah timbang dan memberikan informasi ---- petugas pusat vulkanologi dan mitigasi bencana geologi diterjunkan untuk memantau aktivitas kawah timbang dan memberikan informasi.\r\n",
      "sample:  data yang dikeluarkan oleh pusat vulkanologi mitigasi bencana geologi menyebutkan, gempa dirasakan banyak kali dan gempa dimulai pukul 19 wib ---- data yang dikeluarkan dari pusat vulkanologi dan mitigasi bencana geologi menyebutkan gempa mulai terasa pukul 19.00 wib dan telah terekam gempa sebanyak 86 kali.\r\n",
      "sample:  pengungsi tinggal di kabupaten banjarnegara pada sabtu pagi ---- pengungsi di kabupaten banjarnegara, pada sabtu pagi mulai meninggalkan pengungsian.\r\n",
      "sample:  stasiun banjarnegara badan meteorologi klimatologi geofisika geofisika menyatakan gempa berkekuatan 4 8 skala richter di dataran tinggi dieng pukul 19 00 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara menyatakan gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 skala richter *(sr)*.\r\n",
      "bleu score on test dataset:  25.413898160295428\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x ./evaluate_indoBART_to_all.sh\n",
    "!./evaluate_indoBART_to_all.sh result linearized_penman_with_tree_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53cfb7e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T04:30:19.429883Z",
     "iopub.status.busy": "2022-03-15T04:30:19.429059Z",
     "iopub.status.idle": "2022-03-15T04:31:17.151498Z",
     "shell.execute_reply": "2022-03-15T04:31:17.150197Z"
    },
    "papermill": {
     "duration": 96.213656,
     "end_time": "2022-03-15T04:31:17.151690",
     "exception": false,
     "start_time": "2022-03-15T04:29:40.938034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result/ (stored 0%)\r\n",
      "  adding: result/test_label.txt (deflated 60%)\r\n",
      "  adding: result/result_supervised_task_adaptation/ (stored 0%)\r\n",
      "  adding: result/result_supervised_task_adaptation/test_label.txt (deflated 60%)\r\n",
      "  adding: result/result_supervised_task_adaptation/test_generations.txt (deflated 64%)\r\n",
      "  adding: result/result_supervised_task_adaptation/loss_data.tsv (deflated 3%)\r\n",
      "  adding: result/result_supervised_task_adaptation/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/result_supervised_task_adaptation/model/ (stored 0%)\r\n",
      "  adding: result/result_supervised_task_adaptation/model/pytorch_model.bin (deflated 8%)\r\n",
      "  adding: result/result_supervised_task_adaptation/model/config.json (deflated 63%)\r\n",
      "  adding: result/c-gedung-roboh/ (stored 0%)\r\n",
      "  adding: result/c-gedung-roboh/test_label.txt (deflated 62%)\r\n",
      "  adding: result/c-gedung-roboh/test_generations.txt (deflated 59%)\r\n",
      "  adding: result/c-gedung-roboh/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/g-gempa-dieng/ (stored 0%)\r\n",
      "  adding: result/g-gempa-dieng/test_label.txt (deflated 67%)\r\n",
      "  adding: result/g-gempa-dieng/test_generations.txt (deflated 65%)\r\n",
      "  adding: result/g-gempa-dieng/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/f-bunuh-diri/ (stored 0%)\r\n",
      "  adding: result/f-bunuh-diri/test_label.txt (deflated 68%)\r\n",
      "  adding: result/f-bunuh-diri/test_generations.txt (deflated 64%)\r\n",
      "  adding: result/f-bunuh-diri/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/loss_data.tsv (deflated 34%)\r\n",
      "  adding: result/d-indo-fuji/ (stored 0%)\r\n",
      "  adding: result/d-indo-fuji/test_label.txt (deflated 86%)\r\n",
      "  adding: result/d-indo-fuji/test_generations.txt (deflated 84%)\r\n",
      "  adding: result/d-indo-fuji/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/model/ (stored 0%)\r\n",
      "  adding: result/model/pytorch_model.bin (deflated 8%)\r\n",
      "  adding: result/model/config.json (deflated 63%)\r\n",
      "  adding: result/b-salah-darat/ (stored 0%)\r\n",
      "  adding: result/b-salah-darat/test_label.txt (deflated 66%)\r\n",
      "  adding: result/b-salah-darat/test_generations.txt (deflated 64%)\r\n",
      "  adding: result/b-salah-darat/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/amr_simple_test/ (stored 0%)\r\n",
      "  adding: result/amr_simple_test/test_label.txt (deflated 60%)\r\n",
      "  adding: result/amr_simple_test/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/amr_simple_test/bleu_score_test.txt (stored 0%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r result_with_sta.zip result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9981.192367,
   "end_time": "2022-03-15T04:31:55.457563",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-15T01:45:34.265196",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
