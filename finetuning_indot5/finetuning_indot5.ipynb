{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers==4.12.5 sentencepiece==0.1.96 sacrebleu","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:34:21.322794Z","iopub.execute_input":"2022-01-16T09:34:21.323117Z","iopub.status.idle":"2022-01-16T09:34:30.207810Z","shell.execute_reply.started":"2022-01-16T09:34:21.323033Z","shell.execute_reply":"2022-01-16T09:34:30.207019Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers==4.12.5 in /opt/conda/lib/python3.7/site-packages (4.12.5)\nRequirement already satisfied: sentencepiece==0.1.96 in /opt/conda/lib/python3.7/site-packages (0.1.96)\nCollecting sacrebleu\n  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n     |████████████████████████████████| 90 kB 2.1 MB/s            \n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (1.19.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (21.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (3.3.2)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (0.10.3)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (0.1.2)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (0.0.46)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (4.8.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (4.62.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (2.25.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (2021.11.10)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (6.0)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.8.9)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (2.3.2)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.4.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.12.5) (3.10.0.2)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.12.5) (3.0.6)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.12.5) (3.6.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.5) (2021.10.8)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.5) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.5) (1.26.7)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.5) (4.0.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.12.5) (1.1.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.12.5) (1.16.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.12.5) (8.0.3)\nInstalling collected packages: sacrebleu\nSuccessfully installed sacrebleu-2.0.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://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@github.com/taufiqhusada/amr-to-text-indonesia.git","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:34:30.209895Z","iopub.execute_input":"2022-01-16T09:34:30.210183Z","iopub.status.idle":"2022-01-16T09:34:31.535430Z","shell.execute_reply.started":"2022-01-16T09:34:30.210148Z","shell.execute_reply":"2022-01-16T09:34:31.534649Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'amr-to-text-indonesia'...\nremote: Enumerating objects: 35, done.\u001b[K\nremote: Counting objects: 100% (35/35), done.\u001b[K\nremote: Compressing objects: 100% (24/24), done.\u001b[K\nremote: Total 35 (delta 9), reused 30 (delta 7), pack-reused 0\u001b[K\nUnpacking objects: 100% (35/35), 70.95 KiB | 2.09 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"PREPROCESSED_DATA_PATH = './amr-to-text-indonesia/data/preprocessed_data'","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:34:31.537151Z","iopub.execute_input":"2022-01-16T09:34:31.537388Z","iopub.status.idle":"2022-01-16T09:34:31.541332Z","shell.execute_reply.started":"2022-01-16T09:34:31.537361Z","shell.execute_reply":"2022-01-16T09:34:31.540339Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nfrom transformers.optimization import  AdamW, Adafactor \nimport time\nimport warnings\nfrom tqdm import tqdm\nfrom sacrebleu import corpus_bleu\nimport random\nimport numpy as np\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:34:31.543814Z","iopub.execute_input":"2022-01-16T09:34:31.544428Z","iopub.status.idle":"2022-01-16T09:34:38.230158Z","shell.execute_reply.started":"2022-01-16T09:34:31.544392Z","shell.execute_reply":"2022-01-16T09:34:38.229395Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda:0\") \n    print(\"Running on the GPU\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Running on the CPU\")","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:34:38.231591Z","iopub.execute_input":"2022-01-16T09:34:38.231832Z","iopub.status.idle":"2022-01-16T09:34:38.285436Z","shell.execute_reply.started":"2022-01-16T09:34:38.231798Z","shell.execute_reply":"2022-01-16T09:34:38.283823Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Running on the GPU\n","output_type":"stream"}]},{"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:34:38.286874Z","iopub.execute_input":"2022-01-16T09:34:38.287753Z","iopub.status.idle":"2022-01-16T09:34:38.303141Z","shell.execute_reply.started":"2022-01-16T09:34:38.287687Z","shell.execute_reply":"2022-01-16T09:34:38.302326Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"Wikidepia/IndoT5-base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Wikidepia/IndoT5-base\", return_dict=True)\n\n#moving the model to device(GPU/CPU)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:34:38.304716Z","iopub.execute_input":"2022-01-16T09:34:38.305346Z","iopub.status.idle":"2022-01-16T09:35:25.125942Z","shell.execute_reply.started":"2022-01-16T09:34:38.305281Z","shell.execute_reply":"2022-01-16T09:35:25.125230Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/696 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b2c02d3cdaa4e2caf9f12d2e76b608c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/759k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1a10415cf254f0eb3d378ab3450b92c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/945M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12482b54ccb248fda18913d881334c3e"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 768)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (2): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (3): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (4): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (5): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (6): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (7): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (8): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (9): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (10): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (11): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (2): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (3): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (4): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (5): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (6): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (7): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (8): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (9): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (10): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (11): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"AMR_TOKENS = [':ARG0',':ARG1',':mod',':time', ':name', ':location']\nT5_PREFIX = 'translate Graph to Indonesian: '","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:35:25.127039Z","iopub.execute_input":"2022-01-16T09:35:25.127438Z","iopub.status.idle":"2022-01-16T09:35:25.131985Z","shell.execute_reply.started":"2022-01-16T09:35:25.127401Z","shell.execute_reply":"2022-01-16T09:35:25.131271Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"new_tokens_vocab = {}\nnew_tokens_vocab['additional_special_tokens'] = []\nfor idx, t in enumerate(AMR_TOKENS):\n    new_tokens_vocab['additional_special_tokens'].append(t)\n\nnum_added_toks = tokenizer.add_special_tokens(new_tokens_vocab)\nprint(f'added {num_added_toks} tokens')\n\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:35:25.133456Z","iopub.execute_input":"2022-01-16T09:35:25.133923Z","iopub.status.idle":"2022-01-16T09:35:25.942606Z","shell.execute_reply.started":"2022-01-16T09:35:25.133886Z","shell.execute_reply":"2022-01-16T09:35:25.941882Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"added 6 tokens\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Embedding(32106, 768)"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\n# class to load preprocessed amr data\nclass AMRToTextDataset(Dataset):    \n    def __init__(self, file_amr_path, file_sent_path, tokenizer, split):\n        temp_list_amr_input = []\n        with open(file_amr_path) as f:\n            temp_list_amr_input = f.readlines()\n        list_amr_input = []\n        for item in temp_list_amr_input:\n            list_amr_input.append(item.strip())\n            \n        temp_list_sent_output = []\n        with open(file_sent_path) as f:\n            temp_list_sent_output = f.readlines()\n        list_sent_output = []\n        for item in temp_list_sent_output:\n            list_sent_output.append(item.strip())\n        \n        df = pd.DataFrame(list(zip(list_amr_input, list_sent_output)), columns = ['amr','sent'])\n        self.data = df\n        self.tokenizer = tokenizer\n \n    \n    def __getitem__(self, index):\n        data = self.data.loc[index,:]\n        amr, sent = data['amr'], data['sent']\n       \n        tokenize_amr = self.tokenizer.encode(amr, add_special_tokens=False)\n        tokenize_sent = self.tokenizer.encode(sent, add_special_tokens=False)\n        \n        item = {'input':{}, 'output':{}}\n        item['input']['encoded'] = tokenize_amr\n        item['input']['raw'] = amr\n        item['output']['encoded'] = tokenize_sent\n        item['output']['raw'] = sent\n        return item\n    \n    def __len__(self):\n        return len(self.data)\n    \n## Data loader class\nclass AMRToTextDataLoader(DataLoader):\n    def __init__(self, max_seq_len=384, label_pad_token_id=-100, model_type='indo-t5', tokenizer=None, *args, **kwargs):\n        super(AMRToTextDataLoader, self).__init__(*args, **kwargs)\n        self.tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n        self.label_pad_token_id = label_pad_token_id\n        \n        self.pad_token_id = tokenizer.pad_token_id\n        self.bos_token_id = tokenizer.pad_token_id\n        self.eos_token_id = tokenizer.eos_token_id\n        \n        if model_type == 'indo-t5':\n            if self.tokenizer is not None:\n                self.t5_prefix =np.array(self.tokenizer.encode(T5_PREFIX, add_special_tokens=False))\n            self.collate_fn = self._t5_collate_fn\n            \n    def _t5_collate_fn(self, batch):\n        batch_size = len(batch)\n        max_enc_len = min(self.max_seq_len, max(map(lambda x: len(x['input']['encoded']), batch))  + len(self.t5_prefix))\n        max_dec_len = min(self.max_seq_len, max(map(lambda x: len(x['output']['encoded']), batch)) + 1)\n        \n        id_batch = []\n        enc_batch = np.full((batch_size, max_enc_len), self.pad_token_id, dtype=np.int64)\n        dec_batch = np.full((batch_size, max_dec_len), self.pad_token_id, dtype=np.int64)\n        label_batch = np.full((batch_size, max_dec_len), self.label_pad_token_id, dtype=np.int64)\n        enc_mask_batch = np.full((batch_size, max_enc_len), 0, dtype=np.float32)\n        dec_mask_batch = np.full((batch_size, max_dec_len), 0, dtype=np.float32)\n        \n        for i, item in enumerate(batch):\n            input_seq = item['input']['encoded']\n            label_seq = item['output']['encoded']\n            input_seq, label_seq = input_seq[:max_enc_len - len(self.t5_prefix)], label_seq[:max_dec_len - 1]\n            \n            # Assign content\n            enc_batch[i,len(self.t5_prefix):len(self.t5_prefix) + len(input_seq)] = input_seq\n            dec_batch[i,1:1+len(label_seq)] = label_seq\n            label_batch[i,:len(label_seq)] = label_seq\n            enc_mask_batch[i,:len(input_seq) + len(self.t5_prefix)] = 1\n            dec_mask_batch[i,:len(label_seq) + 1] = 1\n            \n            # Assign special token to encoder input\n            enc_batch[i,:len(self.t5_prefix)] = self.t5_prefix\n            \n            # Assign special token to decoder input\n            dec_batch[i,0] = self.bos_token_id\n            \n            # Assign special token to label\n            label_batch[i,len(label_seq)] = self.eos_token_id\n            \n        \n        return enc_batch, dec_batch, enc_mask_batch, None, label_batch","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:35:25.945562Z","iopub.execute_input":"2022-01-16T09:35:25.945767Z","iopub.status.idle":"2022-01-16T09:35:25.971143Z","shell.execute_reply.started":"2022-01-16T09:35:25.945742Z","shell.execute_reply":"2022-01-16T09:35:25.970264Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"DATA_FOLDER = './amr-to-text-indonesia/data/preprocessed_data/'\n\ntrain_amr_path = os.path.join(DATA_FOLDER, 'train.amr.txt')\ntrain_sent_path = os.path.join(DATA_FOLDER, 'train.sent.txt')\n\ndev_amr_path = os.path.join(DATA_FOLDER, 'dev.amr.txt')\ndev_sent_path = os.path.join(DATA_FOLDER, 'dev.sent.txt')\n\ntest_amr_path = os.path.join(DATA_FOLDER, 'test.amr.txt')\ntest_sent_path = os.path.join(DATA_FOLDER, 'test.sent.txt')\n\ntrain_dataset = AMRToTextDataset(train_amr_path, train_sent_path, tokenizer, 'train')\ndev_dataset = AMRToTextDataset(dev_amr_path, dev_sent_path, tokenizer, 'dev')\ntest_dataset = AMRToTextDataset(test_amr_path, test_sent_path, tokenizer, 'test')\n\nmodel_type = 'indo-t5'\nmax_seq_len = 384\nbatch_size = 4\ntrain_loader = AMRToTextDataLoader(dataset=train_dataset, model_type=model_type, tokenizer=tokenizer, max_seq_len=max_seq_len, \n                                    batch_size=batch_size, shuffle=False)  \ntest_loader = AMRToTextDataLoader(dataset=test_dataset, model_type=model_type, tokenizer=tokenizer, max_seq_len=max_seq_len, \n                                    batch_size=batch_size, shuffle=False)  ","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:35:25.972215Z","iopub.execute_input":"2022-01-16T09:35:25.972776Z","iopub.status.idle":"2022-01-16T09:35:25.989929Z","shell.execute_reply.started":"2022-01-16T09:35:25.972738Z","shell.execute_reply":"2022-01-16T09:35:25.989164Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(len(train_dataset))\nprint(len(dev_dataset))\nprint(len(test_dataset))\n\nprint(len(train_loader))","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:35:25.991584Z","iopub.execute_input":"2022-01-16T09:35:25.992155Z","iopub.status.idle":"2022-01-16T09:35:25.998788Z","shell.execute_reply.started":"2022-01-16T09:35:25.992046Z","shell.execute_reply":"2022-01-16T09:35:25.997958Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"700\n19\n306\n175\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = AdamW(\n    model.parameters(),\n    lr=3e-5,\n    eps=1e-8\n)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\nn_epochs = 4\nnum_beams = 5","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:35:26.000346Z","iopub.execute_input":"2022-01-16T09:35:26.001118Z","iopub.status.idle":"2022-01-16T09:35:26.011345Z","shell.execute_reply.started":"2022-01-16T09:35:26.000946Z","shell.execute_reply":"2022-01-16T09:35:26.010558Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# train\nfor epoch in range(n_epochs):\n    model.train()\n    torch.set_grad_enabled(True)\n \n    total_train_loss = 0\n    list_hyp, list_label = [], []\n\n    train_pbar = tqdm(iter(train_loader), leave=True, total=len(train_loader))\n    for i, batch_data in enumerate(train_pbar):\n        enc_batch = torch.LongTensor(batch_data[0])\n        dec_batch = torch.LongTensor(batch_data[1])\n        enc_mask_batch = torch.FloatTensor(batch_data[2])\n        dec_mask_batch = None\n        label_batch = torch.LongTensor(batch_data[4])\n        token_type_batch = None\n        \n        # cuda\n        enc_batch = enc_batch.cuda()\n        dec_batch = dec_batch.cuda()\n        enc_mask_batch = enc_mask_batch.cuda() \n        dec_mask_batch = None\n        label_batch = label_batch.cuda()\n        token_type_batch = None\n\n        outputs = model(input_ids=enc_batch, attention_mask=enc_mask_batch, decoder_input_ids=dec_batch, \n                    decoder_attention_mask=dec_mask_batch, labels=label_batch)\n        loss, logits = outputs[:2]\n        hyps = logits.topk(1, dim=-1)[1]\n        \n        loss.backward()\n        \n        tr_loss = loss.item()\n        total_train_loss = total_train_loss + tr_loss\n        \n        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n                total_train_loss/(i+1), get_lr(optimizer)))\n        \n        optimizer.step()\n        optimizer.zero_grad()\n        ","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:35:26.012770Z","iopub.execute_input":"2022-01-16T09:35:26.013313Z","iopub.status.idle":"2022-01-16T09:36:50.401525Z","shell.execute_reply.started":"2022-01-16T09:35:26.013277Z","shell.execute_reply":"2022-01-16T09:36:50.400680Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"(Epoch 1) TRAIN LOSS:6.6126 LR:0.00003000: 100%|██████████| 175/175 [00:21<00:00,  8.16it/s]\n(Epoch 2) TRAIN LOSS:4.9292 LR:0.00003000: 100%|██████████| 175/175 [00:21<00:00,  8.23it/s]\n(Epoch 3) TRAIN LOSS:4.1882 LR:0.00003000: 100%|██████████| 175/175 [00:20<00:00,  8.34it/s]\n(Epoch 4) TRAIN LOSS:3.5831 LR:0.00003000: 100%|██████████| 175/175 [00:20<00:00,  8.47it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# test on data test\n\nmodel.eval()\ntorch.set_grad_enabled(False)\n\nlist_hyp, list_label = [], []\n\npbar = tqdm(iter(test_loader), leave=True, total=len(test_loader))\nfor i, batch_data in enumerate(pbar):\n    batch_seq = batch_data[-1]\n\n    enc_batch = torch.LongTensor(batch_data[0])\n    dec_batch = torch.LongTensor(batch_data[1])\n    enc_mask_batch = torch.FloatTensor(batch_data[2])\n    dec_mask_batch = None\n    label_batch = torch.LongTensor(batch_data[4])\n    token_type_batch = None\n\n    # cuda\n    enc_batch = enc_batch.cuda()\n    dec_batch = dec_batch.cuda()\n    enc_mask_batch = enc_mask_batch.cuda() \n    dec_mask_batch = None\n    label_batch = label_batch.cuda()\n    token_type_batch = None\n\n    hyps = model.generate(input_ids=enc_batch, attention_mask=enc_mask_batch, num_beams=num_beams, max_length=max_seq_len, \n                          early_stopping=True, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n\n    batch_list_hyp = []\n    batch_list_label = []\n    for j in range(len(hyps)):\n        hyp = hyps[j]\n        label = label_batch[j,:].squeeze()\n     \n        batch_list_hyp.append(tokenizer.decode(hyp, skip_special_tokens=True))\n        batch_list_label.append(tokenizer.decode(label[label != -100], skip_special_tokens=True))\n    \n    list_hyp += batch_list_hyp\n    list_label += batch_list_label","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:36:50.402862Z","iopub.execute_input":"2022-01-16T09:36:50.403208Z","iopub.status.idle":"2022-01-16T09:37:06.395309Z","shell.execute_reply.started":"2022-01-16T09:36:50.403169Z","shell.execute_reply":"2022-01-16T09:37:06.394514Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"100%|██████████| 77/77 [00:15<00:00,  4.82it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in range(len(list_hyp)):\n    print(list_hyp[i], '----', list_label[i])","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:37:06.396620Z","iopub.execute_input":"2022-01-16T09:37:06.397984Z","iopub.status.idle":"2022-01-16T09:37:06.458564Z","shell.execute_reply.started":"2022-01-16T09:37:06.397935Z","shell.execute_reply":"2022-01-16T09:37:06.458005Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"balon tiup ilham ---- Balon itu ditiup Ilham\npuisi ditulis Obe ---- Obe menulis puisi\nmakalah saya ketik saya ---- Saya mengetik makalah\nanak ajaib ajaib ---- Angga Anak Ajaib\norang tua dosen ---- Ibuku seorang dosen\ndia berenang ---- Dia Sedang berenang\nbuku yang hilang ---- Ilham sedang mencari bukunya yang hilang\nhalaman rumah sapu sapu ibu menyapu halaman rumah ---- Ibu Sedang menyapu halaman rumah\npadi membajak sawah ---- Ayah sedang membajak padi di sawah\nkemah di tepi pantai ---- Andi pergi berkemah di tepi Pantai\npergi ke toko buku ---- Angga pergi ke toko buku\nada saya yang masih di rumah ---- Saya masih ada di rumah\nada ayah di pasar ---- Ayah sekarang ada di pasar\nkakak bermain bola masih ---- Kakak masih bermain bola\nada tikus di rumah ---- Ada banyak tikus di rumah\nadik menangis keras ---- Adik menangis keras karena jatuh\nistana seperti hotel mewah sangat mewah ---- Hotel itu sangat mewah seperti istana\nsisa uang sepuluh ribu rupiah rupiah ---- Sisa uang yang aku miliki saat ini hanya berjumlah sepuluh ribu rupiah\njarak sekolah ke rumah terlalu jauh ---- Jarak sekolah dan rumahku tidak terlalu jauh\nkorban perampokan dengan kejam sangat kejam ---- Perampok itu merampok korbannya dengan sangat kejam\npak roni berangkat ke Bandung besok ---- Pak Roni berangkat ke Bandung besok\nBu Cinta berangkat ke sekolah guna taksi ---- Bu Cinta berangkat ke sekolah menggunakan taksi\nuang diberikan Satria kepada adik ---- Satria memberikan uang kepada adiknya\npak hasan bertemu pak husen kemarin ---- Pak Hasan bertemu dengan Pak Husen kemarin di halte bus\nselokan jatuh ke selokan ---- Uang Maman jatuh ke selokan\nmenyapu halaman ---- Yusron sedang menyapu halaman\nprestasi kampus ---- Dona adalah siswa berprestasi di kampusku\nstatus fb di status fb ---- Caca menulis status fb\nti berjalan pelan ---- Ranti berjalan dengan pelan\ngelas pecah Pak Andre ---- Pak Andre memecahkan gelas dengan sengaja\nlapangan dimainkan Pak Amir di lapangan ---- Pak Amir bermain basket di lapangan\nayah mengunjungi paman kemarin ---- Paman mengunjungi ayah kemarin\nibu menjahit baju rapi ---- Ibu menjahit baju dengan rapi\nsurat ditulis Rini di komputer ---- Rini menulis surat dengan komputer\nanda malam hari ---- Dina menulis puisi untuk ibunda di beranda pada malam hari\nadik berlari kencang sangat sangat ---- Adik berlari dengan sangat kencang\nWahyu pergi ke warung ---- Wahyu pergi ke warung\nmembuat roti dibuat Anita ---- Anita sedang membuat roti\nmangsa mangsa mangsa ---- Singa mengintai mangsanya\nlaju mobil sangat cepat ---- Mobil itu melaju dengan sangat cepat\nrumah sangat memarahi kakek sebelah rumah sangat ---- Kakek sebelah rumahku sangat pemarah\nsetel Jas Romeo terlihat setel Jas ---- Romeo tampan dengan setelan jas itu\nmelihat gaun ---- Arsita terlihat begitu anggun dengan gaunnya\nbohong cecep ---- Cecep adalah pembohong\nhati irawan baik hati siapa saja ---- Irawan sangat baik hati kepada siapa saja\nDudung diam sangat sangat ---- Dudung sangat pendiam di sekolahnya\nkucing hitam sangat takut ---- Kucing hitam itu sangat menakutkan\nadalah ibu yang sangat menyayanginya ---- Ibuku adalah orang yang sangat penyayang\nkacamata dibersihkan Mirna dengan tisu lembar basah ---- Mirna membersihkan kacamatanya dengan selembar tisu basah\nkopi suka ayah ---- Jesica menyeduhkan kopi kesukaan ayahnya\nnovel Sherlock Holmes dibeli paman Sherlock baru ---- Paman membelikanku novel Sherlock Holmes edisi terbaru\nsampah bisa didaur ulang ulang ---- Pemulung itu memunguti sampah yang bisa didaur ulang\nbuah apel dibantu Ayah Weni membantu memanen buah apel ---- Weni membantu ayahnya memanen buah apel di kebun miliknya\nibu mencuci piring ---- Ibu sedang mencuci piring\nbola kaki dimainkan Devis sampai sore sore ---- Devis bermain bola kaki sampai sore\nmencari buku selip ---- Sabrina mencari bukunya yang terselip\nke gunung semeru ---- Kami pergi berkemah di Gunung Semeru\na tunggangi Bu Septia ---- Bu Septia menunggang kerbau\njauh teriak Siska ---- Siska berteriak di kejauhan\npenjahit santoso menjahit celana ---- Santoso menjahit celana\norang melamun dia ---- Dia melamunkan seseorang\nkamar membaca buku di kamar ---- Adik membaca buku pelajaran di kamarnya\nbarang ditemu Nur di jalan ---- Nur menemukan barang itu di jalan\ngol tadi malam tadi malam ---- Messi mencetak gol indah di pertandingan tadi malam\nmembeli kue di toko ---- Anggi membeli kue di toko\nmenonton di ruang tamu ---- Riko menonton TV di ruang tamu\nlagu Semangat dinyanyikan Bu Via ---- Bu Via menyanyikan lagu dengan semangat\nlampu depan rumah dinyalakan wawan ---- Wawan menyalakan lampu di depan rumah\ndibuang sampah di tempat sampah ---- Yasmin membuang sampah di tempat sampah\ntadi malam Rina bertemu teman tadi malam ---- Rina bertemu temannya tadi malam\nbekal dimakan Indah dengan bekal lahap ---- Indah memakan bekalnya dengan lahap\nkomputer mati lima menit lalu ---- Paul mematikan komputernya lima menit yang lalu\npadi ditanam Pak Sutejo di sawah ---- Pak Sutejo menanam padi di sawah\nmembaca di atas panggung ---- Karni membacakan orasi di atas panggung\nbaju mahal dibeli Tata di mall ---- Tata membeli baju mahal itu di mall\nhutan diburu hewan di hutan ---- Pemburu itu berburu hewan di hutan\nmakan nasi goreng ---- Riana makan nasi goreng\nbuku dibaca Citra ---- Citra sedang membaca buku\nibu mencuci pakaian ---- Ibu sedang mencuci pakaian\nmembeli bahan makan ---- Joko membeli bahan makanan\ndina melihat pantai di pantai ---- Dik Dina melihat pemandangan pantai\npara nelayan memancing ikan ---- Para nelayan sedang memancing ikan\nmangsa kucing berburu mangsa ---- Kucing itu berburu mangsa\nanak laki menendang bola ---- Anak laki-laki itu yang menendang bola\nminuman diminum Mey ---- Mey meminum minuman itu\ndrama korea ditonton Suster Ayu di drama korea ---- Suster Ayu suka menonton drama Korea\nbapak rian mencari nafkah ---- Bapak Rian sedang mencari nafkah\nIbu Budi makan nasi ---- Ibu Budi sedang menanak nasi\nGerombol ditangkap Polisi ---- Polisi itu menangkap gerombolan pencuri\nberapa hias berapa ---- Auliya menjual beberapa perhiasannya\ngambar komik ---- Nadya suka menggambar komik\ntertawa Ajeng tertawa bahak ---- Ajeng tertawa terbahak-bahak\ndapur dimasak Ibu ---- Ibu memasak di dapur\nDevin menangis sedu ---- Devin menangis tersedu-sedu\nta mengajar sekolah ---- Arlita sedang belajar di sekolah\nhalaman rumah dimainkan Bugie di halaman rumah ---- Bugie bermain di halaman rumah\npara siswa berdiskusi di waring kopi ---- Para siswa berdiskusi di warung kopi\nmembaca pustaka ---- Rizal sedang membaca di perpustakaan\nSinga keras dipukul Singa keras ---- Singa meraung dengan keras\nguru berpidato di lapangan sekolah ---- Pak guru berpidato di lapangan sekolah\nbu rima dandan di salon ---- Bu Rima berdandan di salon\nmembeli baju di toko baru ---- Kak Dina beli baju di toko baru.\nbudi tidur di kamar depan ---- Paman Budi tidur di kamar depan\nibu menyiram bunga ---- Ibu menyiram bunga\nibu menyiram bunga ---- Bunga disiram ibu\nbunga mawar disiram Ibu Shinta kemarin ---- Ibu Shinta menyiram bunga mawar dengan air hujan yang ditampung kemarin\nteras rumah membaca koran waspada di teras rumah ---- Ayah membaca koran Waspada di teras rumah\nhujan tadi malam tadi malam ---- Ketika hujan tadi malam, penjual nasi goreng berjualan\nbuku ensiklopedia ---- Adik membaca buku ensiklopedia\ngunung Rinjani liburan nanti ---- Kami pergi ke Gunung Rinjani saat liburan nanti\naku suka menulis ---- Aku suka dengan tulisannya\nbaju sekolah robek ---- Ibu menjahit baju sekolahku yang robek\ngol dicetak Tim Indonesia di gawang lawan ---- Tim Indonesia mencetak gol ke gawang lawan\nBu Asti mengenakan topi bundar ---- Bu Asti mengenakan topi bundar\nan ringan diberikan tahanan nusakambangan di hari raya idul fitri ---- Tahanan di Nusakambangan diberikan keringanan saat hari raya Idul Fitri\nada sekolah yang membaca cerpen ---- Cerpen yang ada di sekolah menginpirasi pembacanya\nsekolah kami jadi pustaka ---- Perpustakaan menjadi tempat yang paling nyaman di sekolah kami\nmembeli kue capit ---- Ibu Sri Yuni membeli kue capit\nsawah dengan teknologi modern ---- Pak Ucok membajak sawah dengan teknologi pertanian yang modern\nibu mencuci piring ---- Ibu sedang mencuci piring\nbermain bola sampai sore sore ini ---- Pak Andri bermain bola sampai sore\nbuku selip ---- Rani sedang mencari buku yang terselip\nkemah di gunung semeru ---- Kami pergi berkemah di Gunung Semeru\nada tikus di bawah ranjang tidur ---- Ada tikus di bawah ranjang tidurku\ntoko cat ---- Linda pergi ke toko cat\nada aku di rumah ---- Aku masih ada di rumah\nkami pergi ke sana sekarang ---- Kami pergi ke sana sekarang\nkantor ada di kantor ---- Ayah sedang ada di kantor\nsejak tadi pagi Ibu Rusni memasak sejak tadi pagi ---- Ibu Rusni memasak sejak tadi pagi\nlibur di pantai hari ini ---- Andini berlibur ke pantai hari ini\ntrauma kami pergi ke sana kemarin ---- Setelah kejadian kemarin, kami trauma pergi ke sana\nbersih rumah ---- Mita membersihkan rumah.\nNia pergi ke rumah Jihan belum maghrib ---- Pak Dodi mengatakan Bu Nia pergi ke rumah Jihan sebelum magrib\nayah menghentikan tangisnya ---- Tika berhenti menangis setelah Ayah membelikannya mainan baru\nkami menyelesaikan soal ujian hari ini hari ini ---- Ibu Guru mengatakan bahwa kami harus menyelesaikan soal ujian hari ini\nacara komedi di televisi ---- Saya tertawa ketika melihat acara komedi di televisi\npulang ayah lusa lusa ---- Pak Gilang mengatakan bahwa Ayah pulang lusa\nbulan depan kami ikut cara rentak ---- Bulan depan, kami mengikuti UNBK secara serentak\nkue layak dimakan ---- Kue ini layak dimakan\nobat kadaluarsa ---- Obat ini kadaluarsa\nmalam ini kami menginap di hotel besar malam ini ---- Malam ini kami menginap di hotel yang besar\nrumah sejak marah ayah ---- Jojon pulang ke rumah sejak Ayah memarahinya\ndalam loyang yang telah didinginkan ---- Tuangkan adonan ke dalam loyang setelah adonan dingin\nsup hangat ketika pulang ayah memberi sup hangat ketika pulang ---- Ibu memberikan sup hangat ketika Ayah pulang\nkomputer rusak Paktono tahu komputer rusak ---- Pak Tono tahu bahwa komputernya rusak\npasar jam enam pagi ---- saya pergi ke pasar jam enam pagi\nayah pergi ke kantor minggu kemarin ---- Ayah pergi ke kantor minggu kemarin\nberangkat sekolah ---- Andi berangkat ke sekolah\nAni mendapatkan nilai bagus hari pertama di hari pertama ---- ani mendapat nilai bagus pada hari pertama\nberangkat ke sekolah enam pagi ---- Andre berangkat ke sekolah pada enam pagi\nrumah sejak kemarin sore ---- Sejak kemarin sore, dinda pulang ke rumah\nmusim kemarau menjadi musim kemarau bulan oktober ---- Musim kemarau terjadi di bulan Oktober\nmendengar baik sejak kecil ---- Andre dapat mendengar dengan baik sejak kecil\npindah ke bandung telah menikah ---- Nuryati pindah ke Bandung setelah menikah\nmakan siang setelah makan siang ---- Obat itu diminum setelah makan siang\npagi hari di pagi hari ---- Matahari terbit di pagi hari\nmendengar suara ayam kokok ---- Saat fajar tiba terdengar suara ayam jantan berkokok\nminggu kali ini ---- Ayah pergi memancing seminggu sekali\nmasuk kantor Pak Ilham masuk kantor ---- Pak Ilham masuk kantor\nIndonesia mendapatkan banyak raja di zaman dahulu ---- Zaman dahulu terdapat banyak kerajaan di indonesia\ntumbuh pakcoy di musim kemarau ---- Pakcoy bertumbuhan ketika musim kemarau\nkomunikasi yang mudah sekarang ini ---- Sekarang kami dapat berkomunikasi dengan mudah\nrapat nanti akan dibahas rapat nanti ---- Banyak hal yang perlu dibahas saat rapat nanti\nupacara bendera dilaksanakan hari sabtu ---- Upacara penurunan bendera dilaksanakan pada hari Sabtu\nhalaman ditanami cabe tadi pagi ---- Kakek menanam cabe di halaman tadi pagi\nproklamasi dimulai jam sepuluh pagi ---- Upacara proklamasi dimulai jam sepuluh pagi\nproklamir kemerdekaan Indonesia 17 agustus 1945 ---- Kemerdekaan Indonesia diproklamirkan pada tanggal 17 Agustus 1945 oleh Bapak Soekarno\nhari lalu berangkat ke Jakarta tiga hari lalu ---- Dinda berangkat ke jakarta tiga hari yang lalu\ntiap senin ---- Senam kesegaran jasmani dilaksanakan setiap senin\nsetiap bicara politik dia selalu antusias tiap kali bicara politik ---- Dia terlihat antusias setiap kali membicarakan politik\njadi waktu saat dia mengingat jadi waktu saat itu ---- Dia sedih saat ingat kejadian waktu itu\ndi tahun 1965 peristiwa berontak terjadi di tahun 1965 ---- Peristiwa Pemberontakan G-30-S PKI terjadi pada tahun 1965\nair selokan pukul enam pagi ---- Air selokan meluap pukul enam pagi\nberangkat ke Bandung besok lusa ---- Besok lusa saya berangkat ke bandung\nibu pergi mengunjungi nenek di waktu kecil ---- Waktu kecil andre diajak ibu pergi mengunjungi nenek\nrasa udara terasa lebih panas hujan saat saat ---- Saat hujan, udara terasa lebih panas\ndinda bertemu saya ---- Saya berjumpa dengan dinda\nsakit saat kita sakit saat ini ---- Saat sakit kita menyadari pentingnya menjaga kesehatan\nseminar satu jam lagi ---- Acara seminar dimulai satu jam lagi\ntiga jam lalu kami menunggu di stasiun sejak tiga jam lalu ---- Kami menunggu di stasiun sejak tiga jam lalu\nkaki belum tidur ---- Aku mencuci kaki sebelum tidur\nsatu bangsa dimulai sekarang ---- Kita menjaga persatuan bangsa mulai sekarang\nbisa hadir sore ini hari ini ---- Pak Puguh bisa hadir pada sore hari ini\ntoko buku dibuka jam 8 pagi ---- Toko buku buka jam 8 pagi\nkita mampir ke sini lain kali ---- kita mampir ke sini lain kali\nbersih kelas dimulai sekarang ---- Mulai sekarang kalian memperhatikan kebersihan kelas\nbisa dia malam ini hari ini ---- dia bisa malam hari ini\nujian saat saat ---- Dinda mencontek saat ujian\ntahun depan kuliah di Jakarta tahun depan ---- Adik kuliah di jakarta tahun depan\nsepeda Pak Joko tiap pagi tiap pagi ---- Setiap pagi Pak Joko bersepeda\nhari senin ---- Bu Tati pergi berlibur ke jogja hari senin\nboleh kalian datang ke rumah kapan saja ---- Kalian boleh datang ke rumahku kapan saja\nkurang air bersih di musim kemarin ---- Di musim kemarau, kami kekurangan air bersih\nkita pukul tujuh malam ---- Kita pergi pukul tujuh malam\nmusik setiap hari minggu setiap hari minggu ---- Kami latihan musik setiap hari minggu\nsaya bertemu Pak Ilham besok ---- Saya bertemu Pak Ilham besok\nbaik jalan tol waktu dekat ---- Perbaikan jalan tol dilaksanakan dalam waktu dekat ini\nsejak sah laku sejak sah sejak sah ---- Peraturan berlaku sejak disahkan\ndatang ke pesta tadi sore ---- Natasha datang ke pestaku tadi sore\nrumah tadi malam tadi malam ---- Paman kembali ke rumah tadi malam\nkantor pukul tujuh pagi ---- Ayah berangkat ke kantor pukul tujuh pagi\nbulan lalu di bulan lalu ---- Aku bekerja sejak sebulan yang lalu\ndiskusi dimulai jam delapan ---- Acara diskusi dimulai jam delapan\nBu Lisa memanggil aku ---- Aku memanggil Bu Lisa\nBu Lisa memanggil aku ---- Bu Lisa dipanggil Aku\nkoran Tempo di teras ---- Ayah membaca koran Tempo di teras\nkoran Tempo di teras ---- Koran Tempo dibaca Ayah di teras\ntuk saya ---- Saya mengantuk\ntugas diberikan guru ---- Guru memberikan tugas\ntugas diberikan guru ---- Tugas diberikan guru\nmenyanyikan lagu Indonesia Raya ---- Siswa menyanyikan lagu Indonesia Raya\nmenyanyikan lagu Indonesia Raya ---- lagu Indonesia Raya dinyanyikan Siswa\ntugas tas yang dikunjungi ---- Petugas memeriksa tas pengunjung\ntugas tas yang dikunjungi ---- Tas pengunjung diperiksa petugas\nIbu Budi memasak nasi kuning ---- Ibu Budi memasak nasi kuning\nIbu Budi memasak nasi kuning ---- Nasi kuning dimasak Ibu Budi\nadik meminjam buku ---- Adik meminjam buku\nadik meminjam buku ---- Buku dipinjam Adik\ndibakar Pak Hasan ---- Pak Hasan membakar sampah\ndibakar Pak Hasan ---- Sampah dibakar Pak Hasan\nturun suara ---- Budhe Citra menurunkan suaranya\nturun suara ---- Suaranya diturunkan Budhe Citra\ncerita aku ditulis di buku ---- Aku menuliskan cerita di buku\ncerita aku ditulis di buku ---- Cerita dituliskan aku di buku\npencuri ditangkap pencuri ---- Pencuri tertangkap\naku menangkap aku ---- Aku terperangkap\npiala dibawa ayah ---- Ayah membawa piala\npiala dibawa ayah ---- Piala dibawa ayah\nhalaman meniup terompet ---- Kak Adi meniup terompet di halaman\nhalaman meniup terompet ---- Terompet ditiup Kak Adi di halaman\npak Krisna menanam kedelai di kebun ---- Pak Krisna menanam kedelai di kebun\npak Krisna menanam kedelai di kebun ---- Kedelai ditanam Pak Krisna di kebun\nmuka di kamar mandi dibasuh suster Rina ---- Suster Rina membasuh muka di kamar mandi\nmuka di kamar mandi dibasuh suster Rina ---- Muka dibasuh Suster Rina di kamar mandi\nnenek yeti di teras teras ---- Nenek Yeti menunggu kakek Fian di teras\nnenek yeti di teras teras ---- kakek Fian ditunggu Nenek Yeti di teras\nmembersihkan dapur ---- Kakak membersihkan dapur\nmembersihkan dapur ---- Dapur dibersikan kakak\nBu Neng Lipat Pakai ---- Bu Neneng melipat pakaian\nBu Neng Lipat Pakai ---- Pakaian dilipat Bu Neneng\nberita diliput jurnalis ---- Jurnalis meliput berita\nberita diliput jurnalis ---- Berita diliput jurnalis\nadik Zak mengunyah permen ---- Adik Zaki mengunyah permen\nadik Zak mengunyah permen ---- Permen dikunyah Adik Zaki\nIwan menyetir mobil ---- Pak Iwan menyetir mobil\nmembuat pupuk dibuat Mas Prapto ---- Mas Prapto membuat pupuk\npadi ditanam petani ---- Petani menanam padi\npadi ditanam petani ---- Padi ditanam petani\nhasil panen dibeli borong ---- Pemborong membeli hasil panen\nhasil panen dibeli borong ---- Hasil panen dibeli pemborong\nmenengkulak tani ---- Tengkulak memeras petani\nmenengkulak tani ---- Petani diperas tengkulak\nkartun ditonton adik di televisi ---- Adik menonton kartun di televisi\nkartun ditonton adik di televisi ---- Kartun ditonton adik di televisi\nair di tuang Ibu Raska ---- Ibu Raska menuang air\nair di tuang Ibu Raska ---- Air dituang Ibu Raska\nkisah tragis Pak Zee Zee bercerita tentang kisah tragis ---- Pak Zee Zee menceritakan kisah tragisnya\nkisah tragis Pak Zee Zee bercerita tentang kisah tragis ---- Kisah tragisnya diceritakan Pak Zee Zee\nditangkap Saptam Saptam ---- Satpam menangkap pencuri\nditangkap Saptam Saptam ---- Pencuri ditangkap Satpam\nlantai dipel kakak ---- Kakak mengepel lantai\nlantai dipel kakak ---- Lantai dipel kakak\nperdamaian dijaga tentara di Indonesia ---- Tentara menjaga perdamaian di Indonesia\nperdamaian dijaga tentara di Indonesia ---- Perdamaian dijaga tentara di Indonesia\nayah mencari nafkah ---- Ayah mencari nafkah\nayah mencari nafkah ---- Nafkah dicari ayah\nIbu memberi makan ---- Ibu memberi makan\nIbu memberi makan ---- Makan diberi ibu\nadik menerima uang ---- Adik menerima uang\nadik menerima uang ---- Uang diterima adik\nkakak Byan habis uang ---- Kakak Byan menghabiskan uang\nkakak Byan habis uang ---- Uang dihabiskan Kakak Byan\nmassa mobil dibakar massa di jalan ---- Massa membakar mobil di jalanan\nmassa mobil dibakar massa di jalan ---- Mobil dibakar massa di jalanan\nwarga ditangkap warga hutan di hutan ---- Warga menangkap babi di hutan\nwarga ditangkap warga hutan di hutan ---- Babi ditangkap warga di hutan\nmenjual kue dijual Kak Zahra ---- Kak Zahra menjual kue\nmenjual kue dijual Kak Zahra ---- Kue dijual Kak Zahra\nadik melompat ---- Adik melompat\nacara dibela acara adil ---- Pengacara membela tersangka di pengadilan\nacara dibela acara adil ---- Tersangka dibela Pengacara di pengadilan\nhakim roger jatuh di pengadilan hukum ---- Hakim Roger menjatuhi hukuman\nhakim roger jatuh di pengadilan hukum ---- Hukuman dijatuhkan Hakim Roger\njaksa menuntut keadilan ---- Jaksa menuntut tersangka di pengadilan\njaksa menuntut keadilan ---- Tersangka dituntut Jaksa di pengadilan\nmereka menerima kekalahan ---- Mereka menerima kekalahan\nmereka menerima kekalahan ---- kekalahan diterima mereka\nkami menang ---- Kami merayakan kemenangan\nkami menang ---- Kemenangan dirayakan kami\ngubernur anis keluh ---- Gubernur Anis menyampaikan keluhan\ngubernur anis keluh ---- Keluhan disampaikan Gubernur Anis\ngambar lusi ---- Lusi menggambar\nmelukis perempuan ---- Okta melukis perempuan\nmelukis perempuan ---- Perempuan dilukis Okta\ndicetak Pak Budi di Sampurna ---- Pak Budi mencetak kartu nama di Sampurna\ndicetak Pak Budi di Sampurna ---- Kartu nama dicetak Pak Budi di Sampurna\nkpu tetap menang ---- KPU menetapkan pemenang\nkpu tetap menang ---- Pemenang ditetapkan KPU\nhadiah diberikan guru kepada pemenang ---- Guru memberi hadiah kepada pemenang\nhadiah diberikan guru kepada pemenang ---- Hadiah diberi guru kepada pemenang\nadik digendong Ibu Rusdi ---- Ibu Rusdi menggendong adik\nadik digendong Ibu Rusdi ---- Adik digendong Ibu\nadik menjemput ayah di sekolah hari ini ---- Ayah menjemput adik di sekolah hari ini\nadik menjemput ayah di sekolah hari ini ---- Adik dijemput ayah di sekolah hari ini\nminggu depan ---- Kakak memilih pasangan minggu depan\nminggu depan ---- Pasangan dipilih kakak minggu depan\n","output_type":"stream"}]},{"cell_type":"code","source":"## save model\ntorch.save(model.state_dict(), \"4epoch_t5_fixed.th\")","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:37:06.459536Z","iopub.execute_input":"2022-01-16T09:37:06.459775Z","iopub.status.idle":"2022-01-16T09:37:08.200759Z","shell.execute_reply.started":"2022-01-16T09:37:06.459742Z","shell.execute_reply":"2022-01-16T09:37:08.200022Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"## save generated outputs\nwith open('test_generations.txt', 'w') as f:\n    for i in range(len(list_hyp)):\n        e = list_hyp[i]\n        f.write(e)\n        if (i != len(list_hyp)-1):\n            f.write('\\n')\n          \n## save label \nwith open('test_label.txt', 'w') as f:\n    for i in range(len(list_label)):\n        e = list_label[i]\n        f.write(e)\n        if (i != len(list_label)-1):\n            f.write('\\n')","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:37:08.202127Z","iopub.execute_input":"2022-01-16T09:37:08.202389Z","iopub.status.idle":"2022-01-16T09:37:08.210552Z","shell.execute_reply.started":"2022-01-16T09:37:08.202355Z","shell.execute_reply":"2022-01-16T09:37:08.209766Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"## BLEU SCORE\nfrom sacrebleu import corpus_bleu\n\nbleu = corpus_bleu(list_hyp, [list_label])\nprint(bleu.score)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:37:08.211901Z","iopub.execute_input":"2022-01-16T09:37:08.212185Z","iopub.status.idle":"2022-01-16T09:37:08.253281Z","shell.execute_reply.started":"2022-01-16T09:37:08.212150Z","shell.execute_reply":"2022-01-16T09:37:08.252616Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"21.171556587866135\n","output_type":"stream"}]},{"cell_type":"code","source":"def generate(text):\n    model.eval()\n    input_ids = tokenizer.encode(f\"{T5_PREFIX}{text}\", return_tensors=\"pt\", add_special_tokens=False)  # Batch size 1\n    input_ids = input_ids.to(device)\n    outputs = model.generate(input_ids, num_beams=num_beams)\n    \n    gen_text= tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return gen_text","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:37:08.254503Z","iopub.execute_input":"2022-01-16T09:37:08.254745Z","iopub.status.idle":"2022-01-16T09:37:08.260108Z","shell.execute_reply.started":"2022-01-16T09:37:08.254713Z","shell.execute_reply":"2022-01-16T09:37:08.259088Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"generate(\"( ketik :ARG0 ( saya ) :ARG1 ( makalah ) ) )\")","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:37:08.261630Z","iopub.execute_input":"2022-01-16T09:37:08.261916Z","iopub.status.idle":"2022-01-16T09:37:08.441060Z","shell.execute_reply.started":"2022-01-16T09:37:08.261882Z","shell.execute_reply":"2022-01-16T09:37:08.440399Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'makalah saya ketik saya makalah'"},"metadata":{}}]},{"cell_type":"code","source":"temp_list_hyp = []\ntemp_list_label = []\nfor e in list_hyp:\n    temp_list_hyp.append(e)\nfor e in list_label:\n    temp_list_label.append(e)\nbleu = corpus_bleu(temp_list_hyp, [temp_list_label])\nprint(bleu.score)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:37:08.442292Z","iopub.execute_input":"2022-01-16T09:37:08.442544Z","iopub.status.idle":"2022-01-16T09:37:08.480102Z","shell.execute_reply.started":"2022-01-16T09:37:08.442511Z","shell.execute_reply":"2022-01-16T09:37:08.479404Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"21.171556587866135\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}