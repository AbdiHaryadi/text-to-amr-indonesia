{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers==4.12.5 sentencepiece==0.1.96 sacrebleu","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:34:21.322794Z","iopub.execute_input":"2022-01-16T09:34:21.323117Z","iopub.status.idle":"2022-01-16T09:34:30.207810Z","shell.execute_reply.started":"2022-01-16T09:34:21.323033Z","shell.execute_reply":"2022-01-16T09:34:30.207019Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!git clone https://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@github.com/taufiqhusada/amr-to-text-indonesia.git","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:34:30.209895Z","iopub.execute_input":"2022-01-16T09:34:30.210183Z","iopub.status.idle":"2022-01-16T09:34:31.535430Z","shell.execute_reply.started":"2022-01-16T09:34:30.210148Z","shell.execute_reply":"2022-01-16T09:34:31.534649Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"PREPROCESSED_DATA_PATH = './amr-to-text-indonesia/data/preprocessed_data'","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:34:31.537151Z","iopub.execute_input":"2022-01-16T09:34:31.537388Z","iopub.status.idle":"2022-01-16T09:34:31.541332Z","shell.execute_reply.started":"2022-01-16T09:34:31.537361Z","shell.execute_reply":"2022-01-16T09:34:31.540339Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nfrom transformers.optimization import  AdamW, Adafactor \nimport time\nimport warnings\nfrom tqdm import tqdm\nfrom sacrebleu import corpus_bleu\nimport random\nimport numpy as np\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:34:31.543814Z","iopub.execute_input":"2022-01-16T09:34:31.544428Z","iopub.status.idle":"2022-01-16T09:34:38.230158Z","shell.execute_reply.started":"2022-01-16T09:34:31.544392Z","shell.execute_reply":"2022-01-16T09:34:38.229395Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda:0\") \n    print(\"Running on the GPU\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Running on the CPU\")","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:34:38.231591Z","iopub.execute_input":"2022-01-16T09:34:38.231832Z","iopub.status.idle":"2022-01-16T09:34:38.285436Z","shell.execute_reply.started":"2022-01-16T09:34:38.231798Z","shell.execute_reply":"2022-01-16T09:34:38.283823Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:34:38.286874Z","iopub.execute_input":"2022-01-16T09:34:38.287753Z","iopub.status.idle":"2022-01-16T09:34:38.303141Z","shell.execute_reply.started":"2022-01-16T09:34:38.287687Z","shell.execute_reply":"2022-01-16T09:34:38.302326Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"Wikidepia/IndoT5-base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Wikidepia/IndoT5-base\", return_dict=True)\n\n#moving the model to device(GPU/CPU)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:34:38.304716Z","iopub.execute_input":"2022-01-16T09:34:38.305346Z","iopub.status.idle":"2022-01-16T09:35:25.125942Z","shell.execute_reply.started":"2022-01-16T09:34:38.305281Z","shell.execute_reply":"2022-01-16T09:35:25.125230Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"AMR_TOKENS = [':ARG0',':ARG1',':mod',':time', ':name', ':location']\nT5_PREFIX = 'translate Graph to Indonesian: '","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:35:25.127039Z","iopub.execute_input":"2022-01-16T09:35:25.127438Z","iopub.status.idle":"2022-01-16T09:35:25.131985Z","shell.execute_reply.started":"2022-01-16T09:35:25.127401Z","shell.execute_reply":"2022-01-16T09:35:25.131271Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"new_tokens_vocab = {}\nnew_tokens_vocab['additional_special_tokens'] = []\nfor idx, t in enumerate(AMR_TOKENS):\n    new_tokens_vocab['additional_special_tokens'].append(t)\n\nnum_added_toks = tokenizer.add_special_tokens(new_tokens_vocab)\nprint(f'added {num_added_toks} tokens')\n\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:35:25.133456Z","iopub.execute_input":"2022-01-16T09:35:25.133923Z","iopub.status.idle":"2022-01-16T09:35:25.942606Z","shell.execute_reply.started":"2022-01-16T09:35:25.133886Z","shell.execute_reply":"2022-01-16T09:35:25.941882Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\n# class to load preprocessed amr data\nclass AMRToTextDataset(Dataset):    \n    def __init__(self, file_amr_path, file_sent_path, tokenizer, split):\n        temp_list_amr_input = []\n        with open(file_amr_path) as f:\n            temp_list_amr_input = f.readlines()\n        list_amr_input = []\n        for item in temp_list_amr_input:\n            list_amr_input.append(item.strip())\n            \n        temp_list_sent_output = []\n        with open(file_sent_path) as f:\n            temp_list_sent_output = f.readlines()\n        list_sent_output = []\n        for item in temp_list_sent_output:\n            list_sent_output.append(item.strip())\n        \n        df = pd.DataFrame(list(zip(list_amr_input, list_sent_output)), columns = ['amr','sent'])\n        self.data = df\n        self.tokenizer = tokenizer\n \n    \n    def __getitem__(self, index):\n        data = self.data.loc[index,:]\n        amr, sent = data['amr'], data['sent']\n       \n        tokenize_amr = self.tokenizer.encode(amr, add_special_tokens=False)\n        tokenize_sent = self.tokenizer.encode(sent, add_special_tokens=False)\n        \n        item = {'input':{}, 'output':{}}\n        item['input']['encoded'] = tokenize_amr\n        item['input']['raw'] = amr\n        item['output']['encoded'] = tokenize_sent\n        item['output']['raw'] = sent\n        return item\n    \n    def __len__(self):\n        return len(self.data)\n    \n## Data loader class\nclass AMRToTextDataLoader(DataLoader):\n    def __init__(self, max_seq_len=384, label_pad_token_id=-100, model_type='indo-t5', tokenizer=None, *args, **kwargs):\n        super(AMRToTextDataLoader, self).__init__(*args, **kwargs)\n        self.tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n        self.label_pad_token_id = label_pad_token_id\n        \n        self.pad_token_id = tokenizer.pad_token_id\n        self.bos_token_id = tokenizer.pad_token_id\n        self.eos_token_id = tokenizer.eos_token_id\n        \n        if model_type == 'indo-t5':\n            if self.tokenizer is not None:\n                self.t5_prefix =np.array(self.tokenizer.encode(T5_PREFIX, add_special_tokens=False))\n            self.collate_fn = self._t5_collate_fn\n            \n    def _t5_collate_fn(self, batch):\n        batch_size = len(batch)\n        max_enc_len = min(self.max_seq_len, max(map(lambda x: len(x['input']['encoded']), batch))  + len(self.t5_prefix))\n        max_dec_len = min(self.max_seq_len, max(map(lambda x: len(x['output']['encoded']), batch)) + 1)\n        \n        id_batch = []\n        enc_batch = np.full((batch_size, max_enc_len), self.pad_token_id, dtype=np.int64)\n        dec_batch = np.full((batch_size, max_dec_len), self.pad_token_id, dtype=np.int64)\n        label_batch = np.full((batch_size, max_dec_len), self.label_pad_token_id, dtype=np.int64)\n        enc_mask_batch = np.full((batch_size, max_enc_len), 0, dtype=np.float32)\n        dec_mask_batch = np.full((batch_size, max_dec_len), 0, dtype=np.float32)\n        \n        for i, item in enumerate(batch):\n            input_seq = item['input']['encoded']\n            label_seq = item['output']['encoded']\n            input_seq, label_seq = input_seq[:max_enc_len - len(self.t5_prefix)], label_seq[:max_dec_len - 1]\n            \n            # Assign content\n            enc_batch[i,len(self.t5_prefix):len(self.t5_prefix) + len(input_seq)] = input_seq\n            dec_batch[i,1:1+len(label_seq)] = label_seq\n            label_batch[i,:len(label_seq)] = label_seq\n            enc_mask_batch[i,:len(input_seq) + len(self.t5_prefix)] = 1\n            dec_mask_batch[i,:len(label_seq) + 1] = 1\n            \n            # Assign special token to encoder input\n            enc_batch[i,:len(self.t5_prefix)] = self.t5_prefix\n            \n            # Assign special token to decoder input\n            dec_batch[i,0] = self.bos_token_id\n            \n            # Assign special token to label\n            label_batch[i,len(label_seq)] = self.eos_token_id\n            \n        \n        return enc_batch, dec_batch, enc_mask_batch, None, label_batch","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:35:25.945562Z","iopub.execute_input":"2022-01-16T09:35:25.945767Z","iopub.status.idle":"2022-01-16T09:35:25.971143Z","shell.execute_reply.started":"2022-01-16T09:35:25.945742Z","shell.execute_reply":"2022-01-16T09:35:25.970264Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"DATA_FOLDER = './amr-to-text-indonesia/data/preprocessed_data/'\n\ntrain_amr_path = os.path.join(DATA_FOLDER, 'train.amr.txt')\ntrain_sent_path = os.path.join(DATA_FOLDER, 'train.sent.txt')\n\ndev_amr_path = os.path.join(DATA_FOLDER, 'dev.amr.txt')\ndev_sent_path = os.path.join(DATA_FOLDER, 'dev.sent.txt')\n\ntest_amr_path = os.path.join(DATA_FOLDER, 'test.amr.txt')\ntest_sent_path = os.path.join(DATA_FOLDER, 'test.sent.txt')\n\ntrain_dataset = AMRToTextDataset(train_amr_path, train_sent_path, tokenizer, 'train')\ndev_dataset = AMRToTextDataset(dev_amr_path, dev_sent_path, tokenizer, 'dev')\ntest_dataset = AMRToTextDataset(test_amr_path, test_sent_path, tokenizer, 'test')\n\nmodel_type = 'indo-t5'\nmax_seq_len = 384\nbatch_size = 4\ntrain_loader = AMRToTextDataLoader(dataset=train_dataset, model_type=model_type, tokenizer=tokenizer, max_seq_len=max_seq_len, \n                                    batch_size=batch_size, shuffle=False)  \ntest_loader = AMRToTextDataLoader(dataset=test_dataset, model_type=model_type, tokenizer=tokenizer, max_seq_len=max_seq_len, \n                                    batch_size=batch_size, shuffle=False)  ","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:35:25.972215Z","iopub.execute_input":"2022-01-16T09:35:25.972776Z","iopub.status.idle":"2022-01-16T09:35:25.989929Z","shell.execute_reply.started":"2022-01-16T09:35:25.972738Z","shell.execute_reply":"2022-01-16T09:35:25.989164Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(len(train_dataset))\nprint(len(dev_dataset))\nprint(len(test_dataset))\n\nprint(len(train_loader))","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:35:25.991584Z","iopub.execute_input":"2022-01-16T09:35:25.992155Z","iopub.status.idle":"2022-01-16T09:35:25.998788Z","shell.execute_reply.started":"2022-01-16T09:35:25.992046Z","shell.execute_reply":"2022-01-16T09:35:25.997958Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(\n    model.parameters(),\n    lr=3e-5,\n    eps=1e-8\n)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\nn_epochs = 4\nnum_beams = 5","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:35:26.000346Z","iopub.execute_input":"2022-01-16T09:35:26.001118Z","iopub.status.idle":"2022-01-16T09:35:26.011345Z","shell.execute_reply.started":"2022-01-16T09:35:26.000946Z","shell.execute_reply":"2022-01-16T09:35:26.010558Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# train\nfor epoch in range(n_epochs):\n    model.train()\n    torch.set_grad_enabled(True)\n \n    total_train_loss = 0\n    list_hyp, list_label = [], []\n\n    train_pbar = tqdm(iter(train_loader), leave=True, total=len(train_loader))\n    for i, batch_data in enumerate(train_pbar):\n        enc_batch = torch.LongTensor(batch_data[0])\n        dec_batch = torch.LongTensor(batch_data[1])\n        enc_mask_batch = torch.FloatTensor(batch_data[2])\n        dec_mask_batch = None\n        label_batch = torch.LongTensor(batch_data[4])\n        token_type_batch = None\n        \n        # cuda\n        enc_batch = enc_batch.cuda()\n        dec_batch = dec_batch.cuda()\n        enc_mask_batch = enc_mask_batch.cuda() \n        dec_mask_batch = None\n        label_batch = label_batch.cuda()\n        token_type_batch = None\n\n        outputs = model(input_ids=enc_batch, attention_mask=enc_mask_batch, decoder_input_ids=dec_batch, \n                    decoder_attention_mask=dec_mask_batch, labels=label_batch)\n        loss, logits = outputs[:2]\n        hyps = logits.topk(1, dim=-1)[1]\n        \n        loss.backward()\n        \n        tr_loss = loss.item()\n        total_train_loss = total_train_loss + tr_loss\n        \n        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n                total_train_loss/(i+1), get_lr(optimizer)))\n        \n        optimizer.step()\n        optimizer.zero_grad()\n        ","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:35:26.012770Z","iopub.execute_input":"2022-01-16T09:35:26.013313Z","iopub.status.idle":"2022-01-16T09:36:50.401525Z","shell.execute_reply.started":"2022-01-16T09:35:26.013277Z","shell.execute_reply":"2022-01-16T09:36:50.400680Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# test on data test\n\nmodel.eval()\ntorch.set_grad_enabled(False)\n\nlist_hyp, list_label = [], []\n\npbar = tqdm(iter(test_loader), leave=True, total=len(test_loader))\nfor i, batch_data in enumerate(pbar):\n    batch_seq = batch_data[-1]\n\n    enc_batch = torch.LongTensor(batch_data[0])\n    dec_batch = torch.LongTensor(batch_data[1])\n    enc_mask_batch = torch.FloatTensor(batch_data[2])\n    dec_mask_batch = None\n    label_batch = torch.LongTensor(batch_data[4])\n    token_type_batch = None\n\n    # cuda\n    enc_batch = enc_batch.cuda()\n    dec_batch = dec_batch.cuda()\n    enc_mask_batch = enc_mask_batch.cuda() \n    dec_mask_batch = None\n    label_batch = label_batch.cuda()\n    token_type_batch = None\n\n    hyps = model.generate(input_ids=enc_batch, attention_mask=enc_mask_batch, num_beams=num_beams, max_length=max_seq_len, \n                          early_stopping=True, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n\n    batch_list_hyp = []\n    batch_list_label = []\n    for j in range(len(hyps)):\n        hyp = hyps[j]\n        label = label_batch[j,:].squeeze()\n     \n        batch_list_hyp.append(tokenizer.decode(hyp, skip_special_tokens=True))\n        batch_list_label.append(tokenizer.decode(label[label != -100], skip_special_tokens=True))\n    \n    list_hyp += batch_list_hyp\n    list_label += batch_list_label","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:36:50.402862Z","iopub.execute_input":"2022-01-16T09:36:50.403208Z","iopub.status.idle":"2022-01-16T09:37:06.395309Z","shell.execute_reply.started":"2022-01-16T09:36:50.403169Z","shell.execute_reply":"2022-01-16T09:37:06.394514Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"for i in range(len(list_hyp)):\n    print(list_hyp[i], '----', list_label[i])","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:37:06.396620Z","iopub.execute_input":"2022-01-16T09:37:06.397984Z","iopub.status.idle":"2022-01-16T09:37:06.458564Z","shell.execute_reply.started":"2022-01-16T09:37:06.397935Z","shell.execute_reply":"2022-01-16T09:37:06.458005Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"## save model\ntorch.save(model.state_dict(), \"4epoch_t5_fixed.th\")","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:37:06.459536Z","iopub.execute_input":"2022-01-16T09:37:06.459775Z","iopub.status.idle":"2022-01-16T09:37:08.200759Z","shell.execute_reply.started":"2022-01-16T09:37:06.459742Z","shell.execute_reply":"2022-01-16T09:37:08.200022Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"## save generated outputs\nwith open('test_generations.txt', 'w') as f:\n    for i in range(len(list_hyp)):\n        e = list_hyp[i]\n        f.write(e)\n        if (i != len(list_hyp)-1):\n            f.write('\\n')\n          \n## save label \nwith open('test_label.txt', 'w') as f:\n    for i in range(len(list_label)):\n        e = list_label[i]\n        f.write(e)\n        if (i != len(list_label)-1):\n            f.write('\\n')","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:37:08.202127Z","iopub.execute_input":"2022-01-16T09:37:08.202389Z","iopub.status.idle":"2022-01-16T09:37:08.210552Z","shell.execute_reply.started":"2022-01-16T09:37:08.202355Z","shell.execute_reply":"2022-01-16T09:37:08.209766Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"## BLEU SCORE\nfrom sacrebleu import corpus_bleu\n\nbleu = corpus_bleu(list_hyp, [list_label])\nprint(bleu.score)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:37:08.211901Z","iopub.execute_input":"2022-01-16T09:37:08.212185Z","iopub.status.idle":"2022-01-16T09:37:08.253281Z","shell.execute_reply.started":"2022-01-16T09:37:08.212150Z","shell.execute_reply":"2022-01-16T09:37:08.252616Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def generate(text):\n    model.eval()\n    input_ids = tokenizer.encode(f\"{T5_PREFIX}{text}\", return_tensors=\"pt\", add_special_tokens=False)  # Batch size 1\n    input_ids = input_ids.to(device)\n    outputs = model.generate(input_ids, num_beams=num_beams)\n    \n    gen_text= tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return gen_text","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:37:08.254503Z","iopub.execute_input":"2022-01-16T09:37:08.254745Z","iopub.status.idle":"2022-01-16T09:37:08.260108Z","shell.execute_reply.started":"2022-01-16T09:37:08.254713Z","shell.execute_reply":"2022-01-16T09:37:08.259088Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"generate(\"( ketik :ARG0 ( saya ) :ARG1 ( makalah ) ) )\")","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:37:08.261630Z","iopub.execute_input":"2022-01-16T09:37:08.261916Z","iopub.status.idle":"2022-01-16T09:37:08.441060Z","shell.execute_reply.started":"2022-01-16T09:37:08.261882Z","shell.execute_reply":"2022-01-16T09:37:08.440399Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"temp_list_hyp = []\ntemp_list_label = []\nfor e in list_hyp:\n    temp_list_hyp.append(e)\nfor e in list_label:\n    temp_list_label.append(e)\nbleu = corpus_bleu(temp_list_hyp, [temp_list_label])\nprint(bleu.score)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:37:08.442292Z","iopub.execute_input":"2022-01-16T09:37:08.442544Z","iopub.status.idle":"2022-01-16T09:37:08.480102Z","shell.execute_reply.started":"2022-01-16T09:37:08.442511Z","shell.execute_reply":"2022-01-16T09:37:08.479404Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}