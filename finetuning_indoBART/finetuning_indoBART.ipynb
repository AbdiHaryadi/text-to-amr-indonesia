{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers sentencepiece indobenchmark-toolkit==0.0.4 sacrebleu","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:15:24.980545Z","iopub.execute_input":"2022-01-17T02:15:24.980892Z","iopub.status.idle":"2022-01-17T02:15:50.388457Z","shell.execute_reply.started":"2022-01-17T02:15:24.980795Z","shell.execute_reply":"2022-01-17T02:15:50.387506Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.12.5)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.96)\nCollecting indobenchmark-toolkit==0.0.4\n  Downloading indobenchmark_toolkit-0.0.4-py3-none-any.whl (8.0 kB)\nCollecting sacrebleu\n  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n     |████████████████████████████████| 90 kB 826 kB/s            \n\u001b[?25hCollecting sentencepiece\n  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n     |████████████████████████████████| 1.2 MB 4.1 MB/s            \n\u001b[?25hCollecting datasets==1.4.1\n  Downloading datasets-1.4.1-py3-none-any.whl (186 kB)\n     |████████████████████████████████| 186 kB 49.1 MB/s            \n\u001b[?25hRequirement already satisfied: torch>=1.7.1 in /opt/conda/lib/python3.7/site-packages (from indobenchmark-toolkit==0.0.4) (1.9.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.0.2)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (0.70.12.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2021.11.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.3.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (0.3.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.19.5)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (4.8.2)\nRequirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (6.0.0)\nCollecting tqdm<4.50.0,>=4.27\n  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n     |████████████████████████████████| 69 kB 6.2 MB/s             \n\u001b[?25hCollecting huggingface-hub==0.0.2\n  Downloading huggingface_hub-0.0.2-py3-none-any.whl (24 kB)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub==0.0.2->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (3.3.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nCollecting transformers\n  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n     |████████████████████████████████| 3.4 MB 42.2 MB/s            \n\u001b[?25h  Downloading transformers-4.14.1-py3-none-any.whl (3.4 MB)\n     |████████████████████████████████| 3.4 MB 44.9 MB/s            \n\u001b[?25h  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\n     |████████████████████████████████| 3.3 MB 43.0 MB/s            \n\u001b[?25h  Downloading transformers-4.12.4-py3-none-any.whl (3.1 MB)\n     |████████████████████████████████| 3.1 MB 51.9 MB/s            \n\u001b[?25h  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n     |████████████████████████████████| 3.1 MB 42.2 MB/s            \n\u001b[?25h  Downloading transformers-4.12.2-py3-none-any.whl (3.1 MB)\n     |████████████████████████████████| 3.1 MB 50.2 MB/s            \n\u001b[?25h  Downloading transformers-4.12.1-py3-none-any.whl (3.1 MB)\n     |████████████████████████████████| 3.1 MB 44.6 MB/s            \n\u001b[?25h  Downloading transformers-4.12.0-py3-none-any.whl (3.1 MB)\n     |████████████████████████████████| 3.1 MB 40.6 MB/s            \n\u001b[?25h  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n     |████████████████████████████████| 2.9 MB 44.7 MB/s            \n\u001b[?25h  Downloading transformers-4.11.2-py3-none-any.whl (2.9 MB)\n     |████████████████████████████████| 2.9 MB 44.8 MB/s            \n\u001b[?25h  Downloading transformers-4.11.1-py3-none-any.whl (2.9 MB)\n     |████████████████████████████████| 2.9 MB 30.1 MB/s            \n\u001b[?25h  Downloading transformers-4.11.0-py3-none-any.whl (2.9 MB)\n     |████████████████████████████████| 2.9 MB 43.0 MB/s            \n\u001b[?25h  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\n     |████████████████████████████████| 2.8 MB 44.4 MB/s            \n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (21.0)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\n  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\n     |████████████████████████████████| 2.8 MB 42.4 MB/s            \n\u001b[?25h  Downloading transformers-4.10.1-py3-none-any.whl (2.8 MB)\n     |████████████████████████████████| 2.8 MB 39.1 MB/s            \n\u001b[?25h  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n     |████████████████████████████████| 2.8 MB 43.1 MB/s            \n\u001b[?25h  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n     |████████████████████████████████| 2.6 MB 37.2 MB/s            \n\u001b[?25h  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n     |████████████████████████████████| 2.6 MB 49.2 MB/s            \n\u001b[?25h  Downloading transformers-4.9.0-py3-none-any.whl (2.6 MB)\n     |████████████████████████████████| 2.6 MB 47.8 MB/s            \n\u001b[?25h  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\n     |████████████████████████████████| 2.5 MB 38.9 MB/s            \n\u001b[?25h  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n     |████████████████████████████████| 2.5 MB 43.0 MB/s            \n\u001b[?25h  Downloading transformers-4.8.0-py3-none-any.whl (2.5 MB)\n     |████████████████████████████████| 2.5 MB 47.4 MB/s            \n\u001b[?25h  Downloading transformers-4.7.0-py3-none-any.whl (2.5 MB)\n     |████████████████████████████████| 2.5 MB 27.5 MB/s            \n\u001b[?25h  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n     |████████████████████████████████| 2.2 MB 25.4 MB/s            \n\u001b[?25h  Downloading transformers-4.6.0-py3-none-any.whl (2.3 MB)\n     |████████████████████████████████| 2.3 MB 47.4 MB/s            \n\u001b[?25h  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n     |████████████████████████████████| 2.1 MB 42.3 MB/s            \n\u001b[?25hRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.46)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.8.9)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (2.3.2)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.4.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.26.7)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (4.0.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.7.1->indobenchmark-toolkit==0.0.4) (3.10.0.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (3.6.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (3.0.6)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.8.0)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2021.3)\nInstalling collected packages: tqdm, huggingface-hub, transformers, sentencepiece, datasets, sacrebleu, indobenchmark-toolkit\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.62.3\n    Uninstalling tqdm-4.62.3:\n      Successfully uninstalled tqdm-4.62.3\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.1.2\n    Uninstalling huggingface-hub-0.1.2:\n      Successfully uninstalled huggingface-hub-0.1.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.12.5\n    Uninstalling transformers-4.12.5:\n      Successfully uninstalled transformers-4.12.5\n  Attempting uninstall: sentencepiece\n    Found existing installation: sentencepiece 0.1.96\n    Uninstalling sentencepiece-0.1.96:\n      Successfully uninstalled sentencepiece-0.1.96\n  Attempting uninstall: datasets\n    Found existing installation: datasets 1.16.1\n    Uninstalling datasets-1.16.1:\n      Successfully uninstalled datasets-1.16.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbeatrix-jupyterlab 3.1.4 requires google-cloud-bigquery-storage, which is not installed.\ncached-path 0.3.2 requires huggingface-hub<0.2.0,>=0.0.12, but you have huggingface-hub 0.0.2 which is incompatible.\ncached-path 0.3.2 requires tqdm<4.63,>=4.62, but you have tqdm 4.49.0 which is incompatible.\nallennlp 2.8.0 requires huggingface-hub>=0.0.16, but you have huggingface-hub 0.0.2 which is incompatible.\u001b[0m\nSuccessfully installed datasets-1.4.1 huggingface-hub-0.0.2 indobenchmark-toolkit-0.0.4 sacrebleu-2.0.0 sentencepiece-0.1.95 tqdm-4.49.0 transformers-4.5.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://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@github.com/taufiqhusada/amr-to-text-indonesia.git","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:15:50.391381Z","iopub.execute_input":"2022-01-17T02:15:50.391954Z","iopub.status.idle":"2022-01-17T02:15:52.055668Z","shell.execute_reply.started":"2022-01-17T02:15:50.391912Z","shell.execute_reply":"2022-01-17T02:15:52.054808Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'amr-to-text-indonesia'...\nremote: Enumerating objects: 47, done.\u001b[K\nremote: Counting objects: 100% (47/47), done.\u001b[K\nremote: Compressing objects: 100% (31/31), done.\u001b[K\nremote: Total 47 (delta 13), reused 41 (delta 10), pack-reused 0\u001b[K\nUnpacking objects: 100% (47/47), 194.20 KiB | 1.43 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport torch\nfrom transformers import MBartForConditionalGeneration\nfrom transformers.optimization import  AdamW, Adafactor \nimport time\nimport warnings\nfrom tqdm import tqdm\nfrom sacrebleu import corpus_bleu\nimport random\nimport numpy as np\nwarnings.filterwarnings('ignore')\n\nfrom indobenchmark import IndoNLGTokenizer","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:15:52.058174Z","iopub.execute_input":"2022-01-17T02:15:52.058463Z","iopub.status.idle":"2022-01-17T02:15:57.575479Z","shell.execute_reply.started":"2022-01-17T02:15:52.058425Z","shell.execute_reply":"2022-01-17T02:15:57.574750Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda:0\") \n    print(\"Running on the GPU\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Running on the CPU\")","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:15:57.577669Z","iopub.execute_input":"2022-01-17T02:15:57.577955Z","iopub.status.idle":"2022-01-17T02:15:57.623697Z","shell.execute_reply.started":"2022-01-17T02:15:57.577919Z","shell.execute_reply":"2022-01-17T02:15:57.622851Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Running on the GPU\n","output_type":"stream"}]},{"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:15:57.625267Z","iopub.execute_input":"2022-01-17T02:15:57.625801Z","iopub.status.idle":"2022-01-17T02:15:57.635093Z","shell.execute_reply.started":"2022-01-17T02:15:57.625761Z","shell.execute_reply":"2022-01-17T02:15:57.634312Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"tokenizer = IndoNLGTokenizer.from_pretrained('indobenchmark/indobart')\nmodel = MBartForConditionalGeneration.from_pretrained('indobenchmark/indobart')\n\n#moving the model to device(GPU/CPU)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:15:57.636816Z","iopub.execute_input":"2022-01-17T02:15:57.637184Z","iopub.status.idle":"2022-01-17T02:16:37.239544Z","shell.execute_reply.started":"2022-01-17T02:15:57.637142Z","shell.execute_reply":"2022-01-17T02:16:37.238786Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=932102.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd6a05264b3b4c898261604158c3dd99"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=315.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"938f7c319ed84b7e81aa4b377e518d7f"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=339.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0775a93334d54e3db1aeb3b29d97d829"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1712.0, style=ProgressStyle(description…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bad4bfc9d0a7425db3800dffd5db57b0"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=526426289.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c915287e9b74b99b65a220534ec5c17"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"MBartForConditionalGeneration(\n  (model): MBartModel(\n    (shared): Embedding(40004, 768, padding_idx=1)\n    (encoder): MBartEncoder(\n      (embed_tokens): Embedding(40004, 768, padding_idx=1)\n      (embed_positions): MBartLearnedPositionalEmbedding(1026, 768)\n      (layers): ModuleList(\n        (0): MBartEncoderLayer(\n          (self_attn): MBartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): MBartEncoderLayer(\n          (self_attn): MBartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): MBartEncoderLayer(\n          (self_attn): MBartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): MBartEncoderLayer(\n          (self_attn): MBartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): MBartEncoderLayer(\n          (self_attn): MBartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): MBartEncoderLayer(\n          (self_attn): MBartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): MBartDecoder(\n      (embed_tokens): Embedding(40004, 768, padding_idx=1)\n      (embed_positions): MBartLearnedPositionalEmbedding(1026, 768)\n      (layers): ModuleList(\n        (0): MBartDecoderLayer(\n          (self_attn): MBartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MBartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): MBartDecoderLayer(\n          (self_attn): MBartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MBartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): MBartDecoderLayer(\n          (self_attn): MBartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MBartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): MBartDecoderLayer(\n          (self_attn): MBartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MBartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): MBartDecoderLayer(\n          (self_attn): MBartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MBartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): MBartDecoderLayer(\n          (self_attn): MBartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MBartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=40004, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"AMR_TOKENS = [':ARG0',':ARG1',':mod',':time', ':name', ':location']","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:16:37.241039Z","iopub.execute_input":"2022-01-17T02:16:37.241325Z","iopub.status.idle":"2022-01-17T02:16:37.249247Z","shell.execute_reply.started":"2022-01-17T02:16:37.241286Z","shell.execute_reply":"2022-01-17T02:16:37.245410Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"new_tokens_vocab = {}\nnew_tokens_vocab['additional_special_tokens'] = []\nfor idx, t in enumerate(AMR_TOKENS):\n    new_tokens_vocab['additional_special_tokens'].append(t)\n\nnum_added_toks = tokenizer.add_special_tokens(new_tokens_vocab)\nprint(f'added {num_added_toks} tokens')\n\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:16:37.251598Z","iopub.execute_input":"2022-01-17T02:16:37.252375Z","iopub.status.idle":"2022-01-17T02:16:39.492809Z","shell.execute_reply.started":"2022-01-17T02:16:37.252346Z","shell.execute_reply":"2022-01-17T02:16:39.491946Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"added 6 tokens\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Embedding(40013, 768)"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\n# class to load preprocessed amr data\nclass AMRToTextDataset(Dataset):    \n    def __init__(self, file_amr_path, file_sent_path, tokenizer, split):\n        temp_list_amr_input = []\n        with open(file_amr_path) as f:\n            temp_list_amr_input = f.readlines()\n        list_amr_input = []\n        for item in temp_list_amr_input:\n            list_amr_input.append(item.strip().lower())  # lowercase for bart tokenizer\n            \n        temp_list_sent_output = []\n        with open(file_sent_path) as f:\n            temp_list_sent_output = f.readlines()\n        list_sent_output = []\n        for item in temp_list_sent_output:\n            list_sent_output.append(item.strip().lower())\n        \n        df = pd.DataFrame(list(zip(list_amr_input, list_sent_output)), columns = ['amr','sent'])\n        self.data = df\n        self.tokenizer = tokenizer\n \n    \n    def __getitem__(self, index):\n        data = self.data.loc[index,:]\n        amr, sent = data['amr'], data['sent']\n       \n        tokenize_amr = self.tokenizer.encode(amr, add_special_tokens=False)\n        tokenize_sent = self.tokenizer.encode(sent, add_special_tokens=False)\n        \n        item = {'input':{}, 'output':{}}\n        item['input']['encoded'] = tokenize_amr\n        item['input']['raw'] = amr\n        item['output']['encoded'] = tokenize_sent\n        item['output']['raw'] = sent\n        return item\n    \n    def __len__(self):\n        return len(self.data)\n    \n## Data loader class\nclass AMRToTextDataLoader(DataLoader):\n    def __init__(self, max_seq_len=384, label_pad_token_id=-100, model_type='indo-t5', tokenizer=None, *args, **kwargs):\n        super(AMRToTextDataLoader, self).__init__(*args, **kwargs)\n        self.tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n        \n        self.pad_token_id = tokenizer.pad_token_id\n        self.bos_token_id = tokenizer.bos_token_id\n        self.eos_token_id = tokenizer.eos_token_id\n        \n        self.label_pad_token_id = label_pad_token_id\n        \n        if model_type == 'indo-t5':\n            self.bos_token_id = tokenizer.pad_token_id\n            self.t5_prefix =np.array(self.tokenizer.encode(T5_PREFIX, add_special_tokens=False))\n            self.collate_fn = self._t5_collate_fn\n        elif model_type == 'indo-bart':\n            source_lang = \"[indonesian]\"\n            target_lang = \"[indonesian]\"\n\n            self.src_lid_token_id = tokenizer.special_tokens_to_ids[source_lang]\n            self.tgt_lid_token_id = tokenizer.special_tokens_to_ids[target_lang]\n            self.collate_fn = self._bart_collate_fn\n        else:\n            raise ValueError(f'Unknown model_type `{model_type}`')\n            \n    def _t5_collate_fn(self, batch):\n        batch_size = len(batch)\n        max_enc_len = min(self.max_seq_len, max(map(lambda x: len(x['input']['encoded']), batch))  + len(self.t5_prefix))\n        max_dec_len = min(self.max_seq_len, max(map(lambda x: len(x['output']['encoded']), batch)) + 1)\n        \n        id_batch = []\n        enc_batch = np.full((batch_size, max_enc_len), self.pad_token_id, dtype=np.int64)\n        dec_batch = np.full((batch_size, max_dec_len), self.pad_token_id, dtype=np.int64)\n        label_batch = np.full((batch_size, max_dec_len), self.label_pad_token_id, dtype=np.int64)\n        enc_mask_batch = np.full((batch_size, max_enc_len), 0, dtype=np.float32)\n        dec_mask_batch = np.full((batch_size, max_dec_len), 0, dtype=np.float32)\n        \n        for i, item in enumerate(batch):\n            input_seq = item['input']['encoded']\n            label_seq = item['output']['encoded']\n            input_seq, label_seq = input_seq[:max_enc_len - len(self.t5_prefix)], label_seq[:max_dec_len - 1]\n            \n            # Assign content\n            enc_batch[i,len(self.t5_prefix):len(self.t5_prefix) + len(input_seq)] = input_seq\n            dec_batch[i,1:1+len(label_seq)] = label_seq\n            label_batch[i,:len(label_seq)] = label_seq\n            enc_mask_batch[i,:len(input_seq) + len(self.t5_prefix)] = 1\n            dec_mask_batch[i,:len(label_seq) + 1] = 1\n            \n            # Assign special token to encoder input\n            enc_batch[i,:len(self.t5_prefix)] = self.t5_prefix\n            \n            # Assign special token to decoder input\n            dec_batch[i,0] = self.bos_token_id\n            \n            # Assign special token to label\n            label_batch[i,len(label_seq)] = self.eos_token_id\n            \n        \n        return enc_batch, dec_batch, enc_mask_batch, None, label_batch\n    \n    def _bart_collate_fn(self, batch):\n        # encoder input\n        # <sent><eos><langid>\n        # decoder input - \n        # <langid><sent><eos>\n        # decoder output\n        # <sent><eos><langid>\n        \n        batch_size = len(batch)\n        max_enc_len = min(self.max_seq_len, max(map(lambda x: len(x['input']['encoded']), batch)) + 2) # + 2 for eos and langid\n        max_dec_len = min(self.max_seq_len, max(map(lambda x: len(x['output']['encoded']), batch)) + 2) # + 2 for eos and langid\n        \n        id_batch = []\n        enc_batch = np.full((batch_size, max_enc_len), self.pad_token_id, dtype=np.int64)\n        dec_batch = np.full((batch_size, max_dec_len), self.pad_token_id, dtype=np.int64)\n        label_batch = np.full((batch_size, max_dec_len), self.label_pad_token_id, dtype=np.int64)\n        enc_mask_batch = np.full((batch_size, max_enc_len), 0, dtype=np.float32)\n        dec_mask_batch = np.full((batch_size, max_dec_len), 0, dtype=np.float32)\n        \n        for i, item in enumerate(batch):\n            input_seq = item['input']['encoded']\n            label_seq = item['output']['encoded']\n            input_seq, label_seq = input_seq[:max_enc_len-2], label_seq[:max_dec_len - 2]\n            \n            # Assign content\n            enc_batch[i,0:len(input_seq)] = input_seq\n            dec_batch[i,1:1+len(label_seq)] = label_seq\n            label_batch[i,:len(label_seq)] = label_seq\n            enc_mask_batch[i,:len(input_seq) + 2] = 1\n            dec_mask_batch[i,:len(label_seq) + 2] = 1\n            \n            # Assign special token to encoder input\n            enc_batch[i,len(input_seq)] = self.eos_token_id\n            enc_batch[i,1+len(input_seq)] = self.src_lid_token_id\n            \n            # Assign special token to decoder input\n            dec_batch[i,0] = self.tgt_lid_token_id\n            dec_batch[i,1+len(label_seq)] = self.eos_token_id\n            \n            # Assign special token to label\n            label_batch[i,len(label_seq)] = self.eos_token_id\n            label_batch[i,1+len(label_seq)] = self.tgt_lid_token_id\n        \n        return enc_batch, dec_batch, enc_mask_batch, None, label_batch","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:16:39.497274Z","iopub.execute_input":"2022-01-17T02:16:39.497732Z","iopub.status.idle":"2022-01-17T02:16:39.558387Z","shell.execute_reply.started":"2022-01-17T02:16:39.497691Z","shell.execute_reply":"2022-01-17T02:16:39.557485Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:16:39.562587Z","iopub.execute_input":"2022-01-17T02:16:39.562799Z","iopub.status.idle":"2022-01-17T02:16:39.577449Z","shell.execute_reply.started":"2022-01-17T02:16:39.562774Z","shell.execute_reply":"2022-01-17T02:16:39.576379Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': [':ARG0', ':ARG1', ':mod', ':time', ':name', ':location']})"},"metadata":{}}]},{"cell_type":"code","source":"DATA_FOLDER = './amr-to-text-indonesia/data/preprocessed_data/'\n\ntrain_amr_path = os.path.join(DATA_FOLDER, 'train.amr.txt')\ntrain_sent_path = os.path.join(DATA_FOLDER, 'train.sent.txt')\n\ndev_amr_path = os.path.join(DATA_FOLDER, 'dev.amr.txt')\ndev_sent_path = os.path.join(DATA_FOLDER, 'dev.sent.txt')\n\ntest_amr_path = os.path.join(DATA_FOLDER, 'test.amr.txt')\ntest_sent_path = os.path.join(DATA_FOLDER, 'test.sent.txt')\n\ntrain_dataset = AMRToTextDataset(train_amr_path, train_sent_path, tokenizer, 'train')\ndev_dataset = AMRToTextDataset(dev_amr_path, dev_sent_path, tokenizer, 'dev')\ntest_dataset = AMRToTextDataset(test_amr_path, test_sent_path, tokenizer, 'test')\n\nmodel_type = 'indo-bart'\nmax_seq_len = 384\nbatch_size = 4\ntrain_loader = AMRToTextDataLoader(dataset=train_dataset, model_type=model_type, tokenizer=tokenizer, max_seq_len=max_seq_len, \n                                    batch_size=batch_size, shuffle=False)  \ntest_loader = AMRToTextDataLoader(dataset=test_dataset, model_type=model_type, tokenizer=tokenizer, max_seq_len=max_seq_len, \n                                    batch_size=batch_size, shuffle=False)  \ndev_loader = AMRToTextDataLoader(dataset=dev_dataset, model_type=model_type, tokenizer=tokenizer, max_seq_len=max_seq_len, \n                                    batch_size=batch_size, shuffle=False)  ","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:16:39.579310Z","iopub.execute_input":"2022-01-17T02:16:39.579791Z","iopub.status.idle":"2022-01-17T02:16:39.598683Z","shell.execute_reply.started":"2022-01-17T02:16:39.579755Z","shell.execute_reply":"2022-01-17T02:16:39.598001Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(len(train_dataset))\nprint(len(dev_dataset))\nprint(len(test_dataset))\n\nprint(len(train_loader))","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:16:39.600175Z","iopub.execute_input":"2022-01-17T02:16:39.600467Z","iopub.status.idle":"2022-01-17T02:16:39.608388Z","shell.execute_reply.started":"2022-01-17T02:16:39.600430Z","shell.execute_reply":"2022-01-17T02:16:39.607493Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"700\n19\n306\n175\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = AdamW(\n    model.parameters(),\n    lr=3e-5,\n    eps=1e-8\n)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\nn_epochs = 4\nnum_beams = 5","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:16:39.609673Z","iopub.execute_input":"2022-01-17T02:16:39.610566Z","iopub.status.idle":"2022-01-17T02:16:39.621641Z","shell.execute_reply.started":"2022-01-17T02:16:39.610523Z","shell.execute_reply":"2022-01-17T02:16:39.620656Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# train\nfor epoch in range(n_epochs):\n    model.train()\n    torch.set_grad_enabled(True)\n \n    total_train_loss = 0\n    list_hyp, list_label = [], []\n\n    train_pbar = tqdm(iter(train_loader), leave=True, total=len(train_loader))\n    for i, batch_data in enumerate(train_pbar):\n        enc_batch = torch.LongTensor(batch_data[0])\n        dec_batch = torch.LongTensor(batch_data[1])\n        enc_mask_batch = torch.FloatTensor(batch_data[2])\n        dec_mask_batch = None\n        label_batch = torch.LongTensor(batch_data[4])\n        token_type_batch = None\n        \n        # cuda\n        enc_batch = enc_batch.cuda()\n        dec_batch = dec_batch.cuda()\n        enc_mask_batch = enc_mask_batch.cuda() \n        dec_mask_batch = None\n        label_batch = label_batch.cuda()\n        token_type_batch = None\n\n        outputs = model(input_ids=enc_batch, attention_mask=enc_mask_batch, decoder_input_ids=dec_batch, \n                    decoder_attention_mask=dec_mask_batch, labels=label_batch)\n        loss, logits = outputs[:2]\n        hyps = logits.topk(1, dim=-1)[1]\n        \n        loss.backward()\n        \n        tr_loss = loss.item()\n        total_train_loss = total_train_loss + tr_loss\n        \n        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n                total_train_loss/(i+1), get_lr(optimizer)))\n        \n        optimizer.step()\n        optimizer.zero_grad()\n        \n    ## eval on dev\n    model.eval()\n    torch.set_grad_enabled(False)\n    list_hyp, list_label = [], []\n    \n    total_dev_loss = 0\n\n    pbar = tqdm(iter(dev_loader), leave=True, total=len(dev_loader))\n    for i, batch_data in enumerate(pbar):\n        batch_seq = batch_data[-1]\n\n        enc_batch = torch.LongTensor(batch_data[0])\n        dec_batch = torch.LongTensor(batch_data[1])\n        enc_mask_batch = torch.FloatTensor(batch_data[2])\n        dec_mask_batch = None\n        label_batch = torch.LongTensor(batch_data[4])\n        token_type_batch = None\n\n        # cuda\n        enc_batch = enc_batch.cuda()\n        dec_batch = dec_batch.cuda()\n        enc_mask_batch = enc_mask_batch.cuda() \n        dec_mask_batch = None\n        label_batch = label_batch.cuda()\n        token_type_batch = None\n\n        outputs = model(input_ids=enc_batch, attention_mask=enc_mask_batch, decoder_input_ids=dec_batch, \n                    decoder_attention_mask=dec_mask_batch, labels=label_batch)\n        loss, logits = outputs[:2]\n        hyps = logits.topk(1, dim=-1)[1]\n        \n        batch_list_hyp = []\n        batch_list_label = []\n        for j in range(len(hyps)):\n            hyp = hyps[j,:].squeeze()\n            label = label_batch[j,:].squeeze()\n\n            batch_list_hyp.append(tokenizer.decode(hyp, skip_special_tokens=True))\n            batch_list_label.append(tokenizer.decode(label[label != -100], skip_special_tokens=True))\n\n        list_hyp += batch_list_hyp\n        list_label += batch_list_label\n        \n        total_dev_loss += loss.item()\n        pbar.set_description(\"(Epoch {}) DEV LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n                total_dev_loss/(i+1), get_lr(optimizer)))\n        \n    bleu = corpus_bleu(list_hyp, [list_label])\n    print(bleu.score)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:16:39.623366Z","iopub.execute_input":"2022-01-17T02:16:39.623774Z","iopub.status.idle":"2022-01-17T02:17:26.640284Z","shell.execute_reply.started":"2022-01-17T02:16:39.623730Z","shell.execute_reply":"2022-01-17T02:17:26.639568Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"(Epoch 1) TRAIN LOSS:2.5121 LR:0.00003000: 100%|██████████| 175/175 [00:12<00:00, 14.12it/s]\n(Epoch 1) DEV LOSS:0.9203 LR:0.00003000: 100%|██████████| 5/5 [00:00<00:00, 54.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"23.35606325301685\n","output_type":"stream"},{"name":"stderr","text":"(Epoch 2) TRAIN LOSS:0.8254 LR:0.00003000: 100%|██████████| 175/175 [00:11<00:00, 15.56it/s]\n(Epoch 2) DEV LOSS:0.6045 LR:0.00003000: 100%|██████████| 5/5 [00:00<00:00, 57.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"29.725178534424455\n","output_type":"stream"},{"name":"stderr","text":"(Epoch 3) TRAIN LOSS:0.5069 LR:0.00003000: 100%|██████████| 175/175 [00:11<00:00, 14.98it/s]\n(Epoch 3) DEV LOSS:0.6340 LR:0.00003000: 100%|██████████| 5/5 [00:00<00:00, 59.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"52.20200019020257\n","output_type":"stream"},{"name":"stderr","text":"(Epoch 4) TRAIN LOSS:0.3564 LR:0.00003000: 100%|██████████| 175/175 [00:11<00:00, 15.54it/s]\n(Epoch 4) DEV LOSS:0.5828 LR:0.00003000: 100%|██████████| 5/5 [00:00<00:00, 56.60it/s]","output_type":"stream"},{"name":"stdout","text":"53.75699951825974\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# test on data test\n\nmodel.eval()\ntorch.set_grad_enabled(False)\n\nlist_hyp, list_label = [], []\n\npbar = tqdm(iter(test_loader), leave=True, total=len(test_loader))\nfor i, batch_data in enumerate(pbar):\n    batch_seq = batch_data[-1]\n\n    enc_batch = torch.LongTensor(batch_data[0])\n    dec_batch = torch.LongTensor(batch_data[1])\n    enc_mask_batch = torch.FloatTensor(batch_data[2])\n    dec_mask_batch = None\n    label_batch = torch.LongTensor(batch_data[4])\n    token_type_batch = None\n\n    # cuda\n    enc_batch = enc_batch.cuda()\n    dec_batch = dec_batch.cuda()\n    enc_mask_batch = enc_mask_batch.cuda() \n    dec_mask_batch = None\n    label_batch = label_batch.cuda()\n    token_type_batch = None\n\n    hyps = model.generate(input_ids=enc_batch, attention_mask=enc_mask_batch, num_beams=num_beams, max_length=max_seq_len, \n                          early_stopping=True, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n\n\n    batch_list_hyp = []\n    batch_list_label = []\n    for j in range(len(hyps)):\n        hyp = hyps[j]\n        label = label_batch[j,:].squeeze()\n     \n        batch_list_hyp.append(tokenizer.decode(hyp, skip_special_tokens=True))\n        batch_list_label.append(tokenizer.decode(label[label != -100], skip_special_tokens=True))\n    \n    list_hyp += batch_list_hyp\n    list_label += batch_list_label","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:17:26.641514Z","iopub.execute_input":"2022-01-17T02:17:26.642190Z","iopub.status.idle":"2022-01-17T02:17:34.157084Z","shell.execute_reply.started":"2022-01-17T02:17:26.642151Z","shell.execute_reply":"2022-01-17T02:17:34.156408Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"100%|██████████| 77/77 [00:07<00:00, 10.27it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"list_label = []\nfor i in range(len(list_hyp)):\n    print(list_hyp[i], '----', test_dataset.data['sent'][i])\n    list_label.append(test_dataset.data['sent'][i])","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:17:34.161001Z","iopub.execute_input":"2022-01-17T02:17:34.163097Z","iopub.status.idle":"2022-01-17T02:17:34.272047Z","shell.execute_reply.started":"2022-01-17T02:17:34.163055Z","shell.execute_reply":"2022-01-17T02:17:34.271455Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"ilham meniup balon ---- balon itu ditiup ilham\nob e menulis puisi ---- obe menulis puisi\nsaya mengetik makalah ---- saya mengetik makalah\nangga anak ajaib ---- angga anak ajaib\nibu seorang dosen ---- ibuku seorang dosen\ndia berenang ---- dia sedang berenang\nilham mencari buku hilang ---- ilham sedang mencari bukunya yang hilang\nibu menyapu halaman rumah ---- ibu sedang menyapu halaman rumah\nayah bajak padi di sawah ---- ayah sedang membajak padi di sawah\nandi pergi ke tepi pantai ---- andi pergi berkemah di tepi pantai\nangga pergi ke toko buku ---- angga pergi ke toko buku\nsaya ada masih di rumah ---- saya masih ada di rumah\nayah ada di pasar ---- ayah sekarang ada di pasar\nkakak bermain bola ---- kakak masih bermain bola\ntikus banyak ada di rumah ---- ada banyak tikus di rumah\nadik menangis keras ---- adik menangis keras karena jatuh\nhotel mewah sangat terdapat di istana ---- hotel itu sangat mewah seperti istana\naku menyimpan sisa uang dengan jumlah sepuluh ribu rupiah ---- sisa uang yang aku miliki saat ini hanya berjumlah sepuluh ribu rupiah\nrumah itu jaraknya tidak terlalu jauh ---- jarak sekolah dan rumahku tidak terlalu jauh\nkorban dibunuh r ampok dengan kejam ---- perampok itu merampok korbannya dengan sangat kejam\npak roni berangkat ke bandung besok ---- pak roni berangkat ke bandung besok\nbu cinta berangkat ke sekolah ---- bu cinta berangkat ke sekolah menggunakan taksi\nuang diberikan kepada adik ---- satria memberikan uang kepada adiknya\npak hasan bertemu pak hus en kemarin di halte bus ---- pak hasan bertemu dengan pak husen kemarin di halte bus\nuang maman jatuh di selokan ---- uang maman jatuh ke selokan\nyus ron menyapu halaman ---- yusron sedang menyapu halaman\nd ona siswa dengan prestasi di kampus ---- dona adalah siswa berprestasi di kampusku\ncac a menulis status fb ---- caca menulis status fb\nr anti berjalan pelan ---- ranti berjalan dengan pelan\npak andre memecahkan gelas sengaja. ---- pak andre memecahkan gelas dengan sengaja\npak amir bermain basket di lapangan ---- pak amir bermain basket di lapangan\npaman berkunjung kemarin ---- paman mengunjungi ayah kemarin\nibu menjahit baju dengan rapi ---- ibu menjahit baju dengan rapi\nrini menulis surat di komputer ---- rini menulis surat dengan komputer\ndiana menulis puisi di beranda malam hari ---- dina menulis puisi untuk ibunda di beranda pada malam hari\nadik berlari kencang sangat ---- adik berlari dengan sangat kencang\nwahyu pergi ke warung ---- wahyu pergi ke warung\nanita membuat roti ---- anita sedang membuat roti\nsinga itu sedang mencari mangsa ---- singa mengintai mangsanya\nmobil itu bergerak sangat cepat ---- mobil itu melaju dengan sangat cepat\nkakek marah di sebelah rumah ---- kakek sebelah rumahku sangat pemarah\nromeo tampan setel jas ---- romeo tampan dengan setelan jas itu\nars ita melihat anggun ---- arsita terlihat begitu anggun dengan gaunnya\ncec ep pembohong ---- cecep adalah pembohong\nirawan baik di hati ---- irawan sangat baik hati kepada siapa saja\nd udung diam di sekolah ---- dudung sangat pendiam di sekolahnya\nkucing hitam itu takut sangat ---- kucing hitam itu sangat menakutkan\nibu adalah sayang sangat ---- ibuku adalah orang yang sangat penyayang\nmirna membersihkan kacamata dengan tisu basah ---- mirna membersihkan kacamatanya dengan selembar tisu basah\njessica sed uh kopi ayah ---- jesica menyeduhkan kopi kesukaan ayahnya\npaman membeli novel sher lock holmes baru ---- paman membelikanku novel sherlock holmes edisi terbaru\npul ung pung ut sampah bisa daur ulang ---- pemulung itu memunguti sampah yang bisa didaur ulang\nayah membantu panen buah apel di kebun milik ---- weni membantu ayahnya memanen buah apel di kebun miliknya\nibu mencuci piring ---- ibu sedang mencuci piring\ndev is bermain bola kaki sampai sore ---- devis bermain bola kaki sampai sore\nbuku sel ip dicari sab rina ---- sabrina mencari bukunya yang terselip\nkami pergi ke gunung semeru ---- kami pergi berkemah di gunung semeru\nbu sep tia menungg angi kerbau ---- bu septia menunggang kerbau\nsiska berteriak dari jauh ---- siska berteriak di kejauhan\nsantoso menjahit celana ---- santoso menjahit celana\ndia membenci orang ---- dia melamunkan seseorang\nadik membaca buku ajar di kamar ---- adik membaca buku pelajaran di kamarnya\nnur bertemu dengan barang di jalan ---- nur menemukan barang itu di jalan\ngol dicetak messi tadi malam ---- messi mencetak gol indah di pertandingan tadi malam\nanggi membeli kue di toko ---- anggi membeli kue di toko\nriko menonton tv di ruang tamu ---- riko menonton tv di ruang tamu\nbu via menyanyikan lagu semangat ---- bu via menyanyikan lagu dengan semangat\nwawan menyalakan lampu di depan rumah ---- wawan menyalakan lampu di depan rumah\nyasmin membuang sampah di tempat sampah ---- yasmin membuang sampah di tempat sampah\nrina bertemu temannya tadi malam ---- rina bertemu temannya tadi malam\nindah makan bekal dengan lahap ---- indah memakan bekalnya dengan lahap\npaul itu mati lima menit lalu ---- paul mematikan komputernya lima menit yang lalu\npak sut ejo menanam padi di sawah ---- pak sutejo menanam padi di sawah\nkar ni membaca orasi di atas panggung ---- karni membacakan orasi di atas panggung\nbaju mahal dibeli tata di mall ---- tata membeli baju mahal itu di mall\nhewan itu sedang diburu di hutan ---- pemburu itu berburu hewan di hutan\nr iana makan nasi goreng ---- riana makan nasi goreng\ncitra membaca buku ---- citra sedang membaca buku\nibu mencuci pakaian ---- ibu sedang mencuci pakaian\njoko membeli bahan makanan ---- joko membeli bahan makanan\ndik dina melihat pemandangan pantai ---- dik dina melihat pemandangan pantai\npara nelayan memancing ikan ---- para nelayan sedang memancing ikan\nkucing itu sedang mencari mangsa ---- kucing itu berburu mangsa\nanak laki menendang bola ---- anak laki-laki itu yang menendang bola\nmey minum minuman ---- mey meminum minuman itu\nsuster ayu menonton drama korea ---- suster ayu suka menonton drama korea\nbapak rian mencari nafkah ---- bapak rian sedang mencari nafkah\nibu budi nan ak nasi ---- ibu budi sedang menanak nasi\npolisi menangkap pencuri ger ombol ---- polisi itu menangkap gerombolan pencuri\na uli ya menjual hias berapa ---- auliya menjual beberapa perhiasannya\ngambar komik disukai nad ya ---- nadya suka menggambar komik\najeng tertawa di bah ak ---- ajeng tertawa terbahak-bahak\nibu sedang memasak di dapur ---- ibu memasak di dapur\ndev in menangis ---- devin menangis tersedu-sedu\nar l ita belajar di sekolah ---- arlita sedang belajar di sekolah\nbu gie bermain di halaman rumah ---- bugie bermain di halaman rumah\npara siswa sedang berdiskusi di war ing kopi ---- para siswa berdiskusi di warung kopi\nrizal sedang membaca di perpustakaan ---- rizal sedang membaca di perpustakaan\nsinga itu berteriak keras ---- singa meraung dengan keras\npak guru berpidato di lapangan ---- pak guru berpidato di lapangan sekolah\nbu rima sedang berdandan di salon ---- bu rima berdandan di salon\nkak dina membeli baju di toko baru ---- kak dina beli baju di toko baru.\npaman budi tidur di kamar depan ---- paman budi tidur di kamar depan\nibu menyiram bunga ---- ibu menyiram bunga\nibu menyiram bunga ---- bunga disiram ibu\nibu shinta menyiram bunga mawar kemarin ---- ibu shinta menyiram bunga mawar dengan air hujan yang ditampung kemarin\nayah membaca koran waspada di teras rumah ---- ayah membaca koran waspada di teras rumah\nnasi goreng dijual di pasar tadi malam ---- ketika hujan tadi malam, penjual nasi goreng berjualan\nadik membaca buku ensiklopedia ---- adik membaca buku ensiklopedia\nkami pergi ke gunung rinjani liburan nanti ---- kami pergi ke gunung rinjani saat liburan nanti\naku menyukai tulisan ---- aku suka dengan tulisannya\nibu menjahit baju sekolah robek ---- ibu menjahit baju sekolahku yang robek\ngol dicetak tim indonesia di gawang lawan ---- tim indonesia mencetak gol ke gawang lawan\nbu as ti kena topi bundar ---- bu asti mengenakan topi bundar\ntahanan n usak ambangan diberi hukuman ringan di hari raya idul fitri ---- tahanan di nusakambangan diberikan keringanan saat hari raya idul fitri\ncerpen itu inspirasi ada di sekolah ---- cerpen yang ada di sekolah menginpirasi pembacanya\npustaka jadi tempat nyaman paling dicari sekolah kami ---- perpustakaan menjadi tempat yang paling nyaman di sekolah kami\nibu sri yuni membeli kue capit ---- ibu sri yuni membeli kue capit\npak uc ok membajak sawah menggunakan teknologi modern ---- pak ucok membajak sawah dengan teknologi pertanian yang modern\nibu mencuci piring ---- ibu sedang mencuci piring\npak andri bermain bola sampai sore ---- pak andri bermain bola sampai sore\nrani mencari buku sel ip ---- rani sedang mencari buku yang terselip\nkami pergi ke gunung semeru ---- kami pergi berkemah di gunung semeru\ntikus ada di bawah ranjang tidur ---- ada tikus di bawah ranjang tidurku\nlinda pergi ke toko cat ---- linda pergi ke toko cat\naku ada di rumah ---- aku masih ada di rumah\nkami pergi ke sana sekarang ---- kami pergi ke sana sekarang\nayah ada di kantor ---- ayah sedang ada di kantor\nibu rus ni memasak sejak tadi pagi ---- ibu rusni memasak sejak tadi pagi\nand ini sedang berlibur di pantai hari ini ---- andini berlibur ke pantai hari ini\nkami trauma di sana setelah kejadian kemarin ---- setelah kejadian kemarin, kami trauma pergi ke sana\nmita membersihkan rumah ---- mita membersihkan rumah.\npak dodi berkata pergi ke rumah. ---- pak dodi mengatakan bu nia pergi ke rumah jihan sebelum magrib\ntika henti tangis ketika telah dibeli ayah ---- tika berhenti menangis setelah ayah membelikannya mainan baru\nibu guru mengetik soal ujian hari ini ---- ibu guru mengatakan bahwa kami harus menyelesaikan soal ujian hari ini\nsaya tertawa lihat acara komedi di televisi ---- saya tertawa ketika melihat acara komedi di televisi\npak gilang berkata pulang lusa ---- pak gilang mengatakan bahwa ayah pulang lusa\nkami mengikuti unbk dengan cara rent ak bulan depan ---- bulan depan, kami mengikuti unbk secara serentak\nkue itu layak makan ---- kue ini layak dimakan\nobat itu kadaluarsa ---- obat ini kadaluarsa\nkami inap di hotel besar malam ini ---- malam ini kami menginap di hotel yang besar\njo jon pulang ke rumah sejak marah ---- jojon pulang ke rumah sejak ayah memarahinya\nadon itu ditu ang ke dalam loyang. ---- tuangkan adonan ke dalam loyang setelah adonan dingin\nibu memberikan sup hangat ketika pulang ---- ibu memberikan sup hangat ketika ayah pulang\npak t ono mengetahui komputer rusak ---- pak tono tahu bahwa komputernya rusak\nsaya pergi ke pasar jam enam pagi ---- saya pergi ke pasar jam enam pagi\nayah pergi ke kantor minggu kemarin ---- ayah pergi ke kantor minggu kemarin\nandi berangkat ke sekolah ---- andi berangkat ke sekolah\nani mendapat nilai bagus di hari pertama ---- ani mendapat nilai bagus pada hari pertama\nandre berangkat ke sekolah pukul enam pagi ---- andre berangkat ke sekolah pada enam pagi\ndinda pulang ke rumah sejak kemarin ---- sejak kemarin sore, dinda pulang ke rumah\nmusim kemarau terjadi pada bulan oktober ---- musim kemarau terjadi di bulan oktober\nandre mendapat dengar yang baik sejak kecil ---- andre dapat mendengar dengan baik sejak kecil\nnur yati pindah ke bandung. ---- nuryati pindah ke bandung setelah menikah\nobat diminum setelah makan siang ---- obat itu diminum setelah makan siang\nmatahari terbit di pagi hari ---- matahari terbit di pagi hari\nsuara ayam jantan kok ok terdengar saat fajar tiba ---- saat fajar tiba terdengar suara ayam jantan berkokok\nayah pergi ke pang cing minggu ini ---- ayah pergi memancing seminggu sekali\npak ilham masuk ke kantor ---- pak ilham masuk kantor\nraja merupakan raja di indonesia zaman dahulu ---- zaman dahulu terdapat banyak kerajaan di indonesia\npak co y tumbuh di musim kemarau ---- pakcoy bertumbuhan ketika musim kemarau\nkami mendapat komunikasi mudah sekarang ---- sekarang kami dapat berkomunikasi dengan mudah\nhal banyak perlu dibahas di rapat nanti ---- banyak hal yang perlu dibahas saat rapat nanti\nupacara turun bendera dilaksanakan pada hari sabtu ---- upacara penurunan bendera dilaksanakan pada hari sabtu\nkakek menanam cabe di halaman tadi pagi ---- kakek menanam cabe di halaman tadi pagi\nupacara proklamasi dimulai jam sepuluh pagi ---- upacara proklamasi dimulai jam sepuluh pagi\nkemerdekaan indonesia dipr oklam irkan pada tanggal 17 agustus 1945 ---- kemerdekaan indonesia diproklamirkan pada tanggal 17 agustus 1945 oleh bapak soekarno\ndinda berangkat ke jakarta tiga hari lalu ---- dinda berangkat ke jakarta tiga hari yang lalu\nsenam kesegaran jasmani dilaksanakan tiap senin ---- senam kesegaran jasmani dilaksanakan setiap senin\ndia melihat antusias tiap bicara ---- dia terlihat antusias setiap kali membicarakan politik\ndia sedih. ---- dia sedih saat ingat kejadian waktu itu\nperistiwa pemberontakan pki terjadi pada tahun 1965 ---- peristiwa pemberontakan g-30-s pki terjadi pada tahun 1965\nair sel ok itu lu apkan pukul enam pagi ---- air selokan meluap pukul enam pagi\nsaya berangkat ke bandung besok ---- besok lusa saya berangkat ke bandung\nibu mengajak ibu pergi berlibur ---- waktu kecil andre diajak ibu pergi mengunjungi nenek\nudara terasa panas lebih saat hujan ---- saat hujan, udara terasa lebih panas\nsaya bertemu dinda ---- saya berjumpa dengan dinda\nkita sadar pentingnya kesehatan saat ini ---- saat sakit kita menyadari pentingnya menjaga kesehatan\nacara seminar dimulai satu jam lagi ---- acara seminar dimulai satu jam lagi\nkami menunggu di stasiun sejak tiga jam lalu ---- kami menunggu di stasiun sejak tiga jam lalu\naku mencuci kaki belum tidur ---- aku mencuci kaki sebelum tidur\nkita menjaga satu bangsa mulai sekarang ---- kita menjaga persatuan bangsa mulai sekarang\npak puguh bisa hadir di sore ini ---- pak puguh bisa hadir pada sore hari ini\ntoko buku buka jam 8 pagi ---- toko buku buka jam 8 pagi\nkita mampir ke sini lain kali ---- kita mampir ke sini lain kali\nkalian perh ati bersih kelas mulai sekarang ---- mulai sekarang kalian memperhatikan kebersihan kelas\ndia bisa tidur di malam hari ini ---- dia bisa malam hari ini\ndinda sedang ujian saat ini ---- dinda mencontek saat ujian\nadik kuliah di jakarta tahun depan ---- adik kuliah di jakarta tahun depan\npak joko mengendarai sepeda tiap pagi ---- setiap pagi pak joko bersepeda\nbu t ati pergi ke jogja hari senin ---- bu tati pergi berlibur ke jogja hari senin\nkalian boleh datang ke rumah kapan saja ---- kalian boleh datang ke rumahku kapan saja\nkami kurang mendapatkan air bersih musim kemarin ---- di musim kemarau, kami kekurangan air bersih\nkita pergi pukul tujuh malam ---- kita pergi pukul tujuh malam\nkami latih musik setiap minggu ---- kami latihan musik setiap hari minggu\nsaya bertemu pak ilham besok ---- saya bertemu pak ilham besok\njalan tol dilaksanakan dalam waktu dekat ---- perbaikan jalan tol dilaksanakan dalam waktu dekat ini\natur berlaku sejak disahkan ---- peraturan berlaku sejak disahkan\nnatasha datang tadi sore ---- natasha datang ke pestaku tadi sore\npaman kembali ke rumah tadi malam ---- paman kembali ke rumah tadi malam\nayah berangkat ke kantor pukul tujuh pagi ---- ayah berangkat ke kantor pukul tujuh pagi\naku bekerja bulan lalu ---- aku bekerja sejak sebulan yang lalu\nacara diskusi dimulai jam delapan ---- acara diskusi dimulai jam delapan\naku memanggil bu lisa ---- aku memanggil bu lisa\naku memanggil bu lisa ---- bu lisa dipanggil aku\nayah membaca koran tempo di teras ---- ayah membaca koran tempo di teras\nayah membaca koran tempo di teras ---- koran tempo dibaca ayah di teras\nsaya mengantuk ---- saya mengantuk\nguru memberi tugas ---- guru memberikan tugas\nguru memberi tugas ---- tugas diberikan guru\nsiswa menyanyikan lagu indonesia raya ---- siswa menyanyikan lagu indonesia raya\nsiswa menyanyikan lagu indonesia raya ---- lagu indonesia raya dinyanyikan siswa\ntugas memeriksa tas ---- petugas memeriksa tas pengunjung\ntugas memeriksa tas ---- tas pengunjung diperiksa petugas\nibu budi memasak nasi kuning ---- ibu budi memasak nasi kuning\nibu budi memasak nasi kuning ---- nasi kuning dimasak ibu budi\nadik meminjam buku ---- adik meminjam buku\nadik meminjam buku ---- buku dipinjam adik\npak hasan membakar sampah ---- pak hasan membakar sampah\npak hasan membakar sampah ---- sampah dibakar pak hasan\nbud he cc itra turun ---- budhe citra menurunkan suaranya\nbud he cc itra turun ---- suaranya diturunkan budhe citra\naku menulis cerita di buku ---- aku menuliskan cerita di buku\naku menulis cerita di buku ---- cerita dituliskan aku di buku\npencuri ditangkap ---- pencuri tertangkap\naku menangkap. ---- aku terperangkap\nayah membawa piala ---- ayah membawa piala\nayah membawa piala ---- piala dibawa ayah\nkak adi meniup terompet di halaman ---- kak adi meniup terompet di halaman\nkak adi meniup terompet di halaman ---- terompet ditiup kak adi di halaman\npak krisna menanam kedelai di kebun ---- pak krisna menanam kedelai di kebun\npak krisna menanam kedelai di kebun ---- kedelai ditanam pak krisna di kebun\nsuster rina membasuh muka di kamar mandi ---- suster rina membasuh muka di kamar mandi\nsuster rina membasuh muka di kamar mandi ---- muka dibasuh suster rina di kamar mandi\nnenek y eti sedang ditunggu kakek f ian di teras ---- nenek yeti menunggu kakek fian di teras\nnenek y eti sedang ditunggu kakek f ian di teras ---- kakek fian ditunggu nenek yeti di teras\nkakak membersihkan dapur ---- kakak membersihkan dapur\nkakak membersihkan dapur ---- dapur dibersikan kakak\nbu nen eng melipat pakaian ---- bu neneng melipat pakaian\nbu nen eng melipat pakaian ---- pakaian dilipat bu neneng\njurnalis meliput berita ---- jurnalis meliput berita\njurnalis meliput berita ---- berita diliput jurnalis\nadik zaki mengunyah permen ---- adik zaki mengunyah permen\nadik zaki mengunyah permen ---- permen dikunyah adik zaki\npak iwan setir mobil ---- pak iwan menyetir mobil\nmas prap to membuat pupuk ---- mas prapto membuat pupuk\npadi ditanam pak tani ---- petani menanam padi\npadi ditanam pak tani ---- padi ditanam petani\nhasil panen dibeli pembeli ---- pemborong membeli hasil panen\nhasil panen dibeli pembeli ---- hasil panen dibeli pemborong\ntengk ulak memeras petani ---- tengkulak memeras petani\ntengk ulak memeras petani ---- petani diperas tengkulak\nadik menonton kartun di televisi ---- adik menonton kartun di televisi\nadik menonton kartun di televisi ---- kartun ditonton adik di televisi\nibu ras ka menuangkan air ---- ibu raska menuang air\nibu ras ka menuangkan air ---- air dituang ibu raska\npak z ee z ee menceritakan kisah tragis ---- pak zee zee menceritakan kisah tragisnya\npak z ee z ee menceritakan kisah tragis ---- kisah tragisnya diceritakan pak zee zee\nsap tam menangkap pencuri ---- satpam menangkap pencuri\nsap tam menangkap pencuri ---- pencuri ditangkap satpam\nkakak meny eka lantai ---- kakak mengepel lantai\nkakak meny eka lantai ---- lantai dipel kakak\ntentara menjaga perdamaian di indonesia ---- tentara menjaga perdamaian di indonesia\ntentara menjaga perdamaian di indonesia ---- perdamaian dijaga tentara di indonesia\nayah mencari nafkah ---- ayah mencari nafkah\nayah mencari nafkah ---- nafkah dicari ayah\nibu memberi makan ---- ibu memberi makan\nibu memberi makan ---- makan diberi ibu\nadik menerima uang ---- adik menerima uang\nadik menerima uang ---- uang diterima adik\nkakak b yan menghabiskan uang ---- kakak byan menghabiskan uang\nkakak b yan menghabiskan uang ---- uang dihabiskan kakak byan\nmassa membakar mobil di jalanan ---- massa membakar mobil di jalanan\nmassa membakar mobil di jalanan ---- mobil dibakar massa di jalanan\nwarga menangkap babi di hutan ---- warga menangkap babi di hutan\nwarga menangkap babi di hutan ---- babi ditangkap warga di hutan\nkak zahra menjual kue ---- kak zahra menjual kue\nkak zahra menjual kue ---- kue dijual kak zahra\nadik melompat ---- adik melompat\nacara membela tersangka di pengadilan ---- pengacara membela tersangka di pengadilan\nacara membela tersangka di pengadilan ---- tersangka dibela pengacara di pengadilan\nhakim roger jatuh ---- hakim roger menjatuhi hukuman\nhakim roger jatuh ---- hukuman dijatuhkan hakim roger\ntersangka dituntut jaksa di pengadilan ---- jaksa menuntut tersangka di pengadilan\ntersangka dituntut jaksa di pengadilan ---- tersangka dituntut jaksa di pengadilan\nmereka menerima kekalahan ---- mereka menerima kekalahan\nmereka menerima kekalahan ---- kekalahan diterima mereka\nkami merayakan kemenangan ---- kami merayakan kemenangan\nkami merayakan kemenangan ---- kemenangan dirayakan kami\ngubernur anis menyampaikan keluhan ---- gubernur anis menyampaikan keluhan\ngubernur anis menyampaikan keluhan ---- keluhan disampaikan gubernur anis\nl usi gambar ---- lusi menggambar\nokta melukis perempuan ---- okta melukis perempuan\nokta melukis perempuan ---- perempuan dilukis okta\npak budi mencetak kartu nama di sampurna ---- pak budi mencetak kartu nama di sampurna\npak budi mencetak kartu nama di sampurna ---- kartu nama dicetak pak budi di sampurna\nkpu tetap mengumumkan pemenang ---- kpu menetapkan pemenang\nkpu tetap mengumumkan pemenang ---- pemenang ditetapkan kpu\nguru memberikan hadiah kepada pemenang ---- guru memberi hadiah kepada pemenang\nguru memberikan hadiah kepada pemenang ---- hadiah diberi guru kepada pemenang\nibu rusdi menggendong adik ---- ibu rusdi menggendong adik\nibu rusdi menggendong adik ---- adik digendong ibu\nayah jemput adik di sekolah hari ini ---- ayah menjemput adik di sekolah hari ini\nayah jemput adik di sekolah hari ini ---- adik dijemput ayah di sekolah hari ini\nkakak memilih pasangan minggu depan ---- kakak memilih pasangan minggu depan\nkakak memilih pasangan minggu depan ---- pasangan dipilih kakak minggu depan\n","output_type":"stream"}]},{"cell_type":"code","source":"## save model\ntorch.save(model.state_dict(), \"4epoch_indobart.th\")","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:17:34.273043Z","iopub.execute_input":"2022-01-17T02:17:34.273303Z","iopub.status.idle":"2022-01-17T02:17:35.305098Z","shell.execute_reply.started":"2022-01-17T02:17:34.273257Z","shell.execute_reply":"2022-01-17T02:17:35.304304Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"## save generated outputs\nwith open('test_generations.txt', 'w') as f:\n    for i in range(len(list_hyp)):\n        e = list_hyp[i]\n        f.write(e)\n        if (i != len(list_hyp)-1):\n            f.write('\\n')\n          \n## save label \nwith open('test_label.txt', 'w') as f:\n    for i in range(len(list_label)):\n        e = list_label[i]\n        f.write(e)\n        if (i != len(list_label)-1):\n            f.write('\\n')","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:17:35.306589Z","iopub.execute_input":"2022-01-17T02:17:35.306834Z","iopub.status.idle":"2022-01-17T02:17:35.315430Z","shell.execute_reply.started":"2022-01-17T02:17:35.306800Z","shell.execute_reply":"2022-01-17T02:17:35.314432Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"## BLEU SCORE\nfrom sacrebleu import corpus_bleu\n\nbleu = corpus_bleu(list_hyp, [list_label])\nprint(bleu.score)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:17:35.318792Z","iopub.execute_input":"2022-01-17T02:17:35.319094Z","iopub.status.idle":"2022-01-17T02:17:35.355291Z","shell.execute_reply.started":"2022-01-17T02:17:35.319066Z","shell.execute_reply":"2022-01-17T02:17:35.354550Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"41.61477752442154\n","output_type":"stream"}]},{"cell_type":"code","source":"def generate(text):\n    model.eval()\n    bart_input = tokenizer.prepare_input_for_generation(text,\n                                         lang_token = '[indonesian]', decoder_lang_token='[indonesian]')\n    input_ids = torch.tensor([bart_input['input_ids']]).cuda()\n    print(input_ids)\n    outputs = model.generate(input_ids, num_beams=num_beams)\n    \n    gen_text= tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return gen_text","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:17:35.357101Z","iopub.execute_input":"2022-01-17T02:17:35.357538Z","iopub.status.idle":"2022-01-17T02:17:35.370998Z","shell.execute_reply.started":"2022-01-17T02:17:35.357498Z","shell.execute_reply":"2022-01-17T02:17:35.369855Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"generate(\"( ketik :ARG0 ( saya ) :ARG1 ( makalah ) ) )\")","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:17:35.372689Z","iopub.execute_input":"2022-01-17T02:17:35.373097Z","iopub.status.idle":"2022-01-17T02:17:35.436766Z","shell.execute_reply.started":"2022-01-17T02:17:35.373057Z","shell.execute_reply":"2022-01-17T02:17:35.436053Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"tensor([[    0,   382,  9338, 40007,   382,   475,   384, 40008,   382,  6353,\n           384,   384,   384, 40002]], device='cuda:0')\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'saya mengetik makalah'"},"metadata":{}}]},{"cell_type":"code","source":"# inputs = [' balon itu ditiup ilham <mask>']\n# bart_input = tokenizer.prepare_input_for_generation(inputs, return_tensors='pt',\n#                                          lang_token = '[indonesian]', decoder_lang_token='[indonesian]')\n# bart_input.to(device)\n# bart_out = model(**bart_input)\n# print(tokenizer.decode(bart_input['input_ids'][0]))\n# print(tokenizer.decode(bart_out.logits.topk(1).indices[:,:].squeeze()))","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:17:35.438246Z","iopub.execute_input":"2022-01-17T02:17:35.438729Z","iopub.status.idle":"2022-01-17T02:17:35.443072Z","shell.execute_reply.started":"2022-01-17T02:17:35.438690Z","shell.execute_reply":"2022-01-17T02:17:35.442247Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}