Program FinetuningModel
    input   : model name,
              parameter model,
              path to folder data train

    output  : saved trained model,
              saved tokenizer

    parser <- add_args(argparse.ArgumentParser())
    args <- parser.parse_args()

    // init params
    set_seed(42)
    model_type <- "indo-t5"  // or "indo-bart"
    batch_size <- args.batch_size
    lr <- args.lr
    eps <- args.eps
    n_epochs <- args.n_epochs
    num_beams <- args.num_beams
    max_seq_len_amr <- args.max_seq_len_amr
    max_seq_len_sent <- args.max_seq_len_sent
    result_folder <- args.result_folder
    DATA_FOLDER <- args.data_folder
    if (args.resume_from_checkpoint) then
        saved_model_folder_path <- args.saved_model_folder_path


    if torch.cuda.is_available() then
        device <- torch.device("cuda:0") 
        print("Running on the GPU")
    else
        device <- torch.device("cpu")
        print("Running on the CPU")

    if (args.resume_from_checkpoint) then
        print('resume from checkpoint')
        if (model_type == 'indo-t5') then
            tokenizer <- T5TokenizerFast.from_pretrained(os.path.join(saved_model_folder_path, 'tokenizer'))
            model <- AutoModelForSeq2SeqLM.from_pretrained(os.path.join(saved_model_folder_path, 'model'))
        else if (model_type == 'indo-bart') then
            tokenizer <- IndoNLGTokenizer.from_pretrained('indobenchmark/indobart')
            model <- MBartForConditionalGeneration.from_pretrained(os.path.join(saved_model_folder_path, 'model'))
    else
        if (model_type == 'indo-t5') then
            tokenizer <- T5TokenizerFast.from_pretrained("Wikidepia/IndoT5-base")
            model <- AutoModelForSeq2SeqLM.from_pretrained("Wikidepia/IndoT5-base", return_dict=True)
        else if (model_type == 'indo-bart') then
            tokenizer <- IndoNLGTokenizer.from_pretrained('indobenchmark/indobart')
            model <- MBartForConditionalGeneration.from_pretrained('indobenchmark/indobart')            

    // moving the model to device(GPU/CPU)
    model.to(device)

    // add new vocab (amr special tokens)
    new_tokens_vocab <- {}
    new_tokens_vocab['additional_special_tokens'] <- tokenizer.additional_special_tokens
    for idx, t in enumerate(AMR_TOKENS):
        new_tokens_vocab['additional_special_tokens'].append(t)

    num_added_toks <- tokenizer.add_special_tokens(new_tokens_vocab)
    print(f'added {num_added_toks} tokens')

    model.resize_token_embeddings(len(tokenizer))

    // load data

    train_amr_path <- os.path.join(DATA_FOLDER, 'train.amr.txt')
    train_sent_path <- os.path.join(DATA_FOLDER, 'train.sent.txt')

    dev_amr_path <- os.path.join(DATA_FOLDER, 'dev.amr.txt')
    dev_sent_path <- os.path.join(DATA_FOLDER, 'dev.sent.txt')

    test_amr_path <- os.path.join(DATA_FOLDER, 'test.amr.txt')
    test_sent_path <- os.path.join(DATA_FOLDER, 'test.sent.txt')

    train_dataset <- AMRToTextDataset(train_amr_path, train_sent_path, tokenizer, 'train')
    dev_dataset <- AMRToTextDataset(dev_amr_path, dev_sent_path, tokenizer, 'dev')
    test_dataset <- AMRToTextDataset(test_amr_path, test_sent_path, tokenizer, 'test')

    train_loader <- AMRToTextDataLoader(dataset=train_dataset, model_type=model_type, tokenizer=tokenizer,  max_seq_len_amr=max_seq_len_amr, max_seq_len_sent=max_seq_len_sent, 
                                        batch_size=batch_size, shuffle=True)  
    test_loader <- AMRToTextDataLoader(dataset=test_dataset, model_type=model_type, tokenizer=tokenizer,  max_seq_len_amr=max_seq_len_amr, max_seq_len_sent=max_seq_len_sent, 
                                        batch_size=batch_size, shuffle=False)  
    dev_loader <- AMRToTextDataLoader(dataset=dev_dataset, model_type=model_type, tokenizer=tokenizer,  max_seq_len_amr=max_seq_len_amr, max_seq_len_sent=max_seq_len_sent, 
                                        batch_size=batch_size, shuffle=False)  

    print('len train dataset: ', str(len(train_dataset)))
    print('len dev dataset: ', str(len(dev_dataset)))
    print('len test dataset:', str(len(test_dataset)))

    print('len train dataloader: ', str(len(train_loader)))
    
    // define optimizer
    optimizer <- AdamW(
        model.parameters(),
        lr=lr,
        eps=eps
    )


    // train
    list_loss_train <- []
    list_loss_dev <- []
    for epoch = 0 to n_epochs do
        model.train()
        torch.set_grad_enabled(True)
    
        total_train_loss <- 0
        list_hyp, list_label <- [], []

        train_pbar <- tqdm(iter(train_loader), leave=True, total=len(train_loader))
        for each i, batch_data in enumerate(train_pbar) do
            enc_batch <- torch.LongTensor(batch_data[0]).cuda()
            dec_batch <- torch.LongTensor(batch_data[1]).cuda()
            enc_mask_batch <- torch.FloatTensor(batch_data[2]).cuda()
            dec_mask_batch <- None
            label_batch <- torch.LongTensor(batch_data[4]).cuda()
            token_type_batch <- None

            outputs <- model(input_ids=enc_batch, attention_mask=enc_mask_batch, decoder_input_ids=dec_batch, 
                        decoder_attention_mask=dec_mask_batch, labels=label_batch)
            loss, logits <- outputs[:2]
            hyps <- logits.topk(1, dim=-1)[1]
            
            loss.backward()
            
            tr_loss <- loss.item()
            total_train_loss <- total_train_loss + tr_loss
            
            train_pbar.set_description("(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}".format((epoch+1),
                    total_train_loss/(i+1), get_lr(optimizer)))
            
            optimizer.step()
            optimizer.zero_grad()

        list_loss_train.append(total_train_loss/len(train_loader))

        // eval per epoch
        model.eval()
        torch.set_grad_enabled(False)
        list_hyp, list_label <- [], []
        
        total_dev_loss <- 0

        pbar <- tqdm(iter(dev_loader), leave=True, total=len(dev_loader))
        for i, batch_data in enumerate(pbar) do
            batch_seq <- batch_data[-1]

            enc_batch <- torch.LongTensor(batch_data[0]).cuda()
            dec_batch <- torch.LongTensor(batch_data[1]).cuda()
            enc_mask_batch <- torch.FloatTensor(batch_data[2]).cuda()
            dec_mask_batch <- None
            label_batch <- torch.LongTensor(batch_data[4]).cuda()
            token_type_batch <- None

            outputs <- model(input_ids=enc_batch, attention_mask=enc_mask_batch, decoder_input_ids=dec_batch, 
                        decoder_attention_mask=dec_mask_batch, labels=label_batch)
            loss, logits <- outputs[:2]
            hyps <- logits.topk(1, dim=-1)[1]
            
            batch_list_hyp <- []
            batch_list_label <- []
            for j in range(len(hyps)):
                hyp <- hyps[j,:].squeeze()
                label <- label_batch[j,:].squeeze()

                batch_list_hyp.append(tokenizer.decode(hyp, skip_special_tokens=True))
                batch_list_label.append(tokenizer.decode(label[label != -100], skip_special_tokens=True))

            list_hyp <- list_hyp + batch_list_hyp
            list_label <- list_label + batch_list_label
            
            total_dev_loss <- total_dev_loss + loss.item()
            pbar.set_description("(Epoch {}) DEV LOSS:{:.4f} LR:{:.8f}".format((epoch+1),
                    total_dev_loss/(i+1), get_lr(optimizer)))
            
        bleu <- calc_corpus_bleu_score(list_hyp, list_label)
        print('bleu score on dev: ', str(bleu))

        list_loss_dev.append(total_dev_loss/len(dev_loader))

    // save model
    tokenizer.save_pretrained(os.path.join(result_folder, "tokenizer"))
    model.save_pretrained(os.path.join(result_folder, "model"))