{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e2e6db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T00:53:24.073019Z",
     "iopub.status.busy": "2022-02-12T00:53:24.071525Z",
     "iopub.status.idle": "2022-02-12T00:53:52.740158Z",
     "shell.execute_reply": "2022-02-12T00:53:52.740770Z",
     "shell.execute_reply.started": "2022-01-23T08:51:09.071646Z"
    },
    "papermill": {
     "duration": 28.685652,
     "end_time": "2022-02-12T00:53:52.741056",
     "exception": false,
     "start_time": "2022-02-12T00:53:24.055404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.12.5)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.96)\r\n",
      "Collecting indobenchmark-toolkit==0.0.4\r\n",
      "  Downloading indobenchmark_toolkit-0.0.4-py3-none-any.whl (8.0 kB)\r\n",
      "Collecting sacrebleu\r\n",
      "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\r\n",
      "     |████████████████████████████████| 90 kB 691 kB/s            \r\n",
      "\u001b[?25hCollecting datasets==1.4.1\r\n",
      "  Downloading datasets-1.4.1-py3-none-any.whl (186 kB)\r\n",
      "     |████████████████████████████████| 186 kB 2.5 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.7.1 in /opt/conda/lib/python3.7/site-packages (from indobenchmark-toolkit==0.0.4) (1.9.1)\r\n",
      "Collecting sentencepiece\r\n",
      "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\r\n",
      "     |████████████████████████████████| 1.2 MB 6.2 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.0.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2021.11.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.19.5)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (0.3.4)\r\n",
      "Collecting tqdm<4.50.0,>=4.27\r\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\r\n",
      "     |████████████████████████████████| 69 kB 7.1 MB/s             \r\n",
      "\u001b[?25hRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (0.70.12.2)\r\n",
      "Collecting huggingface-hub==0.0.2\r\n",
      "  Downloading huggingface_hub-0.0.2-py3-none-any.whl (24 kB)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.3.4)\r\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (6.0.1)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.26.0)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (4.10.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub==0.0.2->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (3.3.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\r\n",
      "Collecting transformers\r\n",
      "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\r\n",
      "     |████████████████████████████████| 3.5 MB 58.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.16.1-py3-none-any.whl (3.5 MB)\r\n",
      "     |████████████████████████████████| 3.5 MB 58.6 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.16.0-py3-none-any.whl (3.5 MB)\r\n",
      "     |████████████████████████████████| 3.5 MB 49.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\r\n",
      "     |████████████████████████████████| 3.4 MB 53.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.14.1-py3-none-any.whl (3.4 MB)\r\n",
      "     |████████████████████████████████| 3.4 MB 43.3 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\r\n",
      "     |████████████████████████████████| 3.3 MB 57.2 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.4-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 54.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 58.0 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.2-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 58.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.1-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 44.0 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.0-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 62.0 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 44.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.2-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 50.0 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.1-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 50.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.0-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 58.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 48.4 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 56.0 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.1-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 40.4 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 50.2 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\r\n",
      "     |████████████████████████████████| 2.6 MB 30.8 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.46)\r\n",
      "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\r\n",
      "     |████████████████████████████████| 2.6 MB 49.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.9.0-py3-none-any.whl (2.6 MB)\r\n",
      "     |████████████████████████████████| 2.6 MB 47.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 44.9 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 47.2 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.8.0-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 57.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.7.0-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 51.4 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\r\n",
      "     |████████████████████████████████| 2.2 MB 53.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.6.0-py3-none-any.whl (2.3 MB)\r\n",
      "     |████████████████████████████████| 2.3 MB 46.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\r\n",
      "     |████████████████████████████████| 2.1 MB 56.1 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.4.4)\r\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (2.3.2)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.8.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2021.10.8)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.26.7)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.0.8)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (3.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.7.1->indobenchmark-toolkit==0.0.4) (3.10.0.2)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (3.6.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.8.0)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2021.3)\r\n",
      "Installing collected packages: tqdm, huggingface-hub, transformers, sentencepiece, datasets, sacrebleu, indobenchmark-toolkit\r\n",
      "  Attempting uninstall: tqdm\r\n",
      "    Found existing installation: tqdm 4.62.3\r\n",
      "    Uninstalling tqdm-4.62.3:\r\n",
      "      Successfully uninstalled tqdm-4.62.3\r\n",
      "  Attempting uninstall: huggingface-hub\r\n",
      "    Found existing installation: huggingface-hub 0.1.2\r\n",
      "    Uninstalling huggingface-hub-0.1.2:\r\n",
      "      Successfully uninstalled huggingface-hub-0.1.2\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.12.5\r\n",
      "    Uninstalling transformers-4.12.5:\r\n",
      "      Successfully uninstalled transformers-4.12.5\r\n",
      "  Attempting uninstall: sentencepiece\r\n",
      "    Found existing installation: sentencepiece 0.1.96\r\n",
      "    Uninstalling sentencepiece-0.1.96:\r\n",
      "      Successfully uninstalled sentencepiece-0.1.96\r\n",
      "  Attempting uninstall: datasets\r\n",
      "    Found existing installation: datasets 1.17.0\r\n",
      "    Uninstalling datasets-1.17.0:\r\n",
      "      Successfully uninstalled datasets-1.17.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "beatrix-jupyterlab 3.1.4 requires google-cloud-bigquery-storage, which is not installed.\r\n",
      "tsfresh 0.19.0 requires statsmodels>=0.13, but you have statsmodels 0.12.2 which is incompatible.\r\n",
      "cached-path 0.3.2 requires huggingface-hub<0.2.0,>=0.0.12, but you have huggingface-hub 0.0.2 which is incompatible.\r\n",
      "cached-path 0.3.2 requires tqdm<4.63,>=4.62, but you have tqdm 4.49.0 which is incompatible.\r\n",
      "allennlp 2.8.0 requires huggingface-hub>=0.0.16, but you have huggingface-hub 0.0.2 which is incompatible.\u001b[0m\r\n",
      "Successfully installed datasets-1.4.1 huggingface-hub-0.0.2 indobenchmark-toolkit-0.0.4 sacrebleu-2.0.0 sentencepiece-0.1.95 tqdm-4.49.0 transformers-4.5.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece indobenchmark-toolkit==0.0.4 sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a462e0f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T00:53:52.866208Z",
     "iopub.status.busy": "2022-02-12T00:53:52.865453Z",
     "iopub.status.idle": "2022-02-12T00:53:55.368297Z",
     "shell.execute_reply": "2022-02-12T00:53:55.367794Z",
     "shell.execute_reply.started": "2022-01-23T08:51:34.574695Z"
    },
    "papermill": {
     "duration": 2.567311,
     "end_time": "2022-02-12T00:53:55.368438",
     "exception": false,
     "start_time": "2022-02-12T00:53:52.801127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'amr-to-text-indonesia'...\r\n",
      "remote: Enumerating objects: 1422, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (1422/1422), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (930/930), done.\u001b[K\r\n",
      "remote: Total 1422 (delta 562), reused 1169 (delta 314), pack-reused 0\u001b[K\r\n",
      "Receiving objects: 100% (1422/1422), 1.71 MiB | 3.93 MiB/s, done.\r\n",
      "Resolving deltas: 100% (562/562), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://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@github.com/taufiqhusada/amr-to-text-indonesia.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f15e864e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T00:53:55.501031Z",
     "iopub.status.busy": "2022-02-12T00:53:55.500086Z",
     "iopub.status.idle": "2022-02-12T00:53:55.503518Z",
     "shell.execute_reply": "2022-02-12T00:53:55.503934Z",
     "shell.execute_reply.started": "2022-01-23T08:51:36.468409Z"
    },
    "papermill": {
     "duration": 0.071847,
     "end_time": "2022-02-12T00:53:55.504069",
     "exception": false,
     "start_time": "2022-02-12T00:53:55.432222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/amr-to-text-indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e47fabd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T00:53:55.635302Z",
     "iopub.status.busy": "2022-02-12T00:53:55.634623Z",
     "iopub.status.idle": "2022-02-12T00:53:55.637365Z",
     "shell.execute_reply": "2022-02-12T00:53:55.637775Z",
     "shell.execute_reply.started": "2022-01-23T09:15:07.549862Z"
    },
    "papermill": {
     "duration": 0.070548,
     "end_time": "2022-02-12T00:53:55.637904",
     "exception": false,
     "start_time": "2022-02-12T00:53:55.567356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/train\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/amr-to-text-indonesia/train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279e20b8",
   "metadata": {
    "papermill": {
     "duration": 0.062458,
     "end_time": "2022-02-12T00:53:55.762831",
     "exception": false,
     "start_time": "2022-02-12T00:53:55.700373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## indot5 on gold data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f4b74e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T00:53:55.892568Z",
     "iopub.status.busy": "2022-02-12T00:53:55.891879Z",
     "iopub.status.idle": "2022-02-12T00:59:33.182452Z",
     "shell.execute_reply": "2022-02-12T00:59:33.181894Z",
     "shell.execute_reply.started": "2022-01-23T09:08:07.27874Z"
    },
    "papermill": {
     "duration": 337.358304,
     "end_time": "2022-02-12T00:59:33.182605",
     "exception": false,
     "start_time": "2022-02-12T00:53:55.824301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "Downloading: 100%|███████████████████████████| 777k/777k [00:00<00:00, 1.63MB/s]\r\n",
      "Downloading: 100%|██████████████████████████████| 696/696 [00:00<00:00, 665kB/s]\r\n",
      "Downloading: 100%|███████████████████████████| 990M/990M [00:36<00:00, 26.9MB/s]\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:3.9676 LR:0.00003000: 100%|█| 662/662 [01:24<00:00,  7.81it\r\n",
      "(Epoch 1) DEV LOSS:3.0188 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 24.68it/s]\r\n",
      "bleu score on dev:  34.108651953912975\r\n",
      "(Epoch 2) TRAIN LOSS:2.2271 LR:0.00003000: 100%|█| 662/662 [01:24<00:00,  7.83it\r\n",
      "(Epoch 2) DEV LOSS:1.9367 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 30.36it/s]\r\n",
      "bleu score on dev:  41.06118517857154\r\n",
      "(Epoch 3) TRAIN LOSS:1.6195 LR:0.00003000: 100%|█| 662/662 [01:26<00:00,  7.64it\r\n",
      "(Epoch 3) DEV LOSS:1.6779 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 28.04it/s]\r\n",
      "bleu score on dev:  50.28064502903658\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:17<00:00,  4.48it/s]\r\n",
      "sample:  meniup balon ---- balon itu ditiup ilham\r\n",
      "sample:  obe menulis puisi ---- obe menulis puisi\r\n",
      "sample:  saya mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  anak ajaib angga anak ajaib ---- angga anak ajaib\r\n",
      "sample:  orang yang ibu dosen ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  47.747159068074566\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"train_indoT5.py\", line 250, in <module>\r\n",
      "    with open(os.path.join(result_folder, 'bleu_score_test.txt'), 'w') as f:\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'result/result_linearized_penman/bleu_score_test.txt'\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoT5.py --model_type indo-t5 --n_epochs 3 --lr 3e-5 --data_folder ../data/preprocessed_data/linearized_penman  --result_folder result/result_linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35cf7706",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T00:59:35.104018Z",
     "iopub.status.busy": "2022-02-12T00:59:35.103242Z",
     "iopub.status.idle": "2022-02-12T01:04:30.766547Z",
     "shell.execute_reply": "2022-02-12T01:04:30.765625Z"
    },
    "papermill": {
     "duration": 296.664106,
     "end_time": "2022-02-12T01:04:30.766702",
     "exception": false,
     "start_time": "2022-02-12T00:59:34.102596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:2.7781 LR:0.00010000: 100%|█| 662/662 [01:25<00:00,  7.71it\r\n",
      "(Epoch 1) DEV LOSS:7.6389 LR:0.00010000: 100%|████| 5/5 [00:00<00:00, 31.32it/s]\r\n",
      "bleu score on dev:  0.9750760375995845\r\n",
      "(Epoch 2) TRAIN LOSS:5.1025 LR:0.00010000: 100%|█| 662/662 [01:25<00:00,  7.70it\r\n",
      "(Epoch 2) DEV LOSS:1.7064 LR:0.00010000: 100%|████| 5/5 [00:00<00:00, 27.91it/s]\r\n",
      "bleu score on dev:  17.56128478482164\r\n",
      "(Epoch 3) TRAIN LOSS:1.6329 LR:0.00010000: 100%|█| 662/662 [01:27<00:00,  7.61it\r\n",
      "(Epoch 3) DEV LOSS:1.1906 LR:0.00010000: 100%|████| 5/5 [00:00<00:00, 30.52it/s]\r\n",
      "bleu score on dev:  26.583318552734863\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:18<00:00,  4.27it/s]\r\n",
      "sample:  ditipu ilham ---- balon itu ditiup ilham\r\n",
      "sample:  di tulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  saya mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  dia anak ajaib ---- angga anak ajaib\r\n",
      "sample:  ibu orang yang ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  31.918696964440663\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"train_indoT5.py\", line 250, in <module>\r\n",
      "    with open(os.path.join(result_folder, 'bleu_score_test.txt'), 'w') as f:\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'result/result_linearized_penman/bleu_score_test.txt'\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoT5.py --model_type indo-t5 --n_epochs 3 --lr 1e-4 --data_folder ../data/preprocessed_data/linearized_penman  --result_folder result/result_linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3e6514a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T01:04:34.198892Z",
     "iopub.status.busy": "2022-02-12T01:04:34.198063Z",
     "iopub.status.idle": "2022-02-12T01:09:29.994904Z",
     "shell.execute_reply": "2022-02-12T01:09:29.994420Z"
    },
    "papermill": {
     "duration": 297.496749,
     "end_time": "2022-02-12T01:09:29.995054",
     "exception": false,
     "start_time": "2022-02-12T01:04:32.498305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:1.4768 LR:0.00100000: 100%|█| 662/662 [01:27<00:00,  7.58it\r\n",
      "(Epoch 1) DEV LOSS:0.8216 LR:0.00100000: 100%|████| 5/5 [00:00<00:00, 28.16it/s]\r\n",
      "bleu score on dev:  36.49858882232\r\n",
      "(Epoch 2) TRAIN LOSS:0.5929 LR:0.00100000: 100%|█| 662/662 [01:25<00:00,  7.70it\r\n",
      "(Epoch 2) DEV LOSS:0.6238 LR:0.00100000: 100%|████| 5/5 [00:00<00:00, 30.93it/s]\r\n",
      "bleu score on dev:  50.2772237710375\r\n",
      "(Epoch 3) TRAIN LOSS:0.4780 LR:0.00100000: 100%|█| 662/662 [01:27<00:00,  7.54it\r\n",
      "(Epoch 3) DEV LOSS:0.6108 LR:0.00100000: 100%|████| 5/5 [00:00<00:00, 30.68it/s]\r\n",
      "bleu score on dev:  45.93623228488682\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:16<00:00,  4.81it/s]\r\n",
      "sample:  balon tersebut ditipu oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi tersebut ditulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  saya mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  angga anak yang ajaib ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  28.062271658740517\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"train_indoT5.py\", line 250, in <module>\r\n",
      "    with open(os.path.join(result_folder, 'bleu_score_test.txt'), 'w') as f:\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'result/result_linearized_penman/bleu_score_test.txt'\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoT5.py --model_type indo-t5 --n_epochs 3 --lr 1e-3 --data_folder ../data/preprocessed_data/linearized_penman  --result_folder result/result_linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c475dc57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T01:09:35.319221Z",
     "iopub.status.busy": "2022-02-12T01:09:35.318419Z",
     "iopub.status.idle": "2022-02-12T01:13:09.084967Z",
     "shell.execute_reply": "2022-02-12T01:13:09.084408Z"
    },
    "papermill": {
     "duration": 216.541208,
     "end_time": "2022-02-12T01:13:09.085114",
     "exception": false,
     "start_time": "2022-02-12T01:09:32.543906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:3.9676 LR:0.00003000: 100%|█| 662/662 [01:28<00:00,  7.49it\r\n",
      "(Epoch 1) DEV LOSS:3.0188 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 32.55it/s]\r\n",
      "bleu score on dev:  34.108651953912975\r\n",
      "(Epoch 2) TRAIN LOSS:2.2271 LR:0.00003000: 100%|█| 662/662 [01:26<00:00,  7.61it\r\n",
      "(Epoch 2) DEV LOSS:1.9367 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 32.67it/s]\r\n",
      "bleu score on dev:  41.06118517857154\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:20<00:00,  3.84it/s]\r\n",
      "sample:  meniup balon ---- balon itu ditiup ilham\r\n",
      "sample:  ditulis obe puisi ---- obe menulis puisi\r\n",
      "sample:  saya mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  angga anak ajaib ---- angga anak ajaib\r\n",
      "sample:  orang yang ibu dosen itu ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  36.984849634326046\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"train_indoT5.py\", line 250, in <module>\r\n",
      "    with open(os.path.join(result_folder, 'bleu_score_test.txt'), 'w') as f:\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'result/result_linearized_penman/bleu_score_test.txt'\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoT5.py --model_type indo-t5 --n_epochs 2 --lr 3e-5 --data_folder ../data/preprocessed_data/linearized_penman  --result_folder result/result_linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc772bac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T01:13:15.582690Z",
     "iopub.status.busy": "2022-02-12T01:13:15.581873Z",
     "iopub.status.idle": "2022-02-12T01:16:44.162796Z",
     "shell.execute_reply": "2022-02-12T01:16:44.163360Z"
    },
    "papermill": {
     "duration": 211.953819,
     "end_time": "2022-02-12T01:16:44.163545",
     "exception": false,
     "start_time": "2022-02-12T01:13:12.209726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:2.7781 LR:0.00010000: 100%|█| 662/662 [01:28<00:00,  7.51it\r\n",
      "(Epoch 1) DEV LOSS:7.6389 LR:0.00010000: 100%|████| 5/5 [00:00<00:00, 20.42it/s]\r\n",
      "bleu score on dev:  0.9750760375995845\r\n",
      "(Epoch 2) TRAIN LOSS:5.1025 LR:0.00010000: 100%|█| 662/662 [01:27<00:00,  7.56it\r\n",
      "(Epoch 2) DEV LOSS:1.7064 LR:0.00010000: 100%|████| 5/5 [00:00<00:00, 33.35it/s]\r\n",
      "bleu score on dev:  17.56128478482164\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:14<00:00,  5.34it/s]\r\n",
      "sample:  , ilham meniup balon ---- balon itu ditiup ilham\r\n",
      "sample:  di tulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  diketik oleh saya ---- saya mengetik makalah\r\n",
      "sample:  angga anak ajaib ---- angga anak ajaib\r\n",
      "sample:  ibu ibu ibu ibu ibu dosen ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  25.55129521066688\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"train_indoT5.py\", line 250, in <module>\r\n",
      "    with open(os.path.join(result_folder, 'bleu_score_test.txt'), 'w') as f:\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'result/result_linearized_penman/bleu_score_test.txt'\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoT5.py --model_type indo-t5 --n_epochs 2 --lr 1e-4 --data_folder ../data/preprocessed_data/linearized_penman  --result_folder result/result_linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b5268b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T01:16:51.630934Z",
     "iopub.status.busy": "2022-02-12T01:16:51.630137Z",
     "iopub.status.idle": "2022-02-12T01:20:26.191366Z",
     "shell.execute_reply": "2022-02-12T01:20:26.190425Z"
    },
    "papermill": {
     "duration": 218.191926,
     "end_time": "2022-02-12T01:20:26.191526",
     "exception": false,
     "start_time": "2022-02-12T01:16:47.999600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:1.4768 LR:0.00100000: 100%|█| 662/662 [01:28<00:00,  7.46it\r\n",
      "(Epoch 1) DEV LOSS:0.8216 LR:0.00100000: 100%|████| 5/5 [00:00<00:00, 21.03it/s]\r\n",
      "bleu score on dev:  36.49858882232\r\n",
      "(Epoch 2) TRAIN LOSS:0.5929 LR:0.00100000: 100%|█| 662/662 [01:29<00:00,  7.36it\r\n",
      "(Epoch 2) DEV LOSS:0.6238 LR:0.00100000: 100%|████| 5/5 [00:00<00:00, 29.70it/s]\r\n",
      "bleu score on dev:  50.2772237710375\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:17<00:00,  4.49it/s]\r\n",
      "sample:  balon ditipu oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  saya mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  anak yang ajaib ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  33.00459301150985\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"train_indoT5.py\", line 250, in <module>\r\n",
      "    with open(os.path.join(result_folder, 'bleu_score_test.txt'), 'w') as f:\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'result/result_linearized_penman/bleu_score_test.txt'\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoT5.py --model_type indo-t5 --n_epochs 2 --lr 1e-3 --data_folder ../data/preprocessed_data/linearized_penman  --result_folder result/result_linearized_penman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fbc623",
   "metadata": {
    "papermill": {
     "duration": 4.326681,
     "end_time": "2022-02-12T01:20:34.586078",
     "exception": false,
     "start_time": "2022-02-12T01:20:30.259397",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## indobart on gold data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24e4354b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T01:20:43.713329Z",
     "iopub.status.busy": "2022-02-12T01:20:43.712543Z",
     "iopub.status.idle": "2022-02-12T01:23:40.916296Z",
     "shell.execute_reply": "2022-02-12T01:23:40.914876Z"
    },
    "papermill": {
     "duration": 181.370157,
     "end_time": "2022-02-12T01:23:40.916463",
     "exception": false,
     "start_time": "2022-02-12T01:20:39.546306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "Downloading: 100%|███████████████████████████| 932k/932k [00:00<00:00, 1.92MB/s]\r\n",
      "Downloading: 100%|██████████████████████████████| 315/315 [00:00<00:00, 245kB/s]\r\n",
      "Downloading: 100%|██████████████████████████████| 339/339 [00:00<00:00, 262kB/s]\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "Downloading: 100%|█████████████████████████| 1.71k/1.71k [00:00<00:00, 1.41MB/s]\r\n",
      "Downloading: 100%|███████████████████████████| 526M/526M [00:17<00:00, 29.3MB/s]\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:1.3601 LR:0.00003000: 100%|█| 662/662 [00:44<00:00, 14.91it\r\n",
      "(Epoch 1) DEV LOSS:0.5929 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 58.74it/s]\r\n",
      "bleu score on dev:  57.38434026338936\r\n",
      "(Epoch 2) TRAIN LOSS:0.5418 LR:0.00003000: 100%|█| 662/662 [00:41<00:00, 16.01it\r\n",
      "(Epoch 2) DEV LOSS:0.6770 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 56.78it/s]\r\n",
      "bleu score on dev:  56.57797910100631\r\n",
      "(Epoch 3) TRAIN LOSS:0.4376 LR:0.00003000: 100%|█| 662/662 [00:44<00:00, 14.74it\r\n",
      "(Epoch 3) DEV LOSS:0.4651 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 55.76it/s]\r\n",
      "bleu score on dev:  59.79251743606556\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:07<00:00,  9.69it/s]\r\n",
      "sample:  balon tersebut ditiup oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  saya sedang mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  anak ajaib tersebut bernama angga ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  34.51556796683057\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"train_indoBART.py\", line 251, in <module>\r\n",
      "    with open(os.path.join(result_folder, 'bleu_score_test.txt'), 'w') as f:\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'result/result_linearized_penman/bleu_score_test.txt'\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoBART.py --model_type indo-bart --n_epochs 3 --lr 3e-5 --data_folder ../data/preprocessed_data/linearized_penman  --result_folder result/result_linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0f84173",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T01:23:50.729480Z",
     "iopub.status.busy": "2022-02-12T01:23:50.728648Z",
     "iopub.status.idle": "2022-02-12T01:26:29.712514Z",
     "shell.execute_reply": "2022-02-12T01:26:29.712936Z"
    },
    "papermill": {
     "duration": 163.771774,
     "end_time": "2022-02-12T01:26:29.713112",
     "exception": false,
     "start_time": "2022-02-12T01:23:45.941338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:1.3575 LR:0.00010000: 100%|█| 662/662 [00:42<00:00, 15.70it\r\n",
      "(Epoch 1) DEV LOSS:0.7393 LR:0.00010000: 100%|████| 5/5 [00:00<00:00, 50.95it/s]\r\n",
      "bleu score on dev:  47.617350726030814\r\n",
      "(Epoch 2) TRAIN LOSS:0.6246 LR:0.00010000: 100%|█| 662/662 [00:46<00:00, 14.38it\r\n",
      "(Epoch 2) DEV LOSS:0.7186 LR:0.00010000: 100%|████| 5/5 [00:00<00:00, 53.80it/s]\r\n",
      "bleu score on dev:  55.432698942984466\r\n",
      "(Epoch 3) TRAIN LOSS:0.5373 LR:0.00010000: 100%|█| 662/662 [00:45<00:00, 14.64it\r\n",
      "(Epoch 3) DEV LOSS:0.7974 LR:0.00010000: 100%|████| 5/5 [00:00<00:00, 57.28it/s]\r\n",
      "bleu score on dev:  48.4377968785354\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:08<00:00,  9.48it/s]\r\n",
      "sample:  balonku sedang dipegang oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  tulisan ditulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  saya sedang mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  anak yang sedang bermain ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  27.560473895632075\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"train_indoBART.py\", line 251, in <module>\r\n",
      "    with open(os.path.join(result_folder, 'bleu_score_test.txt'), 'w') as f:\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'result/result_linearized_penman/bleu_score_test.txt'\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoBART.py --model_type indo-bart --n_epochs 3 --lr 1e-4 --data_folder ../data/preprocessed_data/linearized_penman  --result_folder result/result_linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cf3dcc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T01:26:41.193331Z",
     "iopub.status.busy": "2022-02-12T01:26:41.192499Z",
     "iopub.status.idle": "2022-02-12T01:29:19.671816Z",
     "shell.execute_reply": "2022-02-12T01:29:19.672279Z"
    },
    "papermill": {
     "duration": 164.137833,
     "end_time": "2022-02-12T01:29:19.672472",
     "exception": false,
     "start_time": "2022-02-12T01:26:35.534639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:4.9400 LR:0.00100000: 100%|█| 662/662 [00:42<00:00, 15.62it\r\n",
      "(Epoch 1) DEV LOSS:3.3593 LR:0.00100000: 100%|████| 5/5 [00:00<00:00, 57.28it/s]\r\n",
      "bleu score on dev:  1.0700580709533196\r\n",
      "(Epoch 2) TRAIN LOSS:3.3688 LR:0.00100000: 100%|█| 662/662 [00:45<00:00, 14.71it\r\n",
      "(Epoch 2) DEV LOSS:2.3617 LR:0.00100000: 100%|████| 5/5 [00:00<00:00, 57.46it/s]\r\n",
      "bleu score on dev:  5.260963468244618\r\n",
      "(Epoch 3) TRAIN LOSS:2.7457 LR:0.00100000: 100%|█| 662/662 [00:45<00:00, 14.59it\r\n",
      "(Epoch 3) DEV LOSS:2.4301 LR:0.00100000: 100%|████| 5/5 [00:00<00:00, 57.57it/s]\r\n",
      "bleu score on dev:  8.220646987768978\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:08<00:00,  8.68it/s]\r\n",
      "sample:  anda memilih pasangan ---- balon itu ditiup ilham\r\n",
      "sample:  tikar digelar budwi ---- obe menulis puisi\r\n",
      "sample:  kemenangan dibersihkan anda ---- saya mengetik makalah\r\n",
      "sample:  deras mengguyur sejak pukul tujuh pukul tujuh ---- angga anak ajaib\r\n",
      "sample:  peralatan dapur disiapkan ibu ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  0.5040690977513337\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"train_indoBART.py\", line 251, in <module>\r\n",
      "    with open(os.path.join(result_folder, 'bleu_score_test.txt'), 'w') as f:\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'result/result_linearized_penman/bleu_score_test.txt'\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoBART.py --model_type indo-bart --n_epochs 3 --lr 1e-3 --data_folder ../data/preprocessed_data/linearized_penman  --result_folder result/result_linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "669032af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T01:29:31.893724Z",
     "iopub.status.busy": "2022-02-12T01:29:31.892901Z",
     "iopub.status.idle": "2022-02-12T01:31:25.786967Z",
     "shell.execute_reply": "2022-02-12T01:31:25.785868Z"
    },
    "papermill": {
     "duration": 119.852804,
     "end_time": "2022-02-12T01:31:25.787116",
     "exception": false,
     "start_time": "2022-02-12T01:29:25.934312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:1.3601 LR:0.00003000: 100%|█| 662/662 [00:41<00:00, 15.77it\r\n",
      "(Epoch 1) DEV LOSS:0.5929 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 58.16it/s]\r\n",
      "bleu score on dev:  57.38434026338936\r\n",
      "(Epoch 2) TRAIN LOSS:0.5418 LR:0.00003000: 100%|█| 662/662 [00:45<00:00, 14.53it\r\n",
      "(Epoch 2) DEV LOSS:0.6770 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 57.42it/s]\r\n",
      "bleu score on dev:  56.57797910100631\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:09<00:00,  8.42it/s]\r\n",
      "sample:  balon ditiup oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi tersebut ditulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  saya sedang mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  anak ajaib tersebut ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  21.922773275975437\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"train_indoBART.py\", line 251, in <module>\r\n",
      "    with open(os.path.join(result_folder, 'bleu_score_test.txt'), 'w') as f:\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'result/result_linearized_penman/bleu_score_test.txt'\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoBART.py --model_type indo-bart --n_epochs 2 --lr 3e-5 --data_folder ../data/preprocessed_data/linearized_penman  --result_folder result/result_linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2479ff4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T01:31:38.818484Z",
     "iopub.status.busy": "2022-02-12T01:31:38.817666Z",
     "iopub.status.idle": "2022-02-12T01:33:32.659254Z",
     "shell.execute_reply": "2022-02-12T01:33:32.658704Z"
    },
    "papermill": {
     "duration": 120.526163,
     "end_time": "2022-02-12T01:33:32.659401",
     "exception": false,
     "start_time": "2022-02-12T01:31:32.133238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:1.3575 LR:0.00010000: 100%|█| 662/662 [00:42<00:00, 15.54it\r\n",
      "(Epoch 1) DEV LOSS:0.7393 LR:0.00010000: 100%|████| 5/5 [00:00<00:00, 56.05it/s]\r\n",
      "bleu score on dev:  47.617350726030814\r\n",
      "(Epoch 2) TRAIN LOSS:0.6246 LR:0.00010000: 100%|█| 662/662 [00:46<00:00, 14.39it\r\n",
      "(Epoch 2) DEV LOSS:0.7186 LR:0.00010000: 100%|████| 5/5 [00:00<00:00, 56.71it/s]\r\n",
      "bleu score on dev:  55.432698942984466\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:08<00:00,  8.85it/s]\r\n",
      "sample:  balon sedang dipintal oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis oleh oleh obe ---- obe menulis puisi\r\n",
      "sample:  saya sedang mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  anak yang ajaib ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  21.274296092182574\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"train_indoBART.py\", line 251, in <module>\r\n",
      "    with open(os.path.join(result_folder, 'bleu_score_test.txt'), 'w') as f:\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'result/result_linearized_penman/bleu_score_test.txt'\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoBART.py --model_type indo-bart --n_epochs 2 --lr 1e-4 --data_folder ../data/preprocessed_data/linearized_penman  --result_folder result/result_linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bb066c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T01:33:47.433813Z",
     "iopub.status.busy": "2022-02-12T01:33:47.433004Z",
     "iopub.status.idle": "2022-02-12T01:35:41.956377Z",
     "shell.execute_reply": "2022-02-12T01:35:41.955844Z"
    },
    "papermill": {
     "duration": 121.683594,
     "end_time": "2022-02-12T01:35:41.956521",
     "exception": false,
     "start_time": "2022-02-12T01:33:40.272927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:4.9400 LR:0.00100000: 100%|█| 662/662 [00:43<00:00, 15.33it\r\n",
      "(Epoch 1) DEV LOSS:3.3593 LR:0.00100000: 100%|████| 5/5 [00:00<00:00, 55.90it/s]\r\n",
      "bleu score on dev:  1.0700580709533196\r\n",
      "(Epoch 2) TRAIN LOSS:3.3688 LR:0.00100000: 100%|█| 662/662 [00:46<00:00, 14.34it\r\n",
      "(Epoch 2) DEV LOSS:2.3617 LR:0.00100000: 100%|████| 5/5 [00:00<00:00, 56.46it/s]\r\n",
      "bleu score on dev:  5.260963468244618\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:08<00:00,  9.18it/s]\r\n",
      "sample:  penilaian diumumkan direktur ---- balon itu ditiup ilham\r\n",
      "sample:  yang merangkak yang merangkak ---- obe menulis puisi\r\n",
      "sample:  merangkak yang merangkak ---- saya mengetik makalah\r\n",
      "sample:  dewi anak yang manja ---- angga anak ajaib\r\n",
      "sample:  air dituang oleh suster siska ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  0.5872655164109123\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"train_indoBART.py\", line 251, in <module>\r\n",
      "    with open(os.path.join(result_folder, 'bleu_score_test.txt'), 'w') as f:\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'result/result_linearized_penman/bleu_score_test.txt'\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoBART.py --model_type indo-bart --n_epochs 2 --lr 1e-3 --data_folder ../data/preprocessed_data/linearized_penman  --result_folder result/result_linearized_penman"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2554.297956,
   "end_time": "2022-02-12T01:35:50.320944",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-12T00:53:16.022988",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
